{"cells":[{"cell_type":"markdown","metadata":{"id":"MTLIOX9vE6hI"},"source":["baseline with two classifiers. Male and female should have distinct confusion matrix and accuracy"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23196,"status":"ok","timestamp":1662506003721,"user":{"displayName":"jamal Alasadi","userId":"04565293417501620049"},"user_tz":240},"id":"mNmkj2u2cHuF","outputId":"3c2f3245-1852-4e43-a45c-831015193234"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive',force_remount=True)\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":203,"status":"ok","timestamp":1662506003916,"user":{"displayName":"jamal Alasadi","userId":"04565293417501620049"},"user_tz":240},"id":"mOymbijtceu3","outputId":"079c70ce-c0c7-4f63-cda0-0ca9c7576354"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tue Sep  6 23:13:23 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   34C    P0    25W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') \u003e= 0:\n","  print('Select the Runtime \u003e \"Change runtime type\" menu to enable a GPU accelerator, ')\n","  print('and then re-execute this cell.')\n","else:\n","  print(gpu_info)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","output_embedded_package_id":"1v2BnACiHfYACtxAhebd5MvtbK__VuIgy"},"id":"cYFHxp0ach2V","outputId":"ec48fe58-6578-47f1-90f8-c6b824c3b79e"},"outputs":[],"source":["!7z e \"/content/drive/My Drive/celeba_png/img_align_celeba_png.7z.001\" -o\"/content/img/\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"-b1dCLfVcmYE"},"outputs":[],"source":["import os\n","import numpy as np\n","import pandas as pd\n","import sklearn.metrics as mt\n","from skimage import io, transform\n","\n","\n","import torch.optim as optim\n","from torch.autograd import Variable\n","from torch.utils.data import Dataset, DataLoader\n","from torch.utils.data.sampler import SubsetRandomSampler\n","\n","\n","# additional files is located in /home/jamal/Downloads/additional_files/\n","def save_checkpoint_high(state, filename='/content/drive/MyDrive/vre_autoencoder_final/trained_modes_final/checkpoint_propose_Male_celeb_high.pth.tar'):\n","    torch.save(state, filename)\n","def save_checkpoint_low(state, filename='/content/drive/MyDrive/vre_autoencoder_final/trained_modes_final/checkpoint_propose_Male_celeb_low.pth.tar'):\n","    torch.save(state, filename)\n","def save_checkpoint_class_MF(state, filename='/content/drive/MyDrive/vre_autoencoder_final/trained_modes_final/checkpoint_classification_propose_MF.pth.tar'):\n","    torch.save(state, filename)\n","def save_checkpoint_class_SD(state, filename='/content/drive/MyDrive/vre_autoencoder_final/trained_modes_final/checkpoint_classification_propose_SD.pth.tar'):\n","    torch.save(state, filename)\n","\n","file_path ='/content/drive/MyDrive/list_attr_celeba1_train_used.txt'\n","\n","\n","images_path = '/content/img/' #train\n","\n","load_net = 0\n","\n","columns = ['ImgId','5_o_Clock_Shadow',\n","           ' Arched_Eyebrows', 'Attractive',\n","           'Bags_Under_Eyes', 'Bald', 'Bangs',\n","           'Big_Lips', 'Big_Nose', 'Black_Hair',\n","           'Blond_Hair', 'Blurry', 'Brown_Hair',\n","           'Bushy_Eyebrows', 'Chubby', 'Double_Chin',\n","           'Eyeglasses', 'Goatee', 'Gray_Hair', 'Heavy_Makeup',\n","           'High_Cheekbones', 'Male', 'Mouth_Slightly_Open', 'Mustache',\n","           'Narrow_Eyes', 'No_Beard', 'Oval_Face', 'Pale_Skin', 'Pointy_Nose',\n","           'Receding_Hairline', 'Rosy_Cheeks', 'Sideburns', 'Smiling', 'Straight_Hair',\n","           'Wavy_Hair', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Lipstick', 'Wearing_Necklace', 'Wearing_Necktie', 'Young']\n","\n","\n","cele_attrib = pd.read_csv(file_path,delimiter = \"\\s+\",names = columns)# lfw = cele_attrib.set_index('SUBJECT_ID')\n","lfw = cele_attrib\n","\n","\n","import torch\n","import torch.nn.functional as F\n","import torch.nn as nn\n","\n","class VAE(nn.Module):\n","    def __init__(self, nc, ngf, ndf, latent_variable_size):\n","        super(VAE, self).__init__()\n","\n","        self.nc = nc\n","        self.ngf = ngf\n","        self.ndf = ndf\n","        self.latent_variable_size = latent_variable_size\n","\n","        # encoder\n","        self.e1 = nn.Conv2d(nc, self.ndf, 4, 2, 1)\n","        self.bn1 = nn.BatchNorm2d(self.ndf)\n","\n","        self.e2 = nn.Conv2d(self.ndf, self.ndf*2, 4, 2, 1)\n","        self.bn2 = nn.BatchNorm2d(self.ndf*2)\n","\n","        self.e3 = nn.Conv2d(ndf*2, ndf*4, 4, 2, 1)\n","        self.bn3 = nn.BatchNorm2d(ndf*4)\n","\n","        self.e4 = nn.Conv2d(ndf*4, ndf*8, 4, 2, 1)\n","        self.bn4 = nn.BatchNorm2d(ndf*8)\n","\n","        self.e5 = nn.Conv2d(ndf*8, ndf*8, 4, 2, 1)\n","        self.bn5 = nn.BatchNorm2d(ndf*8)\n","\n","        self.fc1 = nn.Linear(ndf*8*4*4, latent_variable_size)\n","        self.fc2 = nn.Linear(ndf*8*4*4, latent_variable_size)\n","\n","        # decoder\n","        self.d1 = nn.Linear(latent_variable_size, self.ngf*8*2*4*4)\n","\n","        self.up1 = nn.UpsamplingNearest2d(scale_factor=2)\n","        self.pd1 = nn.ReplicationPad2d(1)\n","        self.d2 = nn.Conv2d(self.ngf*8*2, self.ngf*8, 3, 1)\n","        self.bn6 = nn.BatchNorm2d(self.ngf*8, 1.e-3)\n","\n","        self.up2 = nn.UpsamplingNearest2d(scale_factor=2)\n","        self.pd2 = nn.ReplicationPad2d(1)\n","        self.d3 = nn.Conv2d(self.ngf*8, self.ngf*4, 3, 1)\n","        self.bn7 = nn.BatchNorm2d(self.ngf*4, 1.e-3)\n","\n","        self.up3 = nn.UpsamplingNearest2d(scale_factor=2)\n","        self.pd3 = nn.ReplicationPad2d(1)\n","        self.d4 = nn.Conv2d(self.ngf*4, self.ngf*2, 3, 1)\n","        self.bn8 = nn.BatchNorm2d(self.ngf*2, 1.e-3)\n","\n","        self.up4 = nn.UpsamplingNearest2d(scale_factor=2)\n","        self.pd4 = nn.ReplicationPad2d(1)\n","        self.d5 = nn.Conv2d(self.ngf*2, self.ngf, 3, 1)\n","        self.bn9 = nn.BatchNorm2d(self.ngf, 1.e-3)\n","\n","        self.up5 = nn.UpsamplingNearest2d(scale_factor=2)\n","        self.pd5 = nn.ReplicationPad2d(1)\n","        self.d6 = nn.Conv2d(self.ngf, nc, 3, 1)\n","\n","        self.leakyrelu = nn.LeakyReLU(0.2)\n","        self.relu = nn.ReLU()\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def encode(self, x):\n","        h1 = self.leakyrelu(self.bn1(self.e1(x)))\n","        h2 = self.leakyrelu(self.bn2(self.e2(h1)))\n","        h3 = self.leakyrelu(self.bn3(self.e3(h2)))\n","        h4 = self.leakyrelu(self.bn4(self.e4(h3)))\n","        h5 = self.leakyrelu(self.bn5(self.e5(h4)))\n","        h5 = h5.view(-1, self.ndf*8*4*4)\n","\n","        return self.fc1(h5), self.fc2(h5), h5\n","\n","    def reparametrize(self, mu, logvar):\n","        std = logvar.mul(0.5).exp_()\n","        if use_cuda:\n","            eps = torch.cuda.FloatTensor(std.size()).normal_()\n","        else:\n","            eps = torch.FloatTensor(std.size()).normal_()\n","        eps = Variable(eps)\n","        return eps.mul(std).add_(mu)\n","\n","    def decode(self, z):\n","        h1 = self.relu(self.d1(z))\n","        h1 = h1.view(-1, self.ngf*8*2, 4, 4)\n","        h2 = self.leakyrelu(self.bn6(self.d2(self.pd1(self.up1(h1)))))\n","        h3 = self.leakyrelu(self.bn7(self.d3(self.pd2(self.up2(h2)))))\n","        h4 = self.leakyrelu(self.bn8(self.d4(self.pd3(self.up3(h3)))))\n","        h5 = self.leakyrelu(self.bn9(self.d5(self.pd4(self.up4(h4)))))\n","\n","        return self.sigmoid(self.d6(self.pd5(self.up5(h5))))\n","\n","    def get_latent_var(self, x):\n","        mu, logvar, h1 = self.encode(x.view(-1, self.nc, self.ndf, self.ngf))\n","        z = self.reparametrize(mu, logvar)\n","        return z\n","\n","    def forward(self, x):\n","        mu, logvar,h2 = self.encode(x.view(-1, self.nc, self.ndf, self.ngf))\n","        z = self.reparametrize(mu, logvar)\n","        res = self.decode(z)\n","        return res, mu, logvar, h2\n","\n","\n","class VAE_low(nn.Module):\n","    def __init__(self, nc, ngf, ndf, latent_variable_size):\n","        super(VAE_low, self).__init__()\n","\n","        self.nc = nc\n","        self.ngf = ngf\n","        self.ndf = ndf\n","        self.latent_variable_size = latent_variable_size\n","\n","        # encoder\n","        self.e1 = nn.Conv2d(nc, self.ndf, 4, 2, 1)\n","        self.bn1 = nn.BatchNorm2d(self.ndf)\n","\n","        self.e2 = nn.Conv2d(self.ndf, self.ndf*2, 4, 2, 1)\n","        self.bn2 = nn.BatchNorm2d(self.ndf*2)\n","\n","        self.e3 = nn.Conv2d(ndf*2, ndf*4, 4, 2, 1)\n","        self.bn3 = nn.BatchNorm2d(ndf*4)\n","        #\n","        # self.e4 = nn.Conv2d(ndf*4, ndf*4, 4, 2, 1)\n","        # self.bn4 = nn.BatchNorm2d(ndf*4)\n","\n","        # self.e5 = nn.Conv2d(ndf*8, ndf*8, 4, 2, 1)\n","        # self.bn5 = nn.BatchNorm2d(ndf*8)\n","\n","        self.fc1 = nn.Linear(ndf*4*4, latent_variable_size)\n","        self.fc2 = nn.Linear(ndf*4*4, latent_variable_size)\n","\n","        # decoder\n","        self.d1 = nn.Linear(latent_variable_size, ngf*4*4)\n","\n","        self.up1 = nn.UpsamplingNearest2d(scale_factor=2)\n","        self.pd1 = nn.ReplicationPad2d(1)\n","        self.d2 = nn.Conv2d(self.ngf, self.ngf//2, 3, 1)\n","        self.bn6 = nn.BatchNorm2d(self.ngf//2, 1.e-3)\n","\n","        self.up2 = nn.UpsamplingNearest2d(scale_factor=2)\n","        self.pd2 = nn.ReplicationPad2d(1)\n","        self.d3 = nn.Conv2d(self.ngf, self.ngf//2, 3, 1)\n","        self.bn7 = nn.BatchNorm2d(self.ngf//2, 1.e-3)\n","\n","        self.up3 = nn.UpsamplingNearest2d(scale_factor=2)\n","        self.pd3 = nn.ReplicationPad2d(1)\n","        self.d4 = nn.Conv2d(self.ngf*4, self.ngf*2, 3, 1)\n","        self.bn8 = nn.BatchNorm2d(self.ngf*2, 1.e-3)\n","\n","        self.up4 = nn.UpsamplingNearest2d(scale_factor=2)\n","        self.pd4 = nn.ReplicationPad2d(1)\n","        self.d5 = nn.Conv2d(self.ngf*2, self.ngf, 3, 1)\n","        self.bn9 = nn.BatchNorm2d(self.ngf, 1.e-3)\n","\n","        self.up5 = nn.UpsamplingNearest2d(scale_factor=2)\n","        self.pd5 = nn.ReplicationPad2d(1)\n","        self.d6 = nn.Conv2d(self.ngf//2, nc, 3, 1)\n","\n","        self.leakyrelu = nn.LeakyReLU(0.2)\n","        self.relu = nn.ReLU()\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def encode(self, x):\n","        h1 = self.leakyrelu(self.bn1(self.e1(x)))\n","        h2 = self.leakyrelu(self.bn2(self.e2(h1)))\n","        h3 = self.leakyrelu(self.bn3(self.e3(h2)))\n","        # h4 = self.leakyrelu(self.bn4(self.e4(h3)))\n","        # h5 = self.leakyrelu(self.bn5(self.e5(h4)))\n","        h5 = h3.view(-1, self.ndf*4*4)\n","\n","        return self.fc1(h5), self.fc2(h5),h5\n","\n","    def reparametrize(self, mu, logvar):\n","        std = logvar.mul(0.5).exp_()\n","        if use_cuda:\n","            eps = torch.cuda.FloatTensor(std.size()).normal_()\n","        else:\n","            eps = torch.FloatTensor(std.size()).normal_()\n","        eps = Variable(eps)\n","        return eps.mul(std).add_(mu)\n","\n","    def decode(self, z):\n","        h1 = self.relu(self.d1(z))\n","        h1 = h1.view(-1, self.ngf, 4, 4)\n","        h2 = self.leakyrelu(self.bn6(self.d2(self.pd1(self.up1(h1)))))\n","\n","\n","        return self.sigmoid(self.d6(self.pd5(self.up5(h2))))\n","\n","    def get_latent_var(self, x):\n","        mu, logvar,h5 = self.encode(x.view(-1, self.nc, self.ndf, self.ngf))\n","        z = self.reparametrize(mu, logvar)\n","        return z\n","\n","    def forward(self, x):\n","        mu, logvar, h5 = self.encode(x.view(-1, self.nc, self.ndf, self.ngf))\n","        z = self.reparametrize(mu, logvar)\n","        res = self.decode(z)\n","        return res, mu, logvar, h5\n","\n","\n","\n","\n","\n","       \n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"bz2q4bLXvbKt"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:198: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["HQ image reconstruction loss: 3267.869384765625 iteration:  0 epoch:  0\n","LQ image reconstruction loss: 402.1827392578125 iteration:  0 epoch:  0\n","Same Different Classification loss: 0.6888946294784546 iteration:  0 epoch:  0\n","Male Female Classification loss: 0.7703017592430115 iteration:  0 epoch:  0\n","----------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:253: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","HQ image reconstruction loss: 4.066238880157471 iteration:  1135 epoch:  3\n","LQ image reconstruction loss: 0.7574517130851746 iteration:  1135 epoch:  3\n","Same Different Classification loss: 0.5914996862411499 iteration:  1135 epoch:  3\n","Male Female Classification loss: 0.6556410789489746 iteration:  1135 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.9412455558776855 iteration:  1140 epoch:  3\n","LQ image reconstruction loss: 0.8591722249984741 iteration:  1140 epoch:  3\n","Same Different Classification loss: 0.5552775859832764 iteration:  1140 epoch:  3\n","Male Female Classification loss: 0.6284099221229553 iteration:  1140 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.3251452445983887 iteration:  1145 epoch:  3\n","LQ image reconstruction loss: 0.8449636697769165 iteration:  1145 epoch:  3\n","Same Different Classification loss: 0.6094129681587219 iteration:  1145 epoch:  3\n","Male Female Classification loss: 0.6868818402290344 iteration:  1145 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.5259392261505127 iteration:  1150 epoch:  3\n","LQ image reconstruction loss: 0.8140946626663208 iteration:  1150 epoch:  3\n","Same Different Classification loss: 0.5662575364112854 iteration:  1150 epoch:  3\n","Male Female Classification loss: 0.6546677947044373 iteration:  1150 epoch:  3\n","----------------------------------------------------\n","Accuracy of SD: 69 %\n","Accuracy of MF: 65 %\n","Mean Accuracy of SD: 57 %\n","Mean Accuracy of MF: 60 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 4.334779262542725 iteration:  1155 epoch:  3\n","LQ image reconstruction loss: 0.8400189876556396 iteration:  1155 epoch:  3\n","Same Different Classification loss: 0.5680255889892578 iteration:  1155 epoch:  3\n","Male Female Classification loss: 0.6123838424682617 iteration:  1155 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.6804616451263428 iteration:  1160 epoch:  3\n","LQ image reconstruction loss: 0.725232720375061 iteration:  1160 epoch:  3\n","Same Different Classification loss: 0.5277194380760193 iteration:  1160 epoch:  3\n","Male Female Classification loss: 0.671906590461731 iteration:  1160 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.52592396736145 iteration:  1165 epoch:  3\n","LQ image reconstruction loss: 0.8276682496070862 iteration:  1165 epoch:  3\n","Same Different Classification loss: 0.6097714900970459 iteration:  1165 epoch:  3\n","Male Female Classification loss: 0.6131907105445862 iteration:  1165 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.3686814308166504 iteration:  1170 epoch:  3\n","LQ image reconstruction loss: 0.8051590323448181 iteration:  1170 epoch:  3\n","Same Different Classification loss: 0.5739341378211975 iteration:  1170 epoch:  3\n","Male Female Classification loss: 0.6723077297210693 iteration:  1170 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.544083595275879 iteration:  1175 epoch:  3\n","LQ image reconstruction loss: 0.7458163499832153 iteration:  1175 epoch:  3\n","Same Different Classification loss: 0.5852720737457275 iteration:  1175 epoch:  3\n","Male Female Classification loss: 0.7052990794181824 iteration:  1175 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.605898141860962 iteration:  1180 epoch:  3\n","LQ image reconstruction loss: 0.7709605097770691 iteration:  1180 epoch:  3\n","Same Different Classification loss: 0.6317332983016968 iteration:  1180 epoch:  3\n","Male Female Classification loss: 0.6969336867332458 iteration:  1180 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.2012033462524414 iteration:  1185 epoch:  3\n","LQ image reconstruction loss: 0.6819068193435669 iteration:  1185 epoch:  3\n","Same Different Classification loss: 0.5259038805961609 iteration:  1185 epoch:  3\n","Male Female Classification loss: 0.7310166954994202 iteration:  1185 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.4734654426574707 iteration:  1190 epoch:  3\n","LQ image reconstruction loss: 0.8695286512374878 iteration:  1190 epoch:  3\n","Same Different Classification loss: 0.6628869771957397 iteration:  1190 epoch:  3\n","Male Female Classification loss: 0.6262292861938477 iteration:  1190 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.2827656269073486 iteration:  1195 epoch:  3\n","LQ image reconstruction loss: 0.7473767995834351 iteration:  1195 epoch:  3\n","Same Different Classification loss: 0.540855348110199 iteration:  1195 epoch:  3\n","Male Female Classification loss: 0.6714907884597778 iteration:  1195 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.749091386795044 iteration:  1200 epoch:  3\n","LQ image reconstruction loss: 0.8121055960655212 iteration:  1200 epoch:  3\n","Same Different Classification loss: 0.6078301072120667 iteration:  1200 epoch:  3\n","Male Female Classification loss: 0.6591538786888123 iteration:  1200 epoch:  3\n","----------------------------------------------------\n","Accuracy of SD: 72 %\n","Accuracy of MF: 56 %\n","Mean Accuracy of SD: 57 %\n","Mean Accuracy of MF: 60 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.0127005577087402 iteration:  1205 epoch:  3\n","LQ image reconstruction loss: 0.7754931449890137 iteration:  1205 epoch:  3\n","Same Different Classification loss: 0.548855185508728 iteration:  1205 epoch:  3\n","Male Female Classification loss: 0.6164633631706238 iteration:  1205 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.0192105770111084 iteration:  1210 epoch:  3\n","LQ image reconstruction loss: 0.8163454532623291 iteration:  1210 epoch:  3\n","Same Different Classification loss: 0.582057535648346 iteration:  1210 epoch:  3\n","Male Female Classification loss: 0.6653872728347778 iteration:  1210 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.6304852962493896 iteration:  1215 epoch:  3\n","LQ image reconstruction loss: 0.8217155933380127 iteration:  1215 epoch:  3\n","Same Different Classification loss: 0.5812998414039612 iteration:  1215 epoch:  3\n","Male Female Classification loss: 0.6108698844909668 iteration:  1215 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.98294997215271 iteration:  1220 epoch:  3\n","LQ image reconstruction loss: 0.9209938645362854 iteration:  1220 epoch:  3\n","Same Different Classification loss: 0.6347668170928955 iteration:  1220 epoch:  3\n","Male Female Classification loss: 0.5658488273620605 iteration:  1220 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.3345255851745605 iteration:  1225 epoch:  3\n","LQ image reconstruction loss: 0.7275550365447998 iteration:  1225 epoch:  3\n","Same Different Classification loss: 0.5397929549217224 iteration:  1225 epoch:  3\n","Male Female Classification loss: 0.7053368091583252 iteration:  1225 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.845630168914795 iteration:  1230 epoch:  3\n","LQ image reconstruction loss: 0.7683004140853882 iteration:  1230 epoch:  3\n","Same Different Classification loss: 0.5987189412117004 iteration:  1230 epoch:  3\n","Male Female Classification loss: 0.6518239378929138 iteration:  1230 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.945908308029175 iteration:  1235 epoch:  3\n","LQ image reconstruction loss: 0.8109354972839355 iteration:  1235 epoch:  3\n","Same Different Classification loss: 0.5585180521011353 iteration:  1235 epoch:  3\n","Male Female Classification loss: 0.6343526244163513 iteration:  1235 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.5946691036224365 iteration:  1240 epoch:  3\n","LQ image reconstruction loss: 0.7717487812042236 iteration:  1240 epoch:  3\n","Same Different Classification loss: 0.5672757625579834 iteration:  1240 epoch:  3\n","Male Female Classification loss: 0.5937815308570862 iteration:  1240 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.0721094608306885 iteration:  1245 epoch:  3\n","LQ image reconstruction loss: 0.7565253973007202 iteration:  1245 epoch:  3\n","Same Different Classification loss: 0.5541972517967224 iteration:  1245 epoch:  3\n","Male Female Classification loss: 0.636421799659729 iteration:  1245 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.0484883785247803 iteration:  1250 epoch:  3\n","LQ image reconstruction loss: 0.8703768253326416 iteration:  1250 epoch:  3\n","Same Different Classification loss: 0.6599301695823669 iteration:  1250 epoch:  3\n","Male Female Classification loss: 0.6789796352386475 iteration:  1250 epoch:  3\n","----------------------------------------------------\n","Accuracy of SD: 71 %\n","Accuracy of MF: 63 %\n","Mean Accuracy of SD: 57 %\n","Mean Accuracy of MF: 60 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.23075008392334 iteration:  1255 epoch:  3\n","LQ image reconstruction loss: 0.8381747007369995 iteration:  1255 epoch:  3\n","Same Different Classification loss: 0.6016425490379333 iteration:  1255 epoch:  3\n","Male Female Classification loss: 0.6087050437927246 iteration:  1255 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.600776195526123 iteration:  1260 epoch:  3\n","LQ image reconstruction loss: 0.7541587948799133 iteration:  1260 epoch:  3\n","Same Different Classification loss: 0.581733763217926 iteration:  1260 epoch:  3\n","Male Female Classification loss: 0.6242360472679138 iteration:  1260 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.0185093879699707 iteration:  1265 epoch:  3\n","LQ image reconstruction loss: 0.8210152983665466 iteration:  1265 epoch:  3\n","Same Different Classification loss: 0.5997259020805359 iteration:  1265 epoch:  3\n","Male Female Classification loss: 0.6466208100318909 iteration:  1265 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.6797306537628174 iteration:  1270 epoch:  3\n","LQ image reconstruction loss: 0.7935335636138916 iteration:  1270 epoch:  3\n","Same Different Classification loss: 0.6123724579811096 iteration:  1270 epoch:  3\n","Male Female Classification loss: 0.629464328289032 iteration:  1270 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.8199329376220703 iteration:  1275 epoch:  3\n","LQ image reconstruction loss: 0.8134981393814087 iteration:  1275 epoch:  3\n","Same Different Classification loss: 0.5802049040794373 iteration:  1275 epoch:  3\n","Male Female Classification loss: 0.6597284078598022 iteration:  1275 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.2536654472351074 iteration:  1280 epoch:  3\n","LQ image reconstruction loss: 0.8112808465957642 iteration:  1280 epoch:  3\n","Same Different Classification loss: 0.5469658374786377 iteration:  1280 epoch:  3\n","Male Female Classification loss: 0.6690701842308044 iteration:  1280 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.4239518642425537 iteration:  1285 epoch:  3\n","LQ image reconstruction loss: 0.7704112529754639 iteration:  1285 epoch:  3\n","Same Different Classification loss: 0.63033527135849 iteration:  1285 epoch:  3\n","Male Female Classification loss: 0.6999812126159668 iteration:  1285 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.7215161323547363 iteration:  1290 epoch:  3\n","LQ image reconstruction loss: 0.9098074436187744 iteration:  1290 epoch:  3\n","Same Different Classification loss: 0.6862829327583313 iteration:  1290 epoch:  3\n","Male Female Classification loss: 0.6533550024032593 iteration:  1290 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.1553850173950195 iteration:  1295 epoch:  3\n","LQ image reconstruction loss: 0.8226196765899658 iteration:  1295 epoch:  3\n","Same Different Classification loss: 0.6012148857116699 iteration:  1295 epoch:  3\n","Male Female Classification loss: 0.6487840414047241 iteration:  1295 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.5580079555511475 iteration:  1300 epoch:  3\n","LQ image reconstruction loss: 0.7923120260238647 iteration:  1300 epoch:  3\n","Same Different Classification loss: 0.5395891666412354 iteration:  1300 epoch:  3\n","Male Female Classification loss: 0.6009318828582764 iteration:  1300 epoch:  3\n","----------------------------------------------------\n","Accuracy of SD: 74 %\n","Accuracy of MF: 66 %\n","Mean Accuracy of SD: 58 %\n","Mean Accuracy of MF: 60 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.225809335708618 iteration:  1305 epoch:  3\n","LQ image reconstruction loss: 0.825493335723877 iteration:  1305 epoch:  3\n","Same Different Classification loss: 0.5905808806419373 iteration:  1305 epoch:  3\n","Male Female Classification loss: 0.6251097321510315 iteration:  1305 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.363135814666748 iteration:  1310 epoch:  3\n","LQ image reconstruction loss: 0.6867566108703613 iteration:  1310 epoch:  3\n","Same Different Classification loss: 0.5033549666404724 iteration:  1310 epoch:  3\n","Male Female Classification loss: 0.65055251121521 iteration:  1310 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.33795166015625 iteration:  1315 epoch:  3\n","LQ image reconstruction loss: 0.7529520392417908 iteration:  1315 epoch:  3\n","Same Different Classification loss: 0.5804463624954224 iteration:  1315 epoch:  3\n","Male Female Classification loss: 0.6058074235916138 iteration:  1315 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.5427427291870117 iteration:  1320 epoch:  3\n","LQ image reconstruction loss: 0.7679476141929626 iteration:  1320 epoch:  3\n","Same Different Classification loss: 0.5724782347679138 iteration:  1320 epoch:  3\n","Male Female Classification loss: 0.6607173085212708 iteration:  1320 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.63590669631958 iteration:  1325 epoch:  3\n","LQ image reconstruction loss: 0.8321710824966431 iteration:  1325 epoch:  3\n","Same Different Classification loss: 0.6282967925071716 iteration:  1325 epoch:  3\n","Male Female Classification loss: 0.5895987153053284 iteration:  1325 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.839695930480957 iteration:  1330 epoch:  3\n","LQ image reconstruction loss: 0.7696772813796997 iteration:  1330 epoch:  3\n","Same Different Classification loss: 0.5862562656402588 iteration:  1330 epoch:  3\n","Male Female Classification loss: 0.6636475920677185 iteration:  1330 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.962383985519409 iteration:  1335 epoch:  3\n","LQ image reconstruction loss: 0.812728762626648 iteration:  1335 epoch:  3\n","Same Different Classification loss: 0.6155837178230286 iteration:  1335 epoch:  3\n","Male Female Classification loss: 0.6581693887710571 iteration:  1335 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 4.318746089935303 iteration:  1340 epoch:  3\n","LQ image reconstruction loss: 0.8464016914367676 iteration:  1340 epoch:  3\n","Same Different Classification loss: 0.657914936542511 iteration:  1340 epoch:  3\n","Male Female Classification loss: 0.6238865852355957 iteration:  1340 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.6446194648742676 iteration:  1345 epoch:  3\n","LQ image reconstruction loss: 0.8651360273361206 iteration:  1345 epoch:  3\n","Same Different Classification loss: 0.6758272051811218 iteration:  1345 epoch:  3\n","Male Female Classification loss: 0.6456092596054077 iteration:  1345 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.998188018798828 iteration:  1350 epoch:  3\n","LQ image reconstruction loss: 0.7060469388961792 iteration:  1350 epoch:  3\n","Same Different Classification loss: 0.5610924363136292 iteration:  1350 epoch:  3\n","Male Female Classification loss: 0.6769763827323914 iteration:  1350 epoch:  3\n","----------------------------------------------------\n","Accuracy of SD: 63 %\n","Accuracy of MF: 68 %\n","Mean Accuracy of SD: 58 %\n","Mean Accuracy of MF: 60 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.7614378929138184 iteration:  1355 epoch:  3\n","LQ image reconstruction loss: 0.7790038585662842 iteration:  1355 epoch:  3\n","Same Different Classification loss: 0.578972339630127 iteration:  1355 epoch:  3\n","Male Female Classification loss: 0.6881108283996582 iteration:  1355 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.836371898651123 iteration:  1360 epoch:  3\n","LQ image reconstruction loss: 0.8106871843338013 iteration:  1360 epoch:  3\n","Same Different Classification loss: 0.5594525933265686 iteration:  1360 epoch:  3\n","Male Female Classification loss: 0.5891417860984802 iteration:  1360 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.2736940383911133 iteration:  1365 epoch:  3\n","LQ image reconstruction loss: 0.7594524621963501 iteration:  1365 epoch:  3\n","Same Different Classification loss: 0.5711371898651123 iteration:  1365 epoch:  3\n","Male Female Classification loss: 0.6313654780387878 iteration:  1365 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.8352742195129395 iteration:  1370 epoch:  3\n","LQ image reconstruction loss: 0.7295724153518677 iteration:  1370 epoch:  3\n","Same Different Classification loss: 0.5551893711090088 iteration:  1370 epoch:  3\n","Male Female Classification loss: 0.6968154907226562 iteration:  1370 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.3345460891723633 iteration:  1375 epoch:  3\n","LQ image reconstruction loss: 0.8483716249465942 iteration:  1375 epoch:  3\n","Same Different Classification loss: 0.5563035607337952 iteration:  1375 epoch:  3\n","Male Female Classification loss: 0.6225869655609131 iteration:  1375 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.9335744380950928 iteration:  1380 epoch:  3\n","LQ image reconstruction loss: 0.8698984980583191 iteration:  1380 epoch:  3\n","Same Different Classification loss: 0.6601617336273193 iteration:  1380 epoch:  3\n","Male Female Classification loss: 0.681633472442627 iteration:  1380 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.0975193977355957 iteration:  1385 epoch:  3\n","LQ image reconstruction loss: 0.6720812320709229 iteration:  1385 epoch:  3\n","Same Different Classification loss: 0.506325900554657 iteration:  1385 epoch:  3\n","Male Female Classification loss: 0.6731961965560913 iteration:  1385 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.0202760696411133 iteration:  1390 epoch:  3\n","LQ image reconstruction loss: 0.7256778478622437 iteration:  1390 epoch:  3\n","Same Different Classification loss: 0.5246922373771667 iteration:  1390 epoch:  3\n","Male Female Classification loss: 0.653249979019165 iteration:  1390 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.4247732162475586 iteration:  1395 epoch:  3\n","LQ image reconstruction loss: 0.6760765314102173 iteration:  1395 epoch:  3\n","Same Different Classification loss: 0.4966694712638855 iteration:  1395 epoch:  3\n","Male Female Classification loss: 0.6052963137626648 iteration:  1395 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.541038751602173 iteration:  1400 epoch:  3\n","LQ image reconstruction loss: 0.9100300073623657 iteration:  1400 epoch:  3\n","Same Different Classification loss: 0.7209020256996155 iteration:  1400 epoch:  3\n","Male Female Classification loss: 0.6299946308135986 iteration:  1400 epoch:  3\n","----------------------------------------------------\n","Accuracy of SD: 61 %\n","Accuracy of MF: 68 %\n","Mean Accuracy of SD: 58 %\n","Mean Accuracy of MF: 60 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.2546117305755615 iteration:  1405 epoch:  3\n","LQ image reconstruction loss: 0.8464624881744385 iteration:  1405 epoch:  3\n","Same Different Classification loss: 0.6383298635482788 iteration:  1405 epoch:  3\n","Male Female Classification loss: 0.6567331552505493 iteration:  1405 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.3919124603271484 iteration:  1410 epoch:  3\n","LQ image reconstruction loss: 0.8248022198677063 iteration:  1410 epoch:  3\n","Same Different Classification loss: 0.587176501750946 iteration:  1410 epoch:  3\n","Male Female Classification loss: 0.5836437344551086 iteration:  1410 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.721970796585083 iteration:  1415 epoch:  3\n","LQ image reconstruction loss: 0.7184260487556458 iteration:  1415 epoch:  3\n","Same Different Classification loss: 0.5230908393859863 iteration:  1415 epoch:  3\n","Male Female Classification loss: 0.6034226417541504 iteration:  1415 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.019102096557617 iteration:  1420 epoch:  3\n","LQ image reconstruction loss: 0.729923665523529 iteration:  1420 epoch:  3\n","Same Different Classification loss: 0.5494590997695923 iteration:  1420 epoch:  3\n","Male Female Classification loss: 0.58378005027771 iteration:  1420 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.831399440765381 iteration:  1425 epoch:  3\n","LQ image reconstruction loss: 0.8129846453666687 iteration:  1425 epoch:  3\n","Same Different Classification loss: 0.599666178226471 iteration:  1425 epoch:  3\n","Male Female Classification loss: 0.5987764596939087 iteration:  1425 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.5392813682556152 iteration:  1430 epoch:  3\n","LQ image reconstruction loss: 0.7773209810256958 iteration:  1430 epoch:  3\n","Same Different Classification loss: 0.5677874088287354 iteration:  1430 epoch:  3\n","Male Female Classification loss: 0.6411652565002441 iteration:  1430 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.676473379135132 iteration:  1435 epoch:  3\n","LQ image reconstruction loss: 0.895869791507721 iteration:  1435 epoch:  3\n","Same Different Classification loss: 0.67215895652771 iteration:  1435 epoch:  3\n","Male Female Classification loss: 0.6123917102813721 iteration:  1435 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.505255699157715 iteration:  1440 epoch:  3\n","LQ image reconstruction loss: 0.7309706211090088 iteration:  1440 epoch:  3\n","Same Different Classification loss: 0.5676104426383972 iteration:  1440 epoch:  3\n","Male Female Classification loss: 0.6900217533111572 iteration:  1440 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.6209933757781982 iteration:  1445 epoch:  3\n","LQ image reconstruction loss: 0.8179075717926025 iteration:  1445 epoch:  3\n","Same Different Classification loss: 0.627406656742096 iteration:  1445 epoch:  3\n","Male Female Classification loss: 0.6607202291488647 iteration:  1445 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.468947649002075 iteration:  1450 epoch:  3\n","LQ image reconstruction loss: 0.6926766633987427 iteration:  1450 epoch:  3\n","Same Different Classification loss: 0.529884397983551 iteration:  1450 epoch:  3\n","Male Female Classification loss: 0.7154046893119812 iteration:  1450 epoch:  3\n","----------------------------------------------------\n","Accuracy of SD: 67 %\n","Accuracy of MF: 68 %\n","Mean Accuracy of SD: 58 %\n","Mean Accuracy of MF: 60 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.132044792175293 iteration:  1455 epoch:  3\n","LQ image reconstruction loss: 0.8237051963806152 iteration:  1455 epoch:  3\n","Same Different Classification loss: 0.6565584540367126 iteration:  1455 epoch:  3\n","Male Female Classification loss: 0.5984205007553101 iteration:  1455 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.995846748352051 iteration:  1460 epoch:  3\n","LQ image reconstruction loss: 0.7281450033187866 iteration:  1460 epoch:  3\n","Same Different Classification loss: 0.5608472228050232 iteration:  1460 epoch:  3\n","Male Female Classification loss: 0.6847145557403564 iteration:  1460 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.624725818634033 iteration:  1465 epoch:  3\n","LQ image reconstruction loss: 0.8281713724136353 iteration:  1465 epoch:  3\n","Same Different Classification loss: 0.6403684020042419 iteration:  1465 epoch:  3\n","Male Female Classification loss: 0.6987236738204956 iteration:  1465 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.012669801712036 iteration:  1470 epoch:  3\n","LQ image reconstruction loss: 0.8237482309341431 iteration:  1470 epoch:  3\n","Same Different Classification loss: 0.6110994815826416 iteration:  1470 epoch:  3\n","Male Female Classification loss: 0.6485191583633423 iteration:  1470 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.8933682441711426 iteration:  1475 epoch:  3\n","LQ image reconstruction loss: 0.7895025014877319 iteration:  1475 epoch:  3\n","Same Different Classification loss: 0.6100571155548096 iteration:  1475 epoch:  3\n","Male Female Classification loss: 0.6601583361625671 iteration:  1475 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.42348051071167 iteration:  1480 epoch:  3\n","LQ image reconstruction loss: 0.7551085948944092 iteration:  1480 epoch:  3\n","Same Different Classification loss: 0.5636634230613708 iteration:  1480 epoch:  3\n","Male Female Classification loss: 0.6472890973091125 iteration:  1480 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.6778557300567627 iteration:  1485 epoch:  3\n","LQ image reconstruction loss: 0.7713466882705688 iteration:  1485 epoch:  3\n","Same Different Classification loss: 0.5615739226341248 iteration:  1485 epoch:  3\n","Male Female Classification loss: 0.6671968698501587 iteration:  1485 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.275841474533081 iteration:  1490 epoch:  3\n","LQ image reconstruction loss: 0.7587510347366333 iteration:  1490 epoch:  3\n","Same Different Classification loss: 0.5845854878425598 iteration:  1490 epoch:  3\n","Male Female Classification loss: 0.6669738292694092 iteration:  1490 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.5176899433135986 iteration:  1495 epoch:  3\n","LQ image reconstruction loss: 0.8083562254905701 iteration:  1495 epoch:  3\n","Same Different Classification loss: 0.6248635649681091 iteration:  1495 epoch:  3\n","Male Female Classification loss: 0.5824010372161865 iteration:  1495 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.447279214859009 iteration:  1500 epoch:  3\n","LQ image reconstruction loss: 0.7168514728546143 iteration:  1500 epoch:  3\n","Same Different Classification loss: 0.5448973178863525 iteration:  1500 epoch:  3\n","Male Female Classification loss: 0.6381564140319824 iteration:  1500 epoch:  3\n","----------------------------------------------------\n","Accuracy of SD: 73 %\n","Accuracy of MF: 52 %\n","Mean Accuracy of SD: 58 %\n","Mean Accuracy of MF: 60 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.3339598178863525 iteration:  1505 epoch:  3\n","LQ image reconstruction loss: 0.7770589590072632 iteration:  1505 epoch:  3\n","Same Different Classification loss: 0.598557710647583 iteration:  1505 epoch:  3\n","Male Female Classification loss: 0.6700100898742676 iteration:  1505 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.0066986083984375 iteration:  1510 epoch:  3\n","LQ image reconstruction loss: 0.7656756639480591 iteration:  1510 epoch:  3\n","Same Different Classification loss: 0.5518940091133118 iteration:  1510 epoch:  3\n","Male Female Classification loss: 0.6510647535324097 iteration:  1510 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.172461748123169 iteration:  1515 epoch:  3\n","LQ image reconstruction loss: 0.8237640857696533 iteration:  1515 epoch:  3\n","Same Different Classification loss: 0.610902726650238 iteration:  1515 epoch:  3\n","Male Female Classification loss: 0.6819972991943359 iteration:  1515 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.6978421211242676 iteration:  1520 epoch:  3\n","LQ image reconstruction loss: 0.7770607471466064 iteration:  1520 epoch:  3\n","Same Different Classification loss: 0.6195526719093323 iteration:  1520 epoch:  3\n","Male Female Classification loss: 0.6466129422187805 iteration:  1520 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.3639445304870605 iteration:  1525 epoch:  3\n","LQ image reconstruction loss: 0.8030784130096436 iteration:  1525 epoch:  3\n","Same Different Classification loss: 0.6396977305412292 iteration:  1525 epoch:  3\n","Male Female Classification loss: 0.6535903215408325 iteration:  1525 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.161076545715332 iteration:  1530 epoch:  3\n","LQ image reconstruction loss: 0.7483211755752563 iteration:  1530 epoch:  3\n","Same Different Classification loss: 0.5576052069664001 iteration:  1530 epoch:  3\n","Male Female Classification loss: 0.6730005145072937 iteration:  1530 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.1896607875823975 iteration:  1535 epoch:  3\n","LQ image reconstruction loss: 0.8685699701309204 iteration:  1535 epoch:  3\n","Same Different Classification loss: 0.6572959423065186 iteration:  1535 epoch:  3\n","Male Female Classification loss: 0.5535825490951538 iteration:  1535 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.248699188232422 iteration:  1540 epoch:  3\n","LQ image reconstruction loss: 0.6604958772659302 iteration:  1540 epoch:  3\n","Same Different Classification loss: 0.5054243803024292 iteration:  1540 epoch:  3\n","Male Female Classification loss: 0.7337406277656555 iteration:  1540 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.015148162841797 iteration:  1545 epoch:  3\n","LQ image reconstruction loss: 0.8923326134681702 iteration:  1545 epoch:  3\n","Same Different Classification loss: 0.6935375928878784 iteration:  1545 epoch:  3\n","Male Female Classification loss: 0.6535647511482239 iteration:  1545 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.2413766384124756 iteration:  1550 epoch:  3\n","LQ image reconstruction loss: 0.794463038444519 iteration:  1550 epoch:  3\n","Same Different Classification loss: 0.5811247825622559 iteration:  1550 epoch:  3\n","Male Female Classification loss: 0.6221649646759033 iteration:  1550 epoch:  3\n","----------------------------------------------------\n","Accuracy of SD: 69 %\n","Accuracy of MF: 67 %\n","Mean Accuracy of SD: 58 %\n","Mean Accuracy of MF: 60 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.3033976554870605 iteration:  1555 epoch:  3\n","LQ image reconstruction loss: 0.7740088701248169 iteration:  1555 epoch:  3\n","Same Different Classification loss: 0.5768731832504272 iteration:  1555 epoch:  3\n","Male Female Classification loss: 0.6523773074150085 iteration:  1555 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.2208824157714844 iteration:  1560 epoch:  3\n","LQ image reconstruction loss: 0.7326297760009766 iteration:  1560 epoch:  3\n","Same Different Classification loss: 0.5656746029853821 iteration:  1560 epoch:  3\n","Male Female Classification loss: 0.6348704099655151 iteration:  1560 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.315765380859375 iteration:  1565 epoch:  3\n","LQ image reconstruction loss: 0.7225347757339478 iteration:  1565 epoch:  3\n","Same Different Classification loss: 0.5731622576713562 iteration:  1565 epoch:  3\n","Male Female Classification loss: 0.6209889650344849 iteration:  1565 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.2976739406585693 iteration:  1570 epoch:  3\n","LQ image reconstruction loss: 0.7753214836120605 iteration:  1570 epoch:  3\n","Same Different Classification loss: 0.5849225521087646 iteration:  1570 epoch:  3\n","Male Female Classification loss: 0.6714164614677429 iteration:  1570 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.298703670501709 iteration:  1575 epoch:  3\n","LQ image reconstruction loss: 0.8736256957054138 iteration:  1575 epoch:  3\n","Same Different Classification loss: 0.6848822832107544 iteration:  1575 epoch:  3\n","Male Female Classification loss: 0.6520427465438843 iteration:  1575 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.932219982147217 iteration:  1580 epoch:  3\n","LQ image reconstruction loss: 0.7304686903953552 iteration:  1580 epoch:  3\n","Same Different Classification loss: 0.5423868298530579 iteration:  1580 epoch:  3\n","Male Female Classification loss: 0.6100766658782959 iteration:  1580 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.0708396434783936 iteration:  1585 epoch:  3\n","LQ image reconstruction loss: 0.7430942058563232 iteration:  1585 epoch:  3\n","Same Different Classification loss: 0.5701935887336731 iteration:  1585 epoch:  3\n","Male Female Classification loss: 0.6376032829284668 iteration:  1585 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.371267795562744 iteration:  1590 epoch:  3\n","LQ image reconstruction loss: 0.7194042205810547 iteration:  1590 epoch:  3\n","Same Different Classification loss: 0.5612976551055908 iteration:  1590 epoch:  3\n","Male Female Classification loss: 0.6645894646644592 iteration:  1590 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.994617462158203 iteration:  1595 epoch:  3\n","LQ image reconstruction loss: 0.6949145793914795 iteration:  1595 epoch:  3\n","Same Different Classification loss: 0.5318894386291504 iteration:  1595 epoch:  3\n","Male Female Classification loss: 0.6298237442970276 iteration:  1595 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.9655351638793945 iteration:  1600 epoch:  3\n","LQ image reconstruction loss: 0.9218391180038452 iteration:  1600 epoch:  3\n","Same Different Classification loss: 0.6811449527740479 iteration:  1600 epoch:  3\n","Male Female Classification loss: 0.6214287281036377 iteration:  1600 epoch:  3\n","----------------------------------------------------\n","Accuracy of SD: 69 %\n","Accuracy of MF: 58 %\n","Mean Accuracy of SD: 58 %\n","Mean Accuracy of MF: 60 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.9474220275878906 iteration:  1605 epoch:  3\n","LQ image reconstruction loss: 0.6755751967430115 iteration:  1605 epoch:  3\n","Same Different Classification loss: 0.4868170917034149 iteration:  1605 epoch:  3\n","Male Female Classification loss: 0.5890547037124634 iteration:  1605 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.575869083404541 iteration:  1610 epoch:  3\n","LQ image reconstruction loss: 0.7127816677093506 iteration:  1610 epoch:  3\n","Same Different Classification loss: 0.5449361205101013 iteration:  1610 epoch:  3\n","Male Female Classification loss: 0.6404452323913574 iteration:  1610 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.699380397796631 iteration:  1615 epoch:  3\n","LQ image reconstruction loss: 0.8085442781448364 iteration:  1615 epoch:  3\n","Same Different Classification loss: 0.5822409987449646 iteration:  1615 epoch:  3\n","Male Female Classification loss: 0.5922863483428955 iteration:  1615 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.773721933364868 iteration:  1620 epoch:  3\n","LQ image reconstruction loss: 0.8248105049133301 iteration:  1620 epoch:  3\n","Same Different Classification loss: 0.621398389339447 iteration:  1620 epoch:  3\n","Male Female Classification loss: 0.6473392248153687 iteration:  1620 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.179614543914795 iteration:  1625 epoch:  3\n","LQ image reconstruction loss: 0.7586147785186768 iteration:  1625 epoch:  3\n","Same Different Classification loss: 0.5911441445350647 iteration:  1625 epoch:  3\n","Male Female Classification loss: 0.6278528571128845 iteration:  1625 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.280646800994873 iteration:  1630 epoch:  3\n","LQ image reconstruction loss: 0.7023638486862183 iteration:  1630 epoch:  3\n","Same Different Classification loss: 0.5451852083206177 iteration:  1630 epoch:  3\n","Male Female Classification loss: 0.5929300785064697 iteration:  1630 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.3533365726470947 iteration:  1635 epoch:  3\n","LQ image reconstruction loss: 0.686345100402832 iteration:  1635 epoch:  3\n","Same Different Classification loss: 0.5590797066688538 iteration:  1635 epoch:  3\n","Male Female Classification loss: 0.6577197313308716 iteration:  1635 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.4437923431396484 iteration:  1640 epoch:  3\n","LQ image reconstruction loss: 0.747753381729126 iteration:  1640 epoch:  3\n","Same Different Classification loss: 0.5211946368217468 iteration:  1640 epoch:  3\n","Male Female Classification loss: 0.6032723188400269 iteration:  1640 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.370121479034424 iteration:  1645 epoch:  3\n","LQ image reconstruction loss: 0.7161870002746582 iteration:  1645 epoch:  3\n","Same Different Classification loss: 0.5375812649726868 iteration:  1645 epoch:  3\n","Male Female Classification loss: 0.5900311470031738 iteration:  1645 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.8261077404022217 iteration:  1650 epoch:  3\n","LQ image reconstruction loss: 0.730924665927887 iteration:  1650 epoch:  3\n","Same Different Classification loss: 0.5406497716903687 iteration:  1650 epoch:  3\n","Male Female Classification loss: 0.6257851123809814 iteration:  1650 epoch:  3\n","----------------------------------------------------\n","Accuracy of SD: 70 %\n","Accuracy of MF: 70 %\n","Mean Accuracy of SD: 58 %\n","Mean Accuracy of MF: 60 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.357668876647949 iteration:  1655 epoch:  3\n","LQ image reconstruction loss: 0.652626633644104 iteration:  1655 epoch:  3\n","Same Different Classification loss: 0.5318752527236938 iteration:  1655 epoch:  3\n","Male Female Classification loss: 0.6091690063476562 iteration:  1655 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.3129401206970215 iteration:  1660 epoch:  3\n","LQ image reconstruction loss: 0.7062231302261353 iteration:  1660 epoch:  3\n","Same Different Classification loss: 0.5152398943901062 iteration:  1660 epoch:  3\n","Male Female Classification loss: 0.593711256980896 iteration:  1660 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.2629570960998535 iteration:  1665 epoch:  3\n","LQ image reconstruction loss: 0.64968341588974 iteration:  1665 epoch:  3\n","Same Different Classification loss: 0.4708511233329773 iteration:  1665 epoch:  3\n","Male Female Classification loss: 0.6630101799964905 iteration:  1665 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.1933810710906982 iteration:  1670 epoch:  3\n","LQ image reconstruction loss: 0.7662683129310608 iteration:  1670 epoch:  3\n","Same Different Classification loss: 0.6206510663032532 iteration:  1670 epoch:  3\n","Male Female Classification loss: 0.6315980553627014 iteration:  1670 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.016425848007202 iteration:  1675 epoch:  3\n","LQ image reconstruction loss: 0.7713377475738525 iteration:  1675 epoch:  3\n","Same Different Classification loss: 0.595749020576477 iteration:  1675 epoch:  3\n","Male Female Classification loss: 0.63768470287323 iteration:  1675 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.293393850326538 iteration:  1680 epoch:  3\n","LQ image reconstruction loss: 0.6532325148582458 iteration:  1680 epoch:  3\n","Same Different Classification loss: 0.4965341091156006 iteration:  1680 epoch:  3\n","Male Female Classification loss: 0.6559842824935913 iteration:  1680 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.602599620819092 iteration:  1685 epoch:  3\n","LQ image reconstruction loss: 0.7451593279838562 iteration:  1685 epoch:  3\n","Same Different Classification loss: 0.5465700030326843 iteration:  1685 epoch:  3\n","Male Female Classification loss: 0.5996718406677246 iteration:  1685 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.8007402420043945 iteration:  1690 epoch:  3\n","LQ image reconstruction loss: 0.7008211016654968 iteration:  1690 epoch:  3\n","Same Different Classification loss: 0.5691412091255188 iteration:  1690 epoch:  3\n","Male Female Classification loss: 0.6569543480873108 iteration:  1690 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.313222885131836 iteration:  1695 epoch:  3\n","LQ image reconstruction loss: 0.6640002131462097 iteration:  1695 epoch:  3\n","Same Different Classification loss: 0.5035282373428345 iteration:  1695 epoch:  3\n","Male Female Classification loss: 0.6796273589134216 iteration:  1695 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.5684030055999756 iteration:  1700 epoch:  3\n","LQ image reconstruction loss: 0.7421548366546631 iteration:  1700 epoch:  3\n","Same Different Classification loss: 0.5979946851730347 iteration:  1700 epoch:  3\n","Male Female Classification loss: 0.638120710849762 iteration:  1700 epoch:  3\n","----------------------------------------------------\n","Accuracy of SD: 74 %\n","Accuracy of MF: 64 %\n","Mean Accuracy of SD: 58 %\n","Mean Accuracy of MF: 60 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.8069565296173096 iteration:  1705 epoch:  3\n","LQ image reconstruction loss: 0.7231016755104065 iteration:  1705 epoch:  3\n","Same Different Classification loss: 0.5386905074119568 iteration:  1705 epoch:  3\n","Male Female Classification loss: 0.6326150894165039 iteration:  1705 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.9178271293640137 iteration:  1710 epoch:  3\n","LQ image reconstruction loss: 0.7765870094299316 iteration:  1710 epoch:  3\n","Same Different Classification loss: 0.6069849729537964 iteration:  1710 epoch:  3\n","Male Female Classification loss: 0.7298063635826111 iteration:  1710 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.2596435546875 iteration:  1715 epoch:  3\n","LQ image reconstruction loss: 0.7151939868927002 iteration:  1715 epoch:  3\n","Same Different Classification loss: 0.5361042618751526 iteration:  1715 epoch:  3\n","Male Female Classification loss: 0.6545162200927734 iteration:  1715 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.9045865535736084 iteration:  1720 epoch:  3\n","LQ image reconstruction loss: 0.8120003342628479 iteration:  1720 epoch:  3\n","Same Different Classification loss: 0.5940564870834351 iteration:  1720 epoch:  3\n","Male Female Classification loss: 0.6033735275268555 iteration:  1720 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.7926583290100098 iteration:  1725 epoch:  3\n","LQ image reconstruction loss: 0.7671982645988464 iteration:  1725 epoch:  3\n","Same Different Classification loss: 0.5886684656143188 iteration:  1725 epoch:  3\n","Male Female Classification loss: 0.6198478937149048 iteration:  1725 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.7043192386627197 iteration:  1730 epoch:  3\n","LQ image reconstruction loss: 0.6896973848342896 iteration:  1730 epoch:  3\n","Same Different Classification loss: 0.5512805581092834 iteration:  1730 epoch:  3\n","Male Female Classification loss: 0.6196630597114563 iteration:  1730 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.870220184326172 iteration:  1735 epoch:  3\n","LQ image reconstruction loss: 0.7566744089126587 iteration:  1735 epoch:  3\n","Same Different Classification loss: 0.5746847987174988 iteration:  1735 epoch:  3\n","Male Female Classification loss: 0.6405547261238098 iteration:  1735 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.741109848022461 iteration:  1740 epoch:  3\n","LQ image reconstruction loss: 0.6914836168289185 iteration:  1740 epoch:  3\n","Same Different Classification loss: 0.5239307284355164 iteration:  1740 epoch:  3\n","Male Female Classification loss: 0.6374627351760864 iteration:  1740 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.2496001720428467 iteration:  1745 epoch:  3\n","LQ image reconstruction loss: 0.7595726251602173 iteration:  1745 epoch:  3\n","Same Different Classification loss: 0.5762694478034973 iteration:  1745 epoch:  3\n","Male Female Classification loss: 0.6010048985481262 iteration:  1745 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.179769277572632 iteration:  1750 epoch:  3\n","LQ image reconstruction loss: 0.847252607345581 iteration:  1750 epoch:  3\n","Same Different Classification loss: 0.6317794919013977 iteration:  1750 epoch:  3\n","Male Female Classification loss: 0.6397722959518433 iteration:  1750 epoch:  3\n","----------------------------------------------------\n","Accuracy of SD: 74 %\n","Accuracy of MF: 63 %\n","Mean Accuracy of SD: 58 %\n","Mean Accuracy of MF: 60 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.9035024642944336 iteration:  1755 epoch:  3\n","LQ image reconstruction loss: 0.7195598483085632 iteration:  1755 epoch:  3\n","Same Different Classification loss: 0.5631694197654724 iteration:  1755 epoch:  3\n","Male Female Classification loss: 0.6283785104751587 iteration:  1755 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.2418715953826904 iteration:  1760 epoch:  3\n","LQ image reconstruction loss: 0.7367091774940491 iteration:  1760 epoch:  3\n","Same Different Classification loss: 0.5460504293441772 iteration:  1760 epoch:  3\n","Male Female Classification loss: 0.6322331428527832 iteration:  1760 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.3479793071746826 iteration:  1765 epoch:  3\n","LQ image reconstruction loss: 0.7034034132957458 iteration:  1765 epoch:  3\n","Same Different Classification loss: 0.5188088417053223 iteration:  1765 epoch:  3\n","Male Female Classification loss: 0.5791885852813721 iteration:  1765 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.7662689685821533 iteration:  1770 epoch:  3\n","LQ image reconstruction loss: 0.6932483315467834 iteration:  1770 epoch:  3\n","Same Different Classification loss: 0.5391439199447632 iteration:  1770 epoch:  3\n","Male Female Classification loss: 0.6601889133453369 iteration:  1770 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.508406162261963 iteration:  1775 epoch:  3\n","LQ image reconstruction loss: 0.7227170467376709 iteration:  1775 epoch:  3\n","Same Different Classification loss: 0.5648530721664429 iteration:  1775 epoch:  3\n","Male Female Classification loss: 0.6335917115211487 iteration:  1775 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 4.0111403465271 iteration:  1780 epoch:  3\n","LQ image reconstruction loss: 0.688528299331665 iteration:  1780 epoch:  3\n","Same Different Classification loss: 0.5307736396789551 iteration:  1780 epoch:  3\n","Male Female Classification loss: 0.7272101044654846 iteration:  1780 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.543720245361328 iteration:  1785 epoch:  3\n","LQ image reconstruction loss: 0.753421425819397 iteration:  1785 epoch:  3\n","Same Different Classification loss: 0.561988890171051 iteration:  1785 epoch:  3\n","Male Female Classification loss: 0.5741358995437622 iteration:  1785 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.428696632385254 iteration:  1790 epoch:  3\n","LQ image reconstruction loss: 0.6430176496505737 iteration:  1790 epoch:  3\n","Same Different Classification loss: 0.5070815086364746 iteration:  1790 epoch:  3\n","Male Female Classification loss: 0.6811079978942871 iteration:  1790 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 4.248226642608643 iteration:  1795 epoch:  3\n","LQ image reconstruction loss: 0.7931677103042603 iteration:  1795 epoch:  3\n","Same Different Classification loss: 0.6030818819999695 iteration:  1795 epoch:  3\n","Male Female Classification loss: 0.6442829966545105 iteration:  1795 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.5415360927581787 iteration:  1800 epoch:  3\n","LQ image reconstruction loss: 0.7884974479675293 iteration:  1800 epoch:  3\n","Same Different Classification loss: 0.6574996113777161 iteration:  1800 epoch:  3\n","Male Female Classification loss: 0.6359283924102783 iteration:  1800 epoch:  3\n","----------------------------------------------------\n","Accuracy of SD: 71 %\n","Accuracy of MF: 60 %\n","Mean Accuracy of SD: 58 %\n","Mean Accuracy of MF: 60 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.4471275806427 iteration:  1805 epoch:  3\n","LQ image reconstruction loss: 0.7214714288711548 iteration:  1805 epoch:  3\n","Same Different Classification loss: 0.5717047452926636 iteration:  1805 epoch:  3\n","Male Female Classification loss: 0.6362980008125305 iteration:  1805 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.4820756912231445 iteration:  1810 epoch:  3\n","LQ image reconstruction loss: 0.8149559497833252 iteration:  1810 epoch:  3\n","Same Different Classification loss: 0.6137996315956116 iteration:  1810 epoch:  3\n","Male Female Classification loss: 0.6017156839370728 iteration:  1810 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.7691006660461426 iteration:  1815 epoch:  3\n","LQ image reconstruction loss: 0.7602085471153259 iteration:  1815 epoch:  3\n","Same Different Classification loss: 0.5747044682502747 iteration:  1815 epoch:  3\n","Male Female Classification loss: 0.6126620173454285 iteration:  1815 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.806802272796631 iteration:  1820 epoch:  3\n","LQ image reconstruction loss: 0.6521292924880981 iteration:  1820 epoch:  3\n","Same Different Classification loss: 0.5288642048835754 iteration:  1820 epoch:  3\n","Male Female Classification loss: 0.677888035774231 iteration:  1820 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.346078872680664 iteration:  1825 epoch:  3\n","LQ image reconstruction loss: 0.75176602602005 iteration:  1825 epoch:  3\n","Same Different Classification loss: 0.5427001714706421 iteration:  1825 epoch:  3\n","Male Female Classification loss: 0.5980108380317688 iteration:  1825 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.1739470958709717 iteration:  1830 epoch:  3\n","LQ image reconstruction loss: 0.8408820033073425 iteration:  1830 epoch:  3\n","Same Different Classification loss: 0.6030645966529846 iteration:  1830 epoch:  3\n","Male Female Classification loss: 0.5592538714408875 iteration:  1830 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.0660922527313232 iteration:  1835 epoch:  3\n","LQ image reconstruction loss: 0.6756833791732788 iteration:  1835 epoch:  3\n","Same Different Classification loss: 0.5074213147163391 iteration:  1835 epoch:  3\n","Male Female Classification loss: 0.6558072566986084 iteration:  1835 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.943990468978882 iteration:  1840 epoch:  3\n","LQ image reconstruction loss: 0.7609090805053711 iteration:  1840 epoch:  3\n","Same Different Classification loss: 0.5916587710380554 iteration:  1840 epoch:  3\n","Male Female Classification loss: 0.6083595752716064 iteration:  1840 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.2095534801483154 iteration:  1845 epoch:  3\n","LQ image reconstruction loss: 0.7429914474487305 iteration:  1845 epoch:  3\n","Same Different Classification loss: 0.6082679629325867 iteration:  1845 epoch:  3\n","Male Female Classification loss: 0.6640985012054443 iteration:  1845 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.7413415908813477 iteration:  1850 epoch:  3\n","LQ image reconstruction loss: 0.7138671278953552 iteration:  1850 epoch:  3\n","Same Different Classification loss: 0.5886930823326111 iteration:  1850 epoch:  3\n","Male Female Classification loss: 0.6079747080802917 iteration:  1850 epoch:  3\n","----------------------------------------------------\n","Accuracy of SD: 67 %\n","Accuracy of MF: 67 %\n","Mean Accuracy of SD: 58 %\n","Mean Accuracy of MF: 60 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.2922136783599854 iteration:  1855 epoch:  3\n","LQ image reconstruction loss: 0.7089669108390808 iteration:  1855 epoch:  3\n","Same Different Classification loss: 0.5176213979721069 iteration:  1855 epoch:  3\n","Male Female Classification loss: 0.5714865326881409 iteration:  1855 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.672818660736084 iteration:  1860 epoch:  3\n","LQ image reconstruction loss: 0.7414876818656921 iteration:  1860 epoch:  3\n","Same Different Classification loss: 0.5446394085884094 iteration:  1860 epoch:  3\n","Male Female Classification loss: 0.6080362200737 iteration:  1860 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.839078187942505 iteration:  1865 epoch:  3\n","LQ image reconstruction loss: 0.7328529357910156 iteration:  1865 epoch:  3\n","Same Different Classification loss: 0.6142513751983643 iteration:  1865 epoch:  3\n","Male Female Classification loss: 0.6554945111274719 iteration:  1865 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.434596538543701 iteration:  1870 epoch:  3\n","LQ image reconstruction loss: 0.6662454605102539 iteration:  1870 epoch:  3\n","Same Different Classification loss: 0.5617663264274597 iteration:  1870 epoch:  3\n","Male Female Classification loss: 0.68462073802948 iteration:  1870 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.2162790298461914 iteration:  1875 epoch:  3\n","LQ image reconstruction loss: 0.6768208742141724 iteration:  1875 epoch:  3\n","Same Different Classification loss: 0.5072472095489502 iteration:  1875 epoch:  3\n","Male Female Classification loss: 0.6189587116241455 iteration:  1875 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.402463674545288 iteration:  1880 epoch:  3\n","LQ image reconstruction loss: 0.7264799475669861 iteration:  1880 epoch:  3\n","Same Different Classification loss: 0.5726590156555176 iteration:  1880 epoch:  3\n","Male Female Classification loss: 0.6885535717010498 iteration:  1880 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.9992175102233887 iteration:  1885 epoch:  3\n","LQ image reconstruction loss: 0.6857492923736572 iteration:  1885 epoch:  3\n","Same Different Classification loss: 0.523786723613739 iteration:  1885 epoch:  3\n","Male Female Classification loss: 0.6566647291183472 iteration:  1885 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.2227323055267334 iteration:  1890 epoch:  3\n","LQ image reconstruction loss: 0.7603724002838135 iteration:  1890 epoch:  3\n","Same Different Classification loss: 0.5683715343475342 iteration:  1890 epoch:  3\n","Male Female Classification loss: 0.6410746574401855 iteration:  1890 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.2475976943969727 iteration:  1895 epoch:  3\n","LQ image reconstruction loss: 0.7559405565261841 iteration:  1895 epoch:  3\n","Same Different Classification loss: 0.5563679933547974 iteration:  1895 epoch:  3\n","Male Female Classification loss: 0.6582080721855164 iteration:  1895 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.109942674636841 iteration:  1900 epoch:  3\n","LQ image reconstruction loss: 0.6971448063850403 iteration:  1900 epoch:  3\n","Same Different Classification loss: 0.520814061164856 iteration:  1900 epoch:  3\n","Male Female Classification loss: 0.590066134929657 iteration:  1900 epoch:  3\n","----------------------------------------------------\n","Accuracy of SD: 74 %\n","Accuracy of MF: 69 %\n","Mean Accuracy of SD: 58 %\n","Mean Accuracy of MF: 60 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.071536064147949 iteration:  1905 epoch:  3\n","LQ image reconstruction loss: 0.762068510055542 iteration:  1905 epoch:  3\n","Same Different Classification loss: 0.5861848592758179 iteration:  1905 epoch:  3\n","Male Female Classification loss: 0.6442772150039673 iteration:  1905 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.024986743927002 iteration:  1910 epoch:  3\n","LQ image reconstruction loss: 0.6881077289581299 iteration:  1910 epoch:  3\n","Same Different Classification loss: 0.5548734068870544 iteration:  1910 epoch:  3\n","Male Female Classification loss: 0.6124213933944702 iteration:  1910 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.826727867126465 iteration:  1915 epoch:  3\n","LQ image reconstruction loss: 0.7946147322654724 iteration:  1915 epoch:  3\n","Same Different Classification loss: 0.5800626873970032 iteration:  1915 epoch:  3\n","Male Female Classification loss: 0.6010692119598389 iteration:  1915 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.0016536712646484 iteration:  1920 epoch:  3\n","LQ image reconstruction loss: 0.6558184027671814 iteration:  1920 epoch:  3\n","Same Different Classification loss: 0.532931387424469 iteration:  1920 epoch:  3\n","Male Female Classification loss: 0.6425163149833679 iteration:  1920 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.775780439376831 iteration:  1925 epoch:  3\n","LQ image reconstruction loss: 0.6581437587738037 iteration:  1925 epoch:  3\n","Same Different Classification loss: 0.5031921863555908 iteration:  1925 epoch:  3\n","Male Female Classification loss: 0.6699584722518921 iteration:  1925 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.462515115737915 iteration:  1930 epoch:  3\n","LQ image reconstruction loss: 0.7570455074310303 iteration:  1930 epoch:  3\n","Same Different Classification loss: 0.6201159954071045 iteration:  1930 epoch:  3\n","Male Female Classification loss: 0.6738095879554749 iteration:  1930 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.3724355697631836 iteration:  1935 epoch:  3\n","LQ image reconstruction loss: 0.8337674736976624 iteration:  1935 epoch:  3\n","Same Different Classification loss: 0.6408160924911499 iteration:  1935 epoch:  3\n","Male Female Classification loss: 0.5486856698989868 iteration:  1935 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.73449444770813 iteration:  1940 epoch:  3\n","LQ image reconstruction loss: 0.6881078481674194 iteration:  1940 epoch:  3\n","Same Different Classification loss: 0.5807214379310608 iteration:  1940 epoch:  3\n","Male Female Classification loss: 0.6726556420326233 iteration:  1940 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.8786134719848633 iteration:  1945 epoch:  3\n","LQ image reconstruction loss: 0.6952691078186035 iteration:  1945 epoch:  3\n","Same Different Classification loss: 0.5287095904350281 iteration:  1945 epoch:  3\n","Male Female Classification loss: 0.6287394762039185 iteration:  1945 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.396127939224243 iteration:  1950 epoch:  3\n","LQ image reconstruction loss: 0.6367896199226379 iteration:  1950 epoch:  3\n","Same Different Classification loss: 0.5620912313461304 iteration:  1950 epoch:  3\n","Male Female Classification loss: 0.709870457649231 iteration:  1950 epoch:  3\n","----------------------------------------------------\n","Accuracy of SD: 73 %\n","Accuracy of MF: 69 %\n","Mean Accuracy of SD: 58 %\n","Mean Accuracy of MF: 60 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.1064321994781494 iteration:  1955 epoch:  3\n","LQ image reconstruction loss: 0.6863468885421753 iteration:  1955 epoch:  3\n","Same Different Classification loss: 0.5773757100105286 iteration:  1955 epoch:  3\n","Male Female Classification loss: 0.6490596532821655 iteration:  1955 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.3632240295410156 iteration:  1960 epoch:  3\n","LQ image reconstruction loss: 0.7928646802902222 iteration:  1960 epoch:  3\n","Same Different Classification loss: 0.6183874607086182 iteration:  1960 epoch:  3\n","Male Female Classification loss: 0.6647216081619263 iteration:  1960 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.423649787902832 iteration:  1965 epoch:  3\n","LQ image reconstruction loss: 0.7515573501586914 iteration:  1965 epoch:  3\n","Same Different Classification loss: 0.5952923893928528 iteration:  1965 epoch:  3\n","Male Female Classification loss: 0.6022936701774597 iteration:  1965 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.057969570159912 iteration:  1970 epoch:  3\n","LQ image reconstruction loss: 0.764773964881897 iteration:  1970 epoch:  3\n","Same Different Classification loss: 0.638235867023468 iteration:  1970 epoch:  3\n","Male Female Classification loss: 0.5696083903312683 iteration:  1970 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.2487616539001465 iteration:  1975 epoch:  3\n","LQ image reconstruction loss: 0.6904608011245728 iteration:  1975 epoch:  3\n","Same Different Classification loss: 0.5813305974006653 iteration:  1975 epoch:  3\n","Male Female Classification loss: 0.6588256359100342 iteration:  1975 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.2603368759155273 iteration:  1980 epoch:  3\n","LQ image reconstruction loss: 0.7794022560119629 iteration:  1980 epoch:  3\n","Same Different Classification loss: 0.6066462397575378 iteration:  1980 epoch:  3\n","Male Female Classification loss: 0.6079346537590027 iteration:  1980 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.3248984813690186 iteration:  1985 epoch:  3\n","LQ image reconstruction loss: 0.6604422330856323 iteration:  1985 epoch:  3\n","Same Different Classification loss: 0.52793288230896 iteration:  1985 epoch:  3\n","Male Female Classification loss: 0.6606181859970093 iteration:  1985 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.929090976715088 iteration:  1990 epoch:  3\n","LQ image reconstruction loss: 0.6820394396781921 iteration:  1990 epoch:  3\n","Same Different Classification loss: 0.5256295800209045 iteration:  1990 epoch:  3\n","Male Female Classification loss: 0.6081117391586304 iteration:  1990 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.430633783340454 iteration:  1995 epoch:  3\n","LQ image reconstruction loss: 0.7567441463470459 iteration:  1995 epoch:  3\n","Same Different Classification loss: 0.592846691608429 iteration:  1995 epoch:  3\n","Male Female Classification loss: 0.6294088959693909 iteration:  1995 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.85254168510437 iteration:  2000 epoch:  3\n","LQ image reconstruction loss: 0.5701210498809814 iteration:  2000 epoch:  3\n","Same Different Classification loss: 0.4718313217163086 iteration:  2000 epoch:  3\n","Male Female Classification loss: 0.6205912232398987 iteration:  2000 epoch:  3\n","----------------------------------------------------\n","Accuracy of SD: 75 %\n","Accuracy of MF: 64 %\n","Mean Accuracy of SD: 58 %\n","Mean Accuracy of MF: 60 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.3160786628723145 iteration:  2005 epoch:  3\n","LQ image reconstruction loss: 0.7269473075866699 iteration:  2005 epoch:  3\n","Same Different Classification loss: 0.570746123790741 iteration:  2005 epoch:  3\n","Male Female Classification loss: 0.626589834690094 iteration:  2005 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.236207962036133 iteration:  2010 epoch:  3\n","LQ image reconstruction loss: 0.665394127368927 iteration:  2010 epoch:  3\n","Same Different Classification loss: 0.5019098520278931 iteration:  2010 epoch:  3\n","Male Female Classification loss: 0.57207852602005 iteration:  2010 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.657806634902954 iteration:  2015 epoch:  3\n","LQ image reconstruction loss: 0.5934818387031555 iteration:  2015 epoch:  3\n","Same Different Classification loss: 0.5099665522575378 iteration:  2015 epoch:  3\n","Male Female Classification loss: 0.6852708458900452 iteration:  2015 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.1419131755828857 iteration:  2020 epoch:  3\n","LQ image reconstruction loss: 0.6065680980682373 iteration:  2020 epoch:  3\n","Same Different Classification loss: 0.5108690857887268 iteration:  2020 epoch:  3\n","Male Female Classification loss: 0.658481240272522 iteration:  2020 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.3350093364715576 iteration:  2025 epoch:  3\n","LQ image reconstruction loss: 0.6916383504867554 iteration:  2025 epoch:  3\n","Same Different Classification loss: 0.538836658000946 iteration:  2025 epoch:  3\n","Male Female Classification loss: 0.5653015375137329 iteration:  2025 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.9111666679382324 iteration:  2030 epoch:  3\n","LQ image reconstruction loss: 0.7286859154701233 iteration:  2030 epoch:  3\n","Same Different Classification loss: 0.5343820452690125 iteration:  2030 epoch:  3\n","Male Female Classification loss: 0.5981205105781555 iteration:  2030 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.8491735458374023 iteration:  2035 epoch:  3\n","LQ image reconstruction loss: 0.7605345845222473 iteration:  2035 epoch:  3\n","Same Different Classification loss: 0.5900561809539795 iteration:  2035 epoch:  3\n","Male Female Classification loss: 0.6122677326202393 iteration:  2035 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.056643486022949 iteration:  2040 epoch:  3\n","LQ image reconstruction loss: 0.7867470979690552 iteration:  2040 epoch:  3\n","Same Different Classification loss: 0.6042608022689819 iteration:  2040 epoch:  3\n","Male Female Classification loss: 0.5822029709815979 iteration:  2040 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.070366382598877 iteration:  2045 epoch:  3\n","LQ image reconstruction loss: 0.6814912557601929 iteration:  2045 epoch:  3\n","Same Different Classification loss: 0.5511776208877563 iteration:  2045 epoch:  3\n","Male Female Classification loss: 0.6409358382225037 iteration:  2045 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.9793894290924072 iteration:  2050 epoch:  3\n","LQ image reconstruction loss: 0.6696754693984985 iteration:  2050 epoch:  3\n","Same Different Classification loss: 0.5003975033760071 iteration:  2050 epoch:  3\n","Male Female Classification loss: 0.6585963368415833 iteration:  2050 epoch:  3\n","----------------------------------------------------\n","Accuracy of SD: 69 %\n","Accuracy of MF: 68 %\n","Mean Accuracy of SD: 58 %\n","Mean Accuracy of MF: 60 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.355522871017456 iteration:  2055 epoch:  3\n","LQ image reconstruction loss: 0.712618350982666 iteration:  2055 epoch:  3\n","Same Different Classification loss: 0.5758407711982727 iteration:  2055 epoch:  3\n","Male Female Classification loss: 0.6351504325866699 iteration:  2055 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.735201835632324 iteration:  2060 epoch:  3\n","LQ image reconstruction loss: 0.6851890087127686 iteration:  2060 epoch:  3\n","Same Different Classification loss: 0.5376021265983582 iteration:  2060 epoch:  3\n","Male Female Classification loss: 0.6398614048957825 iteration:  2060 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.329817056655884 iteration:  2065 epoch:  3\n","LQ image reconstruction loss: 0.695949912071228 iteration:  2065 epoch:  3\n","Same Different Classification loss: 0.5724944472312927 iteration:  2065 epoch:  3\n","Male Female Classification loss: 0.6084263920783997 iteration:  2065 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.718491554260254 iteration:  2070 epoch:  3\n","LQ image reconstruction loss: 0.7373963594436646 iteration:  2070 epoch:  3\n","Same Different Classification loss: 0.5764697790145874 iteration:  2070 epoch:  3\n","Male Female Classification loss: 0.6312013864517212 iteration:  2070 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.2166504859924316 iteration:  2075 epoch:  3\n","LQ image reconstruction loss: 0.694800853729248 iteration:  2075 epoch:  3\n","Same Different Classification loss: 0.5612170100212097 iteration:  2075 epoch:  3\n","Male Female Classification loss: 0.6007609367370605 iteration:  2075 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.193128824234009 iteration:  2080 epoch:  3\n","LQ image reconstruction loss: 0.7693148255348206 iteration:  2080 epoch:  3\n","Same Different Classification loss: 0.5683668851852417 iteration:  2080 epoch:  3\n","Male Female Classification loss: 0.5750720500946045 iteration:  2080 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.5829076766967773 iteration:  2085 epoch:  3\n","LQ image reconstruction loss: 0.655238151550293 iteration:  2085 epoch:  3\n","Same Different Classification loss: 0.5114641785621643 iteration:  2085 epoch:  3\n","Male Female Classification loss: 0.6950900554656982 iteration:  2085 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.19346022605896 iteration:  2090 epoch:  3\n","LQ image reconstruction loss: 0.6989213824272156 iteration:  2090 epoch:  3\n","Same Different Classification loss: 0.534920871257782 iteration:  2090 epoch:  3\n","Male Female Classification loss: 0.6428821086883545 iteration:  2090 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.8452281951904297 iteration:  2095 epoch:  3\n","LQ image reconstruction loss: 0.6827477216720581 iteration:  2095 epoch:  3\n","Same Different Classification loss: 0.5294398665428162 iteration:  2095 epoch:  3\n","Male Female Classification loss: 0.601002037525177 iteration:  2095 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.1468758583068848 iteration:  2100 epoch:  3\n","LQ image reconstruction loss: 0.6745472550392151 iteration:  2100 epoch:  3\n","Same Different Classification loss: 0.5314508676528931 iteration:  2100 epoch:  3\n","Male Female Classification loss: 0.6088974475860596 iteration:  2100 epoch:  3\n","----------------------------------------------------\n","Accuracy of SD: 81 %\n","Accuracy of MF: 76 %\n","Mean Accuracy of SD: 58 %\n","Mean Accuracy of MF: 60 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.5034778118133545 iteration:  2105 epoch:  3\n","LQ image reconstruction loss: 0.7342655658721924 iteration:  2105 epoch:  3\n","Same Different Classification loss: 0.6042364239692688 iteration:  2105 epoch:  3\n","Male Female Classification loss: 0.6849640011787415 iteration:  2105 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.0268192291259766 iteration:  2110 epoch:  3\n","LQ image reconstruction loss: 0.7320731282234192 iteration:  2110 epoch:  3\n","Same Different Classification loss: 0.5426754951477051 iteration:  2110 epoch:  3\n","Male Female Classification loss: 0.6343222856521606 iteration:  2110 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.556684732437134 iteration:  2115 epoch:  3\n","LQ image reconstruction loss: 0.7226217985153198 iteration:  2115 epoch:  3\n","Same Different Classification loss: 0.5766963362693787 iteration:  2115 epoch:  3\n","Male Female Classification loss: 0.6358397006988525 iteration:  2115 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.969730854034424 iteration:  2120 epoch:  3\n","LQ image reconstruction loss: 0.6922892332077026 iteration:  2120 epoch:  3\n","Same Different Classification loss: 0.5518566966056824 iteration:  2120 epoch:  3\n","Male Female Classification loss: 0.5711057782173157 iteration:  2120 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.4221088886260986 iteration:  2125 epoch:  3\n","LQ image reconstruction loss: 0.6187305450439453 iteration:  2125 epoch:  3\n","Same Different Classification loss: 0.502555787563324 iteration:  2125 epoch:  3\n","Male Female Classification loss: 0.6571959853172302 iteration:  2125 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.0458967685699463 iteration:  2130 epoch:  3\n","LQ image reconstruction loss: 0.6514226794242859 iteration:  2130 epoch:  3\n","Same Different Classification loss: 0.5158005356788635 iteration:  2130 epoch:  3\n","Male Female Classification loss: 0.672843873500824 iteration:  2130 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.9978597164154053 iteration:  2135 epoch:  3\n","LQ image reconstruction loss: 0.7429428696632385 iteration:  2135 epoch:  3\n","Same Different Classification loss: 0.6058861017227173 iteration:  2135 epoch:  3\n","Male Female Classification loss: 0.6133217811584473 iteration:  2135 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.00896954536438 iteration:  2140 epoch:  3\n","LQ image reconstruction loss: 0.5944647789001465 iteration:  2140 epoch:  3\n","Same Different Classification loss: 0.4686661660671234 iteration:  2140 epoch:  3\n","Male Female Classification loss: 0.6226027607917786 iteration:  2140 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.6648406982421875 iteration:  2145 epoch:  3\n","LQ image reconstruction loss: 0.8021888732910156 iteration:  2145 epoch:  3\n","Same Different Classification loss: 0.5602331757545471 iteration:  2145 epoch:  3\n","Male Female Classification loss: 0.5590975880622864 iteration:  2145 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.2633323669433594 iteration:  2150 epoch:  3\n","LQ image reconstruction loss: 0.7193496227264404 iteration:  2150 epoch:  3\n","Same Different Classification loss: 0.5631139278411865 iteration:  2150 epoch:  3\n","Male Female Classification loss: 0.6345576047897339 iteration:  2150 epoch:  3\n","----------------------------------------------------\n","Accuracy of SD: 64 %\n","Accuracy of MF: 65 %\n","Mean Accuracy of SD: 58 %\n","Mean Accuracy of MF: 60 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.6444287300109863 iteration:  2155 epoch:  3\n","LQ image reconstruction loss: 0.6221123337745667 iteration:  2155 epoch:  3\n","Same Different Classification loss: 0.5063249468803406 iteration:  2155 epoch:  3\n","Male Female Classification loss: 0.6324824690818787 iteration:  2155 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.2581753730773926 iteration:  2160 epoch:  3\n","LQ image reconstruction loss: 0.7225564122200012 iteration:  2160 epoch:  3\n","Same Different Classification loss: 0.5847907066345215 iteration:  2160 epoch:  3\n","Male Female Classification loss: 0.6181005835533142 iteration:  2160 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.2831454277038574 iteration:  2165 epoch:  3\n","LQ image reconstruction loss: 0.6704977750778198 iteration:  2165 epoch:  3\n","Same Different Classification loss: 0.5642454624176025 iteration:  2165 epoch:  3\n","Male Female Classification loss: 0.6633522510528564 iteration:  2165 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.7495126724243164 iteration:  2170 epoch:  3\n","LQ image reconstruction loss: 0.626812219619751 iteration:  2170 epoch:  3\n","Same Different Classification loss: 0.5418035387992859 iteration:  2170 epoch:  3\n","Male Female Classification loss: 0.6629794836044312 iteration:  2170 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.7348625659942627 iteration:  2175 epoch:  3\n","LQ image reconstruction loss: 0.752655029296875 iteration:  2175 epoch:  3\n","Same Different Classification loss: 0.6073545813560486 iteration:  2175 epoch:  3\n","Male Female Classification loss: 0.6555112600326538 iteration:  2175 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.030853271484375 iteration:  2180 epoch:  3\n","LQ image reconstruction loss: 0.7253270745277405 iteration:  2180 epoch:  3\n","Same Different Classification loss: 0.5724657773971558 iteration:  2180 epoch:  3\n","Male Female Classification loss: 0.5841543674468994 iteration:  2180 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.183110237121582 iteration:  2185 epoch:  3\n","LQ image reconstruction loss: 0.6800941228866577 iteration:  2185 epoch:  3\n","Same Different Classification loss: 0.5246297717094421 iteration:  2185 epoch:  3\n","Male Female Classification loss: 0.6438131928443909 iteration:  2185 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.343421220779419 iteration:  2190 epoch:  3\n","LQ image reconstruction loss: 0.712989091873169 iteration:  2190 epoch:  3\n","Same Different Classification loss: 0.5961580276489258 iteration:  2190 epoch:  3\n","Male Female Classification loss: 0.6551083326339722 iteration:  2190 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.3307440280914307 iteration:  2195 epoch:  3\n","LQ image reconstruction loss: 0.6056931614875793 iteration:  2195 epoch:  3\n","Same Different Classification loss: 0.49390673637390137 iteration:  2195 epoch:  3\n","Male Female Classification loss: 0.666190505027771 iteration:  2195 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.6464102268218994 iteration:  2200 epoch:  3\n","LQ image reconstruction loss: 0.6534591913223267 iteration:  2200 epoch:  3\n","Same Different Classification loss: 0.504590630531311 iteration:  2200 epoch:  3\n","Male Female Classification loss: 0.6193276047706604 iteration:  2200 epoch:  3\n","----------------------------------------------------\n","Accuracy of SD: 63 %\n","Accuracy of MF: 69 %\n","Mean Accuracy of SD: 59 %\n","Mean Accuracy of MF: 61 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.839418649673462 iteration:  2205 epoch:  3\n","LQ image reconstruction loss: 0.6907423734664917 iteration:  2205 epoch:  3\n","Same Different Classification loss: 0.5841319561004639 iteration:  2205 epoch:  3\n","Male Female Classification loss: 0.569375216960907 iteration:  2205 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.2205920219421387 iteration:  2210 epoch:  3\n","LQ image reconstruction loss: 0.7062804698944092 iteration:  2210 epoch:  3\n","Same Different Classification loss: 0.5521523356437683 iteration:  2210 epoch:  3\n","Male Female Classification loss: 0.6761342883110046 iteration:  2210 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.1881418228149414 iteration:  2215 epoch:  3\n","LQ image reconstruction loss: 0.7111729979515076 iteration:  2215 epoch:  3\n","Same Different Classification loss: 0.5816043019294739 iteration:  2215 epoch:  3\n","Male Female Classification loss: 0.6394809484481812 iteration:  2215 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.0699658393859863 iteration:  2220 epoch:  3\n","LQ image reconstruction loss: 0.650793194770813 iteration:  2220 epoch:  3\n","Same Different Classification loss: 0.510408878326416 iteration:  2220 epoch:  3\n","Male Female Classification loss: 0.6734693050384521 iteration:  2220 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.0455470085144043 iteration:  2225 epoch:  3\n","LQ image reconstruction loss: 0.730303168296814 iteration:  2225 epoch:  3\n","Same Different Classification loss: 0.5768331289291382 iteration:  2225 epoch:  3\n","Male Female Classification loss: 0.5743417739868164 iteration:  2225 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.6686184406280518 iteration:  2230 epoch:  3\n","LQ image reconstruction loss: 0.6444742679595947 iteration:  2230 epoch:  3\n","Same Different Classification loss: 0.5346655249595642 iteration:  2230 epoch:  3\n","Male Female Classification loss: 0.6600320935249329 iteration:  2230 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.0567941665649414 iteration:  2235 epoch:  3\n","LQ image reconstruction loss: 0.7015557289123535 iteration:  2235 epoch:  3\n","Same Different Classification loss: 0.5690133571624756 iteration:  2235 epoch:  3\n","Male Female Classification loss: 0.6369066834449768 iteration:  2235 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.2213869094848633 iteration:  2240 epoch:  3\n","LQ image reconstruction loss: 0.6370155811309814 iteration:  2240 epoch:  3\n","Same Different Classification loss: 0.4815632700920105 iteration:  2240 epoch:  3\n","Male Female Classification loss: 0.6189769506454468 iteration:  2240 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.0098512172698975 iteration:  2245 epoch:  3\n","LQ image reconstruction loss: 0.726703405380249 iteration:  2245 epoch:  3\n","Same Different Classification loss: 0.5250654816627502 iteration:  2245 epoch:  3\n","Male Female Classification loss: 0.5797246694564819 iteration:  2245 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.961548328399658 iteration:  2250 epoch:  3\n","LQ image reconstruction loss: 0.6277351975440979 iteration:  2250 epoch:  3\n","Same Different Classification loss: 0.4977152943611145 iteration:  2250 epoch:  3\n","Male Female Classification loss: 0.6452451348304749 iteration:  2250 epoch:  3\n","----------------------------------------------------\n","Accuracy of SD: 65 %\n","Accuracy of MF: 70 %\n","Mean Accuracy of SD: 59 %\n","Mean Accuracy of MF: 61 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.002183675765991 iteration:  2255 epoch:  3\n","LQ image reconstruction loss: 0.6443905830383301 iteration:  2255 epoch:  3\n","Same Different Classification loss: 0.5154919624328613 iteration:  2255 epoch:  3\n","Male Female Classification loss: 0.6461571455001831 iteration:  2255 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.1836307048797607 iteration:  2260 epoch:  3\n","LQ image reconstruction loss: 0.678581178188324 iteration:  2260 epoch:  3\n","Same Different Classification loss: 0.5631052255630493 iteration:  2260 epoch:  3\n","Male Female Classification loss: 0.6167775392532349 iteration:  2260 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.1717944145202637 iteration:  2265 epoch:  3\n","LQ image reconstruction loss: 0.7183510065078735 iteration:  2265 epoch:  3\n","Same Different Classification loss: 0.5780126452445984 iteration:  2265 epoch:  3\n","Male Female Classification loss: 0.6794781684875488 iteration:  2265 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.742689609527588 iteration:  2270 epoch:  3\n","LQ image reconstruction loss: 0.5513995885848999 iteration:  2270 epoch:  3\n","Same Different Classification loss: 0.43669188022613525 iteration:  2270 epoch:  3\n","Male Female Classification loss: 0.6021318435668945 iteration:  2270 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.3619866371154785 iteration:  2275 epoch:  3\n","LQ image reconstruction loss: 0.6469794511795044 iteration:  2275 epoch:  3\n","Same Different Classification loss: 0.5286955237388611 iteration:  2275 epoch:  3\n","Male Female Classification loss: 0.634089469909668 iteration:  2275 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.208991289138794 iteration:  2280 epoch:  3\n","LQ image reconstruction loss: 0.6750484108924866 iteration:  2280 epoch:  3\n","Same Different Classification loss: 0.545705258846283 iteration:  2280 epoch:  3\n","Male Female Classification loss: 0.5549057126045227 iteration:  2280 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.7163383960723877 iteration:  2285 epoch:  3\n","LQ image reconstruction loss: 0.7173635959625244 iteration:  2285 epoch:  3\n","Same Different Classification loss: 0.5647778511047363 iteration:  2285 epoch:  3\n","Male Female Classification loss: 0.6038998365402222 iteration:  2285 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.9293558597564697 iteration:  2290 epoch:  3\n","LQ image reconstruction loss: 0.6700776815414429 iteration:  2290 epoch:  3\n","Same Different Classification loss: 0.5235295295715332 iteration:  2290 epoch:  3\n","Male Female Classification loss: 0.6261351108551025 iteration:  2290 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.4349427223205566 iteration:  2295 epoch:  3\n","LQ image reconstruction loss: 0.7536013126373291 iteration:  2295 epoch:  3\n","Same Different Classification loss: 0.6012876033782959 iteration:  2295 epoch:  3\n","Male Female Classification loss: 0.6238775849342346 iteration:  2295 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.4482204914093018 iteration:  2300 epoch:  3\n","LQ image reconstruction loss: 0.7265563011169434 iteration:  2300 epoch:  3\n","Same Different Classification loss: 0.586356520652771 iteration:  2300 epoch:  3\n","Male Female Classification loss: 0.5856984853744507 iteration:  2300 epoch:  3\n","----------------------------------------------------\n","Accuracy of SD: 73 %\n","Accuracy of MF: 67 %\n","Mean Accuracy of SD: 59 %\n","Mean Accuracy of MF: 61 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.921713352203369 iteration:  2305 epoch:  3\n","LQ image reconstruction loss: 0.7023758888244629 iteration:  2305 epoch:  3\n","Same Different Classification loss: 0.5841689705848694 iteration:  2305 epoch:  3\n","Male Female Classification loss: 0.6368874907493591 iteration:  2305 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.6929433345794678 iteration:  2310 epoch:  3\n","LQ image reconstruction loss: 0.649876594543457 iteration:  2310 epoch:  3\n","Same Different Classification loss: 0.4707047641277313 iteration:  2310 epoch:  3\n","Male Female Classification loss: 0.6355630159378052 iteration:  2310 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.978595733642578 iteration:  2315 epoch:  3\n","LQ image reconstruction loss: 0.7132946252822876 iteration:  2315 epoch:  3\n","Same Different Classification loss: 0.5877118110656738 iteration:  2315 epoch:  3\n","Male Female Classification loss: 0.6379945874214172 iteration:  2315 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.7368810176849365 iteration:  2320 epoch:  3\n","LQ image reconstruction loss: 0.6343435049057007 iteration:  2320 epoch:  3\n","Same Different Classification loss: 0.524436354637146 iteration:  2320 epoch:  3\n","Male Female Classification loss: 0.649799644947052 iteration:  2320 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.9991042613983154 iteration:  2325 epoch:  3\n","LQ image reconstruction loss: 0.657328724861145 iteration:  2325 epoch:  3\n","Same Different Classification loss: 0.5604209303855896 iteration:  2325 epoch:  3\n","Male Female Classification loss: 0.6540161371231079 iteration:  2325 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.909262180328369 iteration:  2330 epoch:  3\n","LQ image reconstruction loss: 0.6671774983406067 iteration:  2330 epoch:  3\n","Same Different Classification loss: 0.5431914329528809 iteration:  2330 epoch:  3\n","Male Female Classification loss: 0.6214639544487 iteration:  2330 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.5339958667755127 iteration:  2335 epoch:  3\n","LQ image reconstruction loss: 0.6912777423858643 iteration:  2335 epoch:  3\n","Same Different Classification loss: 0.5769590735435486 iteration:  2335 epoch:  3\n","Male Female Classification loss: 0.6374605298042297 iteration:  2335 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.6884865760803223 iteration:  2340 epoch:  3\n","LQ image reconstruction loss: 0.5821071863174438 iteration:  2340 epoch:  3\n","Same Different Classification loss: 0.5011362433433533 iteration:  2340 epoch:  3\n","Male Female Classification loss: 0.6070820093154907 iteration:  2340 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.920703649520874 iteration:  2345 epoch:  3\n","LQ image reconstruction loss: 0.6778204441070557 iteration:  2345 epoch:  3\n","Same Different Classification loss: 0.5937235355377197 iteration:  2345 epoch:  3\n","Male Female Classification loss: 0.6883788108825684 iteration:  2345 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.285684823989868 iteration:  2350 epoch:  3\n","LQ image reconstruction loss: 0.7420950531959534 iteration:  2350 epoch:  3\n","Same Different Classification loss: 0.5690842866897583 iteration:  2350 epoch:  3\n","Male Female Classification loss: 0.5991490483283997 iteration:  2350 epoch:  3\n","----------------------------------------------------\n","Accuracy of SD: 67 %\n","Accuracy of MF: 70 %\n","Mean Accuracy of SD: 59 %\n","Mean Accuracy of MF: 61 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.785306692123413 iteration:  2355 epoch:  3\n","LQ image reconstruction loss: 0.6262219548225403 iteration:  2355 epoch:  3\n","Same Different Classification loss: 0.5185344815254211 iteration:  2355 epoch:  3\n","Male Female Classification loss: 0.5855768322944641 iteration:  2355 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.0064620971679688 iteration:  2360 epoch:  3\n","LQ image reconstruction loss: 0.650113582611084 iteration:  2360 epoch:  3\n","Same Different Classification loss: 0.5515198707580566 iteration:  2360 epoch:  3\n","Male Female Classification loss: 0.6367941498756409 iteration:  2360 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.976473331451416 iteration:  2365 epoch:  3\n","LQ image reconstruction loss: 0.710849404335022 iteration:  2365 epoch:  3\n","Same Different Classification loss: 0.5678800344467163 iteration:  2365 epoch:  3\n","Male Female Classification loss: 0.6190469861030579 iteration:  2365 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.0576698780059814 iteration:  2370 epoch:  3\n","LQ image reconstruction loss: 0.6390289068222046 iteration:  2370 epoch:  3\n","Same Different Classification loss: 0.5162850022315979 iteration:  2370 epoch:  3\n","Male Female Classification loss: 0.643456220626831 iteration:  2370 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.76222825050354 iteration:  2375 epoch:  3\n","LQ image reconstruction loss: 0.6904304027557373 iteration:  2375 epoch:  3\n","Same Different Classification loss: 0.5609398484230042 iteration:  2375 epoch:  3\n","Male Female Classification loss: 0.6104562282562256 iteration:  2375 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.4164209365844727 iteration:  2380 epoch:  3\n","LQ image reconstruction loss: 0.6886563897132874 iteration:  2380 epoch:  3\n","Same Different Classification loss: 0.5537317395210266 iteration:  2380 epoch:  3\n","Male Female Classification loss: 0.6103174686431885 iteration:  2380 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.1471729278564453 iteration:  2385 epoch:  3\n","LQ image reconstruction loss: 0.7003850936889648 iteration:  2385 epoch:  3\n","Same Different Classification loss: 0.5223559141159058 iteration:  2385 epoch:  3\n","Male Female Classification loss: 0.6364966034889221 iteration:  2385 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.8401637077331543 iteration:  2390 epoch:  3\n","LQ image reconstruction loss: 0.6873883008956909 iteration:  2390 epoch:  3\n","Same Different Classification loss: 0.5454053282737732 iteration:  2390 epoch:  3\n","Male Female Classification loss: 0.6511240601539612 iteration:  2390 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.858304977416992 iteration:  2395 epoch:  3\n","LQ image reconstruction loss: 0.7183607220649719 iteration:  2395 epoch:  3\n","Same Different Classification loss: 0.5741558074951172 iteration:  2395 epoch:  3\n","Male Female Classification loss: 0.634823739528656 iteration:  2395 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.009760856628418 iteration:  2400 epoch:  3\n","LQ image reconstruction loss: 0.6777392029762268 iteration:  2400 epoch:  3\n","Same Different Classification loss: 0.549912691116333 iteration:  2400 epoch:  3\n","Male Female Classification loss: 0.6297550797462463 iteration:  2400 epoch:  3\n","----------------------------------------------------\n","Accuracy of SD: 75 %\n","Accuracy of MF: 69 %\n","Mean Accuracy of SD: 59 %\n","Mean Accuracy of MF: 61 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.170475482940674 iteration:  2405 epoch:  3\n","LQ image reconstruction loss: 0.672259509563446 iteration:  2405 epoch:  3\n","Same Different Classification loss: 0.5825784206390381 iteration:  2405 epoch:  3\n","Male Female Classification loss: 0.6726768612861633 iteration:  2405 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.9730072021484375 iteration:  2410 epoch:  3\n","LQ image reconstruction loss: 0.7115451693534851 iteration:  2410 epoch:  3\n","Same Different Classification loss: 0.5646997690200806 iteration:  2410 epoch:  3\n","Male Female Classification loss: 0.6058266758918762 iteration:  2410 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.9332594871520996 iteration:  2415 epoch:  3\n","LQ image reconstruction loss: 0.6716591715812683 iteration:  2415 epoch:  3\n","Same Different Classification loss: 0.5595082640647888 iteration:  2415 epoch:  3\n","Male Female Classification loss: 0.612912654876709 iteration:  2415 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.1660497188568115 iteration:  2420 epoch:  3\n","LQ image reconstruction loss: 0.6517586708068848 iteration:  2420 epoch:  3\n","Same Different Classification loss: 0.541212797164917 iteration:  2420 epoch:  3\n","Male Female Classification loss: 0.6770886778831482 iteration:  2420 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.0988588333129883 iteration:  2425 epoch:  3\n","LQ image reconstruction loss: 0.6154139637947083 iteration:  2425 epoch:  3\n","Same Different Classification loss: 0.5128090977668762 iteration:  2425 epoch:  3\n","Male Female Classification loss: 0.6285662651062012 iteration:  2425 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.5063672065734863 iteration:  2430 epoch:  3\n","LQ image reconstruction loss: 0.7042632102966309 iteration:  2430 epoch:  3\n","Same Different Classification loss: 0.6078824400901794 iteration:  2430 epoch:  3\n","Male Female Classification loss: 0.602175235748291 iteration:  2430 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.4183189868927 iteration:  2435 epoch:  3\n","LQ image reconstruction loss: 0.7324141263961792 iteration:  2435 epoch:  3\n","Same Different Classification loss: 0.5828465223312378 iteration:  2435 epoch:  3\n","Male Female Classification loss: 0.5731145143508911 iteration:  2435 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.003859519958496 iteration:  2440 epoch:  3\n","LQ image reconstruction loss: 0.5960448980331421 iteration:  2440 epoch:  3\n","Same Different Classification loss: 0.49634090065956116 iteration:  2440 epoch:  3\n","Male Female Classification loss: 0.6322547793388367 iteration:  2440 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.4597415924072266 iteration:  2445 epoch:  3\n","LQ image reconstruction loss: 0.6476557850837708 iteration:  2445 epoch:  3\n","Same Different Classification loss: 0.5579394698143005 iteration:  2445 epoch:  3\n","Male Female Classification loss: 0.6361290812492371 iteration:  2445 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.6026947498321533 iteration:  2450 epoch:  3\n","LQ image reconstruction loss: 0.6790351271629333 iteration:  2450 epoch:  3\n","Same Different Classification loss: 0.5598613619804382 iteration:  2450 epoch:  3\n","Male Female Classification loss: 0.622287929058075 iteration:  2450 epoch:  3\n","----------------------------------------------------\n","Accuracy of SD: 68 %\n","Accuracy of MF: 75 %\n","Mean Accuracy of SD: 59 %\n","Mean Accuracy of MF: 61 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.6807992458343506 iteration:  2455 epoch:  3\n","LQ image reconstruction loss: 0.596228837966919 iteration:  2455 epoch:  3\n","Same Different Classification loss: 0.45834481716156006 iteration:  2455 epoch:  3\n","Male Female Classification loss: 0.6101720929145813 iteration:  2455 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.9963512420654297 iteration:  2460 epoch:  3\n","LQ image reconstruction loss: 0.7341327667236328 iteration:  2460 epoch:  3\n","Same Different Classification loss: 0.5758312940597534 iteration:  2460 epoch:  3\n","Male Female Classification loss: 0.5804738402366638 iteration:  2460 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.9334845542907715 iteration:  2465 epoch:  3\n","LQ image reconstruction loss: 0.6017611026763916 iteration:  2465 epoch:  3\n","Same Different Classification loss: 0.49910876154899597 iteration:  2465 epoch:  3\n","Male Female Classification loss: 0.6678663492202759 iteration:  2465 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.595235586166382 iteration:  2470 epoch:  3\n","LQ image reconstruction loss: 0.6278566122055054 iteration:  2470 epoch:  3\n","Same Different Classification loss: 0.5327803492546082 iteration:  2470 epoch:  3\n","Male Female Classification loss: 0.696147084236145 iteration:  2470 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.8663101196289062 iteration:  2475 epoch:  3\n","LQ image reconstruction loss: 0.6692578792572021 iteration:  2475 epoch:  3\n","Same Different Classification loss: 0.48278185725212097 iteration:  2475 epoch:  3\n","Male Female Classification loss: 0.591224730014801 iteration:  2475 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.296208381652832 iteration:  2480 epoch:  3\n","LQ image reconstruction loss: 0.6844673752784729 iteration:  2480 epoch:  3\n","Same Different Classification loss: 0.5504888296127319 iteration:  2480 epoch:  3\n","Male Female Classification loss: 0.6167806386947632 iteration:  2480 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.516500949859619 iteration:  2485 epoch:  3\n","LQ image reconstruction loss: 0.5498124957084656 iteration:  2485 epoch:  3\n","Same Different Classification loss: 0.4769505560398102 iteration:  2485 epoch:  3\n","Male Female Classification loss: 0.6978444457054138 iteration:  2485 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.5325770378112793 iteration:  2490 epoch:  3\n","LQ image reconstruction loss: 0.7008588314056396 iteration:  2490 epoch:  3\n","Same Different Classification loss: 0.5835182666778564 iteration:  2490 epoch:  3\n","Male Female Classification loss: 0.6569592952728271 iteration:  2490 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.925384283065796 iteration:  2495 epoch:  3\n","LQ image reconstruction loss: 0.6629827618598938 iteration:  2495 epoch:  3\n","Same Different Classification loss: 0.529853105545044 iteration:  2495 epoch:  3\n","Male Female Classification loss: 0.6347198486328125 iteration:  2495 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.428307056427002 iteration:  2500 epoch:  3\n","LQ image reconstruction loss: 0.681098461151123 iteration:  2500 epoch:  3\n","Same Different Classification loss: 0.5640345215797424 iteration:  2500 epoch:  3\n","Male Female Classification loss: 0.6440698504447937 iteration:  2500 epoch:  3\n","----------------------------------------------------\n","Accuracy of SD: 70 %\n","Accuracy of MF: 61 %\n","Mean Accuracy of SD: 59 %\n","Mean Accuracy of MF: 61 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.182697057723999 iteration:  2505 epoch:  3\n","LQ image reconstruction loss: 0.517048716545105 iteration:  2505 epoch:  3\n","Same Different Classification loss: 0.46053433418273926 iteration:  2505 epoch:  3\n","Male Female Classification loss: 0.6690280437469482 iteration:  2505 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.382014274597168 iteration:  2510 epoch:  3\n","LQ image reconstruction loss: 0.6553856134414673 iteration:  2510 epoch:  3\n","Same Different Classification loss: 0.5754364132881165 iteration:  2510 epoch:  3\n","Male Female Classification loss: 0.6736276149749756 iteration:  2510 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.625908374786377 iteration:  2515 epoch:  3\n","LQ image reconstruction loss: 0.7813833355903625 iteration:  2515 epoch:  3\n","Same Different Classification loss: 0.6042035818099976 iteration:  2515 epoch:  3\n","Male Female Classification loss: 0.5824421048164368 iteration:  2515 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.619128465652466 iteration:  2520 epoch:  3\n","LQ image reconstruction loss: 0.6618033647537231 iteration:  2520 epoch:  3\n","Same Different Classification loss: 0.5163288712501526 iteration:  2520 epoch:  3\n","Male Female Classification loss: 0.5627484321594238 iteration:  2520 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.138192892074585 iteration:  2525 epoch:  3\n","LQ image reconstruction loss: 0.6066011190414429 iteration:  2525 epoch:  3\n","Same Different Classification loss: 0.5052083134651184 iteration:  2525 epoch:  3\n","Male Female Classification loss: 0.6928714513778687 iteration:  2525 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.9206650257110596 iteration:  2530 epoch:  3\n","LQ image reconstruction loss: 0.6314041018486023 iteration:  2530 epoch:  3\n","Same Different Classification loss: 0.5019707083702087 iteration:  2530 epoch:  3\n","Male Female Classification loss: 0.5825923681259155 iteration:  2530 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.0090842247009277 iteration:  2535 epoch:  3\n","LQ image reconstruction loss: 0.580848217010498 iteration:  2535 epoch:  3\n","Same Different Classification loss: 0.49774712324142456 iteration:  2535 epoch:  3\n","Male Female Classification loss: 0.66365647315979 iteration:  2535 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.5213699340820312 iteration:  2540 epoch:  3\n","LQ image reconstruction loss: 0.6666035056114197 iteration:  2540 epoch:  3\n","Same Different Classification loss: 0.5349742770195007 iteration:  2540 epoch:  3\n","Male Female Classification loss: 0.5794527530670166 iteration:  2540 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.9766879081726074 iteration:  2545 epoch:  3\n","LQ image reconstruction loss: 0.6279940605163574 iteration:  2545 epoch:  3\n","Same Different Classification loss: 0.5204601883888245 iteration:  2545 epoch:  3\n","Male Female Classification loss: 0.5965235829353333 iteration:  2545 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.7791364192962646 iteration:  2550 epoch:  3\n","LQ image reconstruction loss: 0.6214728355407715 iteration:  2550 epoch:  3\n","Same Different Classification loss: 0.4950471818447113 iteration:  2550 epoch:  3\n","Male Female Classification loss: 0.5734958648681641 iteration:  2550 epoch:  3\n","----------------------------------------------------\n","Accuracy of SD: 67 %\n","Accuracy of MF: 65 %\n","Mean Accuracy of SD: 59 %\n","Mean Accuracy of MF: 61 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.3011341094970703 iteration:  2555 epoch:  3\n","LQ image reconstruction loss: 0.6442945003509521 iteration:  2555 epoch:  3\n","Same Different Classification loss: 0.511579155921936 iteration:  2555 epoch:  3\n","Male Female Classification loss: 0.5849087238311768 iteration:  2555 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.806533098220825 iteration:  2560 epoch:  3\n","LQ image reconstruction loss: 0.7095741033554077 iteration:  2560 epoch:  3\n","Same Different Classification loss: 0.5625413060188293 iteration:  2560 epoch:  3\n","Male Female Classification loss: 0.6151291131973267 iteration:  2560 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.06418776512146 iteration:  2565 epoch:  3\n","LQ image reconstruction loss: 0.6584852337837219 iteration:  2565 epoch:  3\n","Same Different Classification loss: 0.49387115240097046 iteration:  2565 epoch:  3\n","Male Female Classification loss: 0.5911414623260498 iteration:  2565 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.0291924476623535 iteration:  2570 epoch:  3\n","LQ image reconstruction loss: 0.6315958499908447 iteration:  2570 epoch:  3\n","Same Different Classification loss: 0.5303641557693481 iteration:  2570 epoch:  3\n","Male Female Classification loss: 0.557895839214325 iteration:  2570 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.1936216354370117 iteration:  2575 epoch:  3\n","LQ image reconstruction loss: 0.6546658277511597 iteration:  2575 epoch:  3\n","Same Different Classification loss: 0.517274022102356 iteration:  2575 epoch:  3\n","Male Female Classification loss: 0.6578638553619385 iteration:  2575 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.6838159561157227 iteration:  2580 epoch:  3\n","LQ image reconstruction loss: 0.7772347927093506 iteration:  2580 epoch:  3\n","Same Different Classification loss: 0.6221652030944824 iteration:  2580 epoch:  3\n","Male Female Classification loss: 0.620124876499176 iteration:  2580 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.3758487701416016 iteration:  2585 epoch:  3\n","LQ image reconstruction loss: 0.7408236861228943 iteration:  2585 epoch:  3\n","Same Different Classification loss: 0.5682847499847412 iteration:  2585 epoch:  3\n","Male Female Classification loss: 0.6148416996002197 iteration:  2585 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.7839245796203613 iteration:  2590 epoch:  3\n","LQ image reconstruction loss: 0.7285582423210144 iteration:  2590 epoch:  3\n","Same Different Classification loss: 0.6056168079376221 iteration:  2590 epoch:  3\n","Male Female Classification loss: 0.611860990524292 iteration:  2590 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.87542462348938 iteration:  2595 epoch:  3\n","LQ image reconstruction loss: 0.6410081386566162 iteration:  2595 epoch:  3\n","Same Different Classification loss: 0.5342668890953064 iteration:  2595 epoch:  3\n","Male Female Classification loss: 0.649412989616394 iteration:  2595 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.9474432468414307 iteration:  2600 epoch:  3\n","LQ image reconstruction loss: 0.599379301071167 iteration:  2600 epoch:  3\n","Same Different Classification loss: 0.5018520951271057 iteration:  2600 epoch:  3\n","Male Female Classification loss: 0.6598233580589294 iteration:  2600 epoch:  3\n","----------------------------------------------------\n","Accuracy of SD: 81 %\n","Accuracy of MF: 73 %\n","Mean Accuracy of SD: 59 %\n","Mean Accuracy of MF: 61 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.8216731548309326 iteration:  2605 epoch:  3\n","LQ image reconstruction loss: 0.7546407580375671 iteration:  2605 epoch:  3\n","Same Different Classification loss: 0.5350950360298157 iteration:  2605 epoch:  3\n","Male Female Classification loss: 0.5840866565704346 iteration:  2605 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.544461488723755 iteration:  2610 epoch:  3\n","LQ image reconstruction loss: 0.7202892899513245 iteration:  2610 epoch:  3\n","Same Different Classification loss: 0.56951904296875 iteration:  2610 epoch:  3\n","Male Female Classification loss: 0.594248354434967 iteration:  2610 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.840857982635498 iteration:  2615 epoch:  3\n","LQ image reconstruction loss: 0.6286437511444092 iteration:  2615 epoch:  3\n","Same Different Classification loss: 0.4923756718635559 iteration:  2615 epoch:  3\n","Male Female Classification loss: 0.6257333159446716 iteration:  2615 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.0691874027252197 iteration:  2620 epoch:  3\n","LQ image reconstruction loss: 0.6776083707809448 iteration:  2620 epoch:  3\n","Same Different Classification loss: 0.5256627202033997 iteration:  2620 epoch:  3\n","Male Female Classification loss: 0.599082887172699 iteration:  2620 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.0995757579803467 iteration:  2625 epoch:  3\n","LQ image reconstruction loss: 0.6537671685218811 iteration:  2625 epoch:  3\n","Same Different Classification loss: 0.5135728716850281 iteration:  2625 epoch:  3\n","Male Female Classification loss: 0.6553391814231873 iteration:  2625 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.9278736114501953 iteration:  2630 epoch:  3\n","LQ image reconstruction loss: 0.6095999479293823 iteration:  2630 epoch:  3\n","Same Different Classification loss: 0.4912746548652649 iteration:  2630 epoch:  3\n","Male Female Classification loss: 0.5739313364028931 iteration:  2630 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.5644679069519043 iteration:  2635 epoch:  3\n","LQ image reconstruction loss: 0.7136338949203491 iteration:  2635 epoch:  3\n","Same Different Classification loss: 0.6487682461738586 iteration:  2635 epoch:  3\n","Male Female Classification loss: 0.6599319577217102 iteration:  2635 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.0157392024993896 iteration:  2640 epoch:  3\n","LQ image reconstruction loss: 0.6349274516105652 iteration:  2640 epoch:  3\n","Same Different Classification loss: 0.5150840878486633 iteration:  2640 epoch:  3\n","Male Female Classification loss: 0.570504903793335 iteration:  2640 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.546605348587036 iteration:  2645 epoch:  3\n","LQ image reconstruction loss: 0.5845944285392761 iteration:  2645 epoch:  3\n","Same Different Classification loss: 0.4859410226345062 iteration:  2645 epoch:  3\n","Male Female Classification loss: 0.6563625931739807 iteration:  2645 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.413689136505127 iteration:  2650 epoch:  3\n","LQ image reconstruction loss: 0.5404355525970459 iteration:  2650 epoch:  3\n","Same Different Classification loss: 0.47018423676490784 iteration:  2650 epoch:  3\n","Male Female Classification loss: 0.6985884308815002 iteration:  2650 epoch:  3\n","----------------------------------------------------\n","Accuracy of SD: 74 %\n","Accuracy of MF: 56 %\n","Mean Accuracy of SD: 59 %\n","Mean Accuracy of MF: 61 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.0341508388519287 iteration:  2655 epoch:  3\n","LQ image reconstruction loss: 0.6682095527648926 iteration:  2655 epoch:  3\n","Same Different Classification loss: 0.5491064190864563 iteration:  2655 epoch:  3\n","Male Female Classification loss: 0.574820876121521 iteration:  2655 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.949047088623047 iteration:  2660 epoch:  3\n","LQ image reconstruction loss: 0.6191438436508179 iteration:  2660 epoch:  3\n","Same Different Classification loss: 0.48774588108062744 iteration:  2660 epoch:  3\n","Male Female Classification loss: 0.6036522388458252 iteration:  2660 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.464826822280884 iteration:  2665 epoch:  3\n","LQ image reconstruction loss: 0.5939214825630188 iteration:  2665 epoch:  3\n","Same Different Classification loss: 0.5004507303237915 iteration:  2665 epoch:  3\n","Male Female Classification loss: 0.6529003381729126 iteration:  2665 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.5036373138427734 iteration:  2670 epoch:  3\n","LQ image reconstruction loss: 0.604706883430481 iteration:  2670 epoch:  3\n","Same Different Classification loss: 0.506845235824585 iteration:  2670 epoch:  3\n","Male Female Classification loss: 0.6033685207366943 iteration:  2670 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.512042999267578 iteration:  2675 epoch:  3\n","LQ image reconstruction loss: 0.622270941734314 iteration:  2675 epoch:  3\n","Same Different Classification loss: 0.48835599422454834 iteration:  2675 epoch:  3\n","Male Female Classification loss: 0.6313775777816772 iteration:  2675 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.93168306350708 iteration:  2680 epoch:  3\n","LQ image reconstruction loss: 0.6718268394470215 iteration:  2680 epoch:  3\n","Same Different Classification loss: 0.5882105827331543 iteration:  2680 epoch:  3\n","Male Female Classification loss: 0.6749321818351746 iteration:  2680 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.969208002090454 iteration:  2685 epoch:  3\n","LQ image reconstruction loss: 0.6831559538841248 iteration:  2685 epoch:  3\n","Same Different Classification loss: 0.5700923800468445 iteration:  2685 epoch:  3\n","Male Female Classification loss: 0.6096835732460022 iteration:  2685 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.8531758785247803 iteration:  2690 epoch:  3\n","LQ image reconstruction loss: 0.62860107421875 iteration:  2690 epoch:  3\n","Same Different Classification loss: 0.4936581552028656 iteration:  2690 epoch:  3\n","Male Female Classification loss: 0.5874354243278503 iteration:  2690 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.775952100753784 iteration:  2695 epoch:  3\n","LQ image reconstruction loss: 0.6002026796340942 iteration:  2695 epoch:  3\n","Same Different Classification loss: 0.5048130750656128 iteration:  2695 epoch:  3\n","Male Female Classification loss: 0.6418808698654175 iteration:  2695 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.0508196353912354 iteration:  2700 epoch:  3\n","LQ image reconstruction loss: 0.6260809302330017 iteration:  2700 epoch:  3\n","Same Different Classification loss: 0.5001279711723328 iteration:  2700 epoch:  3\n","Male Female Classification loss: 0.5923821330070496 iteration:  2700 epoch:  3\n","----------------------------------------------------\n","Accuracy of SD: 76 %\n","Accuracy of MF: 71 %\n","Mean Accuracy of SD: 59 %\n","Mean Accuracy of MF: 61 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.70135760307312 iteration:  2705 epoch:  3\n","LQ image reconstruction loss: 0.6784956455230713 iteration:  2705 epoch:  3\n","Same Different Classification loss: 0.5722116231918335 iteration:  2705 epoch:  3\n","Male Female Classification loss: 0.5403486490249634 iteration:  2705 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 4.079696178436279 iteration:  2710 epoch:  3\n","LQ image reconstruction loss: 0.7263181805610657 iteration:  2710 epoch:  3\n","Same Different Classification loss: 0.5604768991470337 iteration:  2710 epoch:  3\n","Male Female Classification loss: 0.5375933051109314 iteration:  2710 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.2155661582946777 iteration:  2715 epoch:  3\n","LQ image reconstruction loss: 0.6268188953399658 iteration:  2715 epoch:  3\n","Same Different Classification loss: 0.5029973983764648 iteration:  2715 epoch:  3\n","Male Female Classification loss: 0.6685609221458435 iteration:  2715 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.567913770675659 iteration:  2720 epoch:  3\n","LQ image reconstruction loss: 0.6939359307289124 iteration:  2720 epoch:  3\n","Same Different Classification loss: 0.553992509841919 iteration:  2720 epoch:  3\n","Male Female Classification loss: 0.611559271812439 iteration:  2720 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.0564842224121094 iteration:  2725 epoch:  3\n","LQ image reconstruction loss: 0.5988080501556396 iteration:  2725 epoch:  3\n","Same Different Classification loss: 0.4530755877494812 iteration:  2725 epoch:  3\n","Male Female Classification loss: 0.5707197785377502 iteration:  2725 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.5489585399627686 iteration:  2730 epoch:  3\n","LQ image reconstruction loss: 0.5739995837211609 iteration:  2730 epoch:  3\n","Same Different Classification loss: 0.46887585520744324 iteration:  2730 epoch:  3\n","Male Female Classification loss: 0.5687727332115173 iteration:  2730 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.9639933109283447 iteration:  2735 epoch:  3\n","LQ image reconstruction loss: 0.6405673027038574 iteration:  2735 epoch:  3\n","Same Different Classification loss: 0.5354964733123779 iteration:  2735 epoch:  3\n","Male Female Classification loss: 0.6065698862075806 iteration:  2735 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.2099742889404297 iteration:  2740 epoch:  3\n","LQ image reconstruction loss: 0.670943021774292 iteration:  2740 epoch:  3\n","Same Different Classification loss: 0.565660834312439 iteration:  2740 epoch:  3\n","Male Female Classification loss: 0.5818832516670227 iteration:  2740 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.6715214252471924 iteration:  2745 epoch:  3\n","LQ image reconstruction loss: 0.5865364074707031 iteration:  2745 epoch:  3\n","Same Different Classification loss: 0.4269564151763916 iteration:  2745 epoch:  3\n","Male Female Classification loss: 0.5713496208190918 iteration:  2745 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.5317223072052 iteration:  2750 epoch:  3\n","LQ image reconstruction loss: 0.6418764591217041 iteration:  2750 epoch:  3\n","Same Different Classification loss: 0.5306252241134644 iteration:  2750 epoch:  3\n","Male Female Classification loss: 0.6000918745994568 iteration:  2750 epoch:  3\n","----------------------------------------------------\n","Accuracy of SD: 74 %\n","Accuracy of MF: 73 %\n","Mean Accuracy of SD: 59 %\n","Mean Accuracy of MF: 61 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.478259325027466 iteration:  2755 epoch:  3\n","LQ image reconstruction loss: 0.6998726725578308 iteration:  2755 epoch:  3\n","Same Different Classification loss: 0.5532292723655701 iteration:  2755 epoch:  3\n","Male Female Classification loss: 0.6105579137802124 iteration:  2755 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.6309995651245117 iteration:  2760 epoch:  3\n","LQ image reconstruction loss: 0.6210392713546753 iteration:  2760 epoch:  3\n","Same Different Classification loss: 0.5010144114494324 iteration:  2760 epoch:  3\n","Male Female Classification loss: 0.6292146444320679 iteration:  2760 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.409161329269409 iteration:  2765 epoch:  3\n","LQ image reconstruction loss: 0.6448112726211548 iteration:  2765 epoch:  3\n","Same Different Classification loss: 0.5356770157814026 iteration:  2765 epoch:  3\n","Male Female Classification loss: 0.5740219950675964 iteration:  2765 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.30446720123291 iteration:  2770 epoch:  3\n","LQ image reconstruction loss: 0.6147928833961487 iteration:  2770 epoch:  3\n","Same Different Classification loss: 0.4822072684764862 iteration:  2770 epoch:  3\n","Male Female Classification loss: 0.6205194592475891 iteration:  2770 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.62758469581604 iteration:  2775 epoch:  3\n","LQ image reconstruction loss: 0.6074075102806091 iteration:  2775 epoch:  3\n","Same Different Classification loss: 0.512459397315979 iteration:  2775 epoch:  3\n","Male Female Classification loss: 0.6268815398216248 iteration:  2775 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.5703177452087402 iteration:  2780 epoch:  3\n","LQ image reconstruction loss: 0.6406745910644531 iteration:  2780 epoch:  3\n","Same Different Classification loss: 0.5497963428497314 iteration:  2780 epoch:  3\n","Male Female Classification loss: 0.6098460555076599 iteration:  2780 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.8857340812683105 iteration:  2785 epoch:  3\n","LQ image reconstruction loss: 0.7685651779174805 iteration:  2785 epoch:  3\n","Same Different Classification loss: 0.687256395816803 iteration:  2785 epoch:  3\n","Male Female Classification loss: 0.658560037612915 iteration:  2785 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.5477921962738037 iteration:  2790 epoch:  3\n","LQ image reconstruction loss: 0.622329831123352 iteration:  2790 epoch:  3\n","Same Different Classification loss: 0.47325554490089417 iteration:  2790 epoch:  3\n","Male Female Classification loss: 0.5921584367752075 iteration:  2790 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.7124831676483154 iteration:  2795 epoch:  3\n","LQ image reconstruction loss: 0.5792276859283447 iteration:  2795 epoch:  3\n","Same Different Classification loss: 0.5103128552436829 iteration:  2795 epoch:  3\n","Male Female Classification loss: 0.644727885723114 iteration:  2795 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.873988389968872 iteration:  2800 epoch:  3\n","LQ image reconstruction loss: 0.5812903642654419 iteration:  2800 epoch:  3\n","Same Different Classification loss: 0.4954862594604492 iteration:  2800 epoch:  3\n","Male Female Classification loss: 0.6394001245498657 iteration:  2800 epoch:  3\n","----------------------------------------------------\n","Accuracy of SD: 75 %\n","Accuracy of MF: 61 %\n","Mean Accuracy of SD: 59 %\n","Mean Accuracy of MF: 61 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.0412964820861816 iteration:  2805 epoch:  3\n","LQ image reconstruction loss: 0.6671427488327026 iteration:  2805 epoch:  3\n","Same Different Classification loss: 0.5721727609634399 iteration:  2805 epoch:  3\n","Male Female Classification loss: 0.6794701218605042 iteration:  2805 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.269888401031494 iteration:  2810 epoch:  3\n","LQ image reconstruction loss: 0.5847820043563843 iteration:  2810 epoch:  3\n","Same Different Classification loss: 0.4508911073207855 iteration:  2810 epoch:  3\n","Male Female Classification loss: 0.6153983473777771 iteration:  2810 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.8337244987487793 iteration:  2815 epoch:  3\n","LQ image reconstruction loss: 0.6350293755531311 iteration:  2815 epoch:  3\n","Same Different Classification loss: 0.555949330329895 iteration:  2815 epoch:  3\n","Male Female Classification loss: 0.6013038158416748 iteration:  2815 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.690521240234375 iteration:  2820 epoch:  3\n","LQ image reconstruction loss: 0.5569421052932739 iteration:  2820 epoch:  3\n","Same Different Classification loss: 0.4625997245311737 iteration:  2820 epoch:  3\n","Male Female Classification loss: 0.631260335445404 iteration:  2820 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.807046890258789 iteration:  2825 epoch:  3\n","LQ image reconstruction loss: 0.637188196182251 iteration:  2825 epoch:  3\n","Same Different Classification loss: 0.5174512267112732 iteration:  2825 epoch:  3\n","Male Female Classification loss: 0.6084177494049072 iteration:  2825 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.7766404151916504 iteration:  2830 epoch:  3\n","LQ image reconstruction loss: 0.6606075167655945 iteration:  2830 epoch:  3\n","Same Different Classification loss: 0.529231607913971 iteration:  2830 epoch:  3\n","Male Female Classification loss: 0.5836923122406006 iteration:  2830 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.864574670791626 iteration:  2835 epoch:  3\n","LQ image reconstruction loss: 0.6046382188796997 iteration:  2835 epoch:  3\n","Same Different Classification loss: 0.5153791308403015 iteration:  2835 epoch:  3\n","Male Female Classification loss: 0.6232244372367859 iteration:  2835 epoch:  3\n","----------------------------------------------------\n","HQ image reconstruction loss: 4.660584926605225 iteration:  0 epoch:  4\n","LQ image reconstruction loss: 0.5939716696739197 iteration:  0 epoch:  4\n","Same Different Classification loss: 0.5444547533988953 iteration:  0 epoch:  4\n","Male Female Classification loss: 0.6239055395126343 iteration:  0 epoch:  4\n","----------------------------------------------------\n","Accuracy of SD: 76 %\n","Accuracy of MF: 72 %\n","Mean Accuracy of SD: 59 %\n","Mean Accuracy of MF: 61 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.4466540813446045 iteration:  5 epoch:  4\n","LQ image reconstruction loss: 0.6090835332870483 iteration:  5 epoch:  4\n","Same Different Classification loss: 0.49340954422950745 iteration:  5 epoch:  4\n","Male Female Classification loss: 0.6368706226348877 iteration:  5 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.9424474239349365 iteration:  10 epoch:  4\n","LQ image reconstruction loss: 0.606813371181488 iteration:  10 epoch:  4\n","Same Different Classification loss: 0.5062767267227173 iteration:  10 epoch:  4\n","Male Female Classification loss: 0.6064327955245972 iteration:  10 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.2657101154327393 iteration:  15 epoch:  4\n","LQ image reconstruction loss: 0.5901679992675781 iteration:  15 epoch:  4\n","Same Different Classification loss: 0.5289818644523621 iteration:  15 epoch:  4\n","Male Female Classification loss: 0.6631788015365601 iteration:  15 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.4861392974853516 iteration:  20 epoch:  4\n","LQ image reconstruction loss: 0.6378990411758423 iteration:  20 epoch:  4\n","Same Different Classification loss: 0.5261293649673462 iteration:  20 epoch:  4\n","Male Female Classification loss: 0.6086125373840332 iteration:  20 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.7697157859802246 iteration:  25 epoch:  4\n","LQ image reconstruction loss: 0.5313152074813843 iteration:  25 epoch:  4\n","Same Different Classification loss: 0.4479907751083374 iteration:  25 epoch:  4\n","Male Female Classification loss: 0.6735290288925171 iteration:  25 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.3323769569396973 iteration:  30 epoch:  4\n","LQ image reconstruction loss: 0.5822596549987793 iteration:  30 epoch:  4\n","Same Different Classification loss: 0.48943138122558594 iteration:  30 epoch:  4\n","Male Female Classification loss: 0.6639658212661743 iteration:  30 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.968764543533325 iteration:  35 epoch:  4\n","LQ image reconstruction loss: 0.6342299580574036 iteration:  35 epoch:  4\n","Same Different Classification loss: 0.5247597694396973 iteration:  35 epoch:  4\n","Male Female Classification loss: 0.6546587944030762 iteration:  35 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.1168084144592285 iteration:  40 epoch:  4\n","LQ image reconstruction loss: 0.6403169631958008 iteration:  40 epoch:  4\n","Same Different Classification loss: 0.5683660507202148 iteration:  40 epoch:  4\n","Male Female Classification loss: 0.6453835368156433 iteration:  40 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.8933277130126953 iteration:  45 epoch:  4\n","LQ image reconstruction loss: 0.5870405435562134 iteration:  45 epoch:  4\n","Same Different Classification loss: 0.4689323902130127 iteration:  45 epoch:  4\n","Male Female Classification loss: 0.6223995089530945 iteration:  45 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.4439027309417725 iteration:  50 epoch:  4\n","LQ image reconstruction loss: 0.6360309720039368 iteration:  50 epoch:  4\n","Same Different Classification loss: 0.5504676699638367 iteration:  50 epoch:  4\n","Male Female Classification loss: 0.6213399171829224 iteration:  50 epoch:  4\n","----------------------------------------------------\n","Accuracy of SD: 71 %\n","Accuracy of MF: 70 %\n","Mean Accuracy of SD: 59 %\n","Mean Accuracy of MF: 61 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.9282236099243164 iteration:  55 epoch:  4\n","LQ image reconstruction loss: 0.6101144552230835 iteration:  55 epoch:  4\n","Same Different Classification loss: 0.5009204745292664 iteration:  55 epoch:  4\n","Male Female Classification loss: 0.5897517800331116 iteration:  55 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.1813130378723145 iteration:  60 epoch:  4\n","LQ image reconstruction loss: 0.5945004820823669 iteration:  60 epoch:  4\n","Same Different Classification loss: 0.5035004615783691 iteration:  60 epoch:  4\n","Male Female Classification loss: 0.6123412847518921 iteration:  60 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.7334563732147217 iteration:  65 epoch:  4\n","LQ image reconstruction loss: 0.6845636367797852 iteration:  65 epoch:  4\n","Same Different Classification loss: 0.6086052060127258 iteration:  65 epoch:  4\n","Male Female Classification loss: 0.5567117929458618 iteration:  65 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.9317967891693115 iteration:  70 epoch:  4\n","LQ image reconstruction loss: 0.600715696811676 iteration:  70 epoch:  4\n","Same Different Classification loss: 0.4788705110549927 iteration:  70 epoch:  4\n","Male Female Classification loss: 0.6051700115203857 iteration:  70 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.918360948562622 iteration:  75 epoch:  4\n","LQ image reconstruction loss: 0.5709670782089233 iteration:  75 epoch:  4\n","Same Different Classification loss: 0.4822250306606293 iteration:  75 epoch:  4\n","Male Female Classification loss: 0.6042844653129578 iteration:  75 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.6863515377044678 iteration:  80 epoch:  4\n","LQ image reconstruction loss: 0.7531701922416687 iteration:  80 epoch:  4\n","Same Different Classification loss: 0.6084547638893127 iteration:  80 epoch:  4\n","Male Female Classification loss: 0.5816012620925903 iteration:  80 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.831852436065674 iteration:  85 epoch:  4\n","LQ image reconstruction loss: 0.6288596391677856 iteration:  85 epoch:  4\n","Same Different Classification loss: 0.5238121151924133 iteration:  85 epoch:  4\n","Male Female Classification loss: 0.6242188811302185 iteration:  85 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.258234977722168 iteration:  90 epoch:  4\n","LQ image reconstruction loss: 0.6152416467666626 iteration:  90 epoch:  4\n","Same Different Classification loss: 0.537519097328186 iteration:  90 epoch:  4\n","Male Female Classification loss: 0.613039493560791 iteration:  90 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.2618865966796875 iteration:  95 epoch:  4\n","LQ image reconstruction loss: 0.5900399088859558 iteration:  95 epoch:  4\n","Same Different Classification loss: 0.5033743977546692 iteration:  95 epoch:  4\n","Male Female Classification loss: 0.6688604950904846 iteration:  95 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.735975742340088 iteration:  100 epoch:  4\n","LQ image reconstruction loss: 0.5519050359725952 iteration:  100 epoch:  4\n","Same Different Classification loss: 0.4806479513645172 iteration:  100 epoch:  4\n","Male Female Classification loss: 0.6203969717025757 iteration:  100 epoch:  4\n","----------------------------------------------------\n","Accuracy of SD: 72 %\n","Accuracy of MF: 73 %\n","Mean Accuracy of SD: 59 %\n","Mean Accuracy of MF: 61 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.801828145980835 iteration:  105 epoch:  4\n","LQ image reconstruction loss: 0.5999119877815247 iteration:  105 epoch:  4\n","Same Different Classification loss: 0.47287437319755554 iteration:  105 epoch:  4\n","Male Female Classification loss: 0.562578558921814 iteration:  105 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.3574304580688477 iteration:  110 epoch:  4\n","LQ image reconstruction loss: 0.5742462873458862 iteration:  110 epoch:  4\n","Same Different Classification loss: 0.4552457928657532 iteration:  110 epoch:  4\n","Male Female Classification loss: 0.6210348606109619 iteration:  110 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.0889551639556885 iteration:  115 epoch:  4\n","LQ image reconstruction loss: 0.7035822868347168 iteration:  115 epoch:  4\n","Same Different Classification loss: 0.5933082699775696 iteration:  115 epoch:  4\n","Male Female Classification loss: 0.6042514443397522 iteration:  115 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.1402862071990967 iteration:  120 epoch:  4\n","LQ image reconstruction loss: 0.7213997840881348 iteration:  120 epoch:  4\n","Same Different Classification loss: 0.6200850605964661 iteration:  120 epoch:  4\n","Male Female Classification loss: 0.647449254989624 iteration:  120 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.1580607891082764 iteration:  125 epoch:  4\n","LQ image reconstruction loss: 0.5829124450683594 iteration:  125 epoch:  4\n","Same Different Classification loss: 0.4511086344718933 iteration:  125 epoch:  4\n","Male Female Classification loss: 0.5501860976219177 iteration:  125 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.022794246673584 iteration:  130 epoch:  4\n","LQ image reconstruction loss: 0.6708880662918091 iteration:  130 epoch:  4\n","Same Different Classification loss: 0.5731677412986755 iteration:  130 epoch:  4\n","Male Female Classification loss: 0.5819424986839294 iteration:  130 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.673799991607666 iteration:  135 epoch:  4\n","LQ image reconstruction loss: 0.7470234036445618 iteration:  135 epoch:  4\n","Same Different Classification loss: 0.6600602865219116 iteration:  135 epoch:  4\n","Male Female Classification loss: 0.5888103246688843 iteration:  135 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.2026278972625732 iteration:  140 epoch:  4\n","LQ image reconstruction loss: 0.5772725939750671 iteration:  140 epoch:  4\n","Same Different Classification loss: 0.5171968340873718 iteration:  140 epoch:  4\n","Male Female Classification loss: 0.6789456009864807 iteration:  140 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.875049591064453 iteration:  145 epoch:  4\n","LQ image reconstruction loss: 0.5969225168228149 iteration:  145 epoch:  4\n","Same Different Classification loss: 0.460779070854187 iteration:  145 epoch:  4\n","Male Female Classification loss: 0.5963719487190247 iteration:  145 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.777472972869873 iteration:  150 epoch:  4\n","LQ image reconstruction loss: 0.5503854751586914 iteration:  150 epoch:  4\n","Same Different Classification loss: 0.4235156774520874 iteration:  150 epoch:  4\n","Male Female Classification loss: 0.648847758769989 iteration:  150 epoch:  4\n","----------------------------------------------------\n","Accuracy of SD: 72 %\n","Accuracy of MF: 71 %\n","Mean Accuracy of SD: 59 %\n","Mean Accuracy of MF: 61 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.747281789779663 iteration:  155 epoch:  4\n","LQ image reconstruction loss: 0.6239305734634399 iteration:  155 epoch:  4\n","Same Different Classification loss: 0.5043161511421204 iteration:  155 epoch:  4\n","Male Female Classification loss: 0.6511567831039429 iteration:  155 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.4054033756256104 iteration:  160 epoch:  4\n","LQ image reconstruction loss: 0.5141297578811646 iteration:  160 epoch:  4\n","Same Different Classification loss: 0.4656856060028076 iteration:  160 epoch:  4\n","Male Female Classification loss: 0.6497743129730225 iteration:  160 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.7448885440826416 iteration:  165 epoch:  4\n","LQ image reconstruction loss: 0.46583589911460876 iteration:  165 epoch:  4\n","Same Different Classification loss: 0.4311925768852234 iteration:  165 epoch:  4\n","Male Female Classification loss: 0.6667049527168274 iteration:  165 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.0621490478515625 iteration:  170 epoch:  4\n","LQ image reconstruction loss: 0.5137506127357483 iteration:  170 epoch:  4\n","Same Different Classification loss: 0.4590401351451874 iteration:  170 epoch:  4\n","Male Female Classification loss: 0.637986958026886 iteration:  170 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.3052170276641846 iteration:  175 epoch:  4\n","LQ image reconstruction loss: 0.5058287978172302 iteration:  175 epoch:  4\n","Same Different Classification loss: 0.4127318859100342 iteration:  175 epoch:  4\n","Male Female Classification loss: 0.5736379027366638 iteration:  175 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.587423801422119 iteration:  180 epoch:  4\n","LQ image reconstruction loss: 0.6331375241279602 iteration:  180 epoch:  4\n","Same Different Classification loss: 0.5350534915924072 iteration:  180 epoch:  4\n","Male Female Classification loss: 0.5842118263244629 iteration:  180 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.2615013122558594 iteration:  185 epoch:  4\n","LQ image reconstruction loss: 0.6464303731918335 iteration:  185 epoch:  4\n","Same Different Classification loss: 0.48700836300849915 iteration:  185 epoch:  4\n","Male Female Classification loss: 0.5910051465034485 iteration:  185 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.473881721496582 iteration:  190 epoch:  4\n","LQ image reconstruction loss: 0.6054061055183411 iteration:  190 epoch:  4\n","Same Different Classification loss: 0.5034016966819763 iteration:  190 epoch:  4\n","Male Female Classification loss: 0.6356275081634521 iteration:  190 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.198803424835205 iteration:  195 epoch:  4\n","LQ image reconstruction loss: 0.6749427318572998 iteration:  195 epoch:  4\n","Same Different Classification loss: 0.5013294816017151 iteration:  195 epoch:  4\n","Male Female Classification loss: 0.5512884259223938 iteration:  195 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.39931321144104 iteration:  200 epoch:  4\n","LQ image reconstruction loss: 0.6147990822792053 iteration:  200 epoch:  4\n","Same Different Classification loss: 0.5679624676704407 iteration:  200 epoch:  4\n","Male Female Classification loss: 0.7012842297554016 iteration:  200 epoch:  4\n","----------------------------------------------------\n","Accuracy of SD: 71 %\n","Accuracy of MF: 60 %\n","Mean Accuracy of SD: 59 %\n","Mean Accuracy of MF: 61 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.4515509605407715 iteration:  205 epoch:  4\n","LQ image reconstruction loss: 0.6093478202819824 iteration:  205 epoch:  4\n","Same Different Classification loss: 0.5319685935974121 iteration:  205 epoch:  4\n","Male Female Classification loss: 0.6742179989814758 iteration:  205 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.8872556686401367 iteration:  210 epoch:  4\n","LQ image reconstruction loss: 0.6143290996551514 iteration:  210 epoch:  4\n","Same Different Classification loss: 0.47768115997314453 iteration:  210 epoch:  4\n","Male Female Classification loss: 0.5869707465171814 iteration:  210 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.6612160205841064 iteration:  215 epoch:  4\n","LQ image reconstruction loss: 0.5779149532318115 iteration:  215 epoch:  4\n","Same Different Classification loss: 0.4750254452228546 iteration:  215 epoch:  4\n","Male Female Classification loss: 0.6428720951080322 iteration:  215 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.3558902740478516 iteration:  220 epoch:  4\n","LQ image reconstruction loss: 0.6476301550865173 iteration:  220 epoch:  4\n","Same Different Classification loss: 0.575526237487793 iteration:  220 epoch:  4\n","Male Female Classification loss: 0.6156310439109802 iteration:  220 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.486337661743164 iteration:  225 epoch:  4\n","LQ image reconstruction loss: 0.6824250221252441 iteration:  225 epoch:  4\n","Same Different Classification loss: 0.5659838318824768 iteration:  225 epoch:  4\n","Male Female Classification loss: 0.6213288903236389 iteration:  225 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.0502355098724365 iteration:  230 epoch:  4\n","LQ image reconstruction loss: 0.5782612562179565 iteration:  230 epoch:  4\n","Same Different Classification loss: 0.4890934228897095 iteration:  230 epoch:  4\n","Male Female Classification loss: 0.5834393501281738 iteration:  230 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.432483673095703 iteration:  235 epoch:  4\n","LQ image reconstruction loss: 0.6384742259979248 iteration:  235 epoch:  4\n","Same Different Classification loss: 0.5316603779792786 iteration:  235 epoch:  4\n","Male Female Classification loss: 0.5930720567703247 iteration:  235 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.8296823501586914 iteration:  240 epoch:  4\n","LQ image reconstruction loss: 0.5773736834526062 iteration:  240 epoch:  4\n","Same Different Classification loss: 0.5017770528793335 iteration:  240 epoch:  4\n","Male Female Classification loss: 0.6037340760231018 iteration:  240 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.059520721435547 iteration:  245 epoch:  4\n","LQ image reconstruction loss: 0.5364842414855957 iteration:  245 epoch:  4\n","Same Different Classification loss: 0.4599963426589966 iteration:  245 epoch:  4\n","Male Female Classification loss: 0.626162052154541 iteration:  245 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.656963586807251 iteration:  250 epoch:  4\n","LQ image reconstruction loss: 0.5264663696289062 iteration:  250 epoch:  4\n","Same Different Classification loss: 0.45183178782463074 iteration:  250 epoch:  4\n","Male Female Classification loss: 0.5870739221572876 iteration:  250 epoch:  4\n","----------------------------------------------------\n","Accuracy of SD: 82 %\n","Accuracy of MF: 76 %\n","Mean Accuracy of SD: 60 %\n","Mean Accuracy of MF: 61 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.004110813140869 iteration:  255 epoch:  4\n","LQ image reconstruction loss: 0.56638103723526 iteration:  255 epoch:  4\n","Same Different Classification loss: 0.4757521152496338 iteration:  255 epoch:  4\n","Male Female Classification loss: 0.5794919729232788 iteration:  255 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.7870779037475586 iteration:  260 epoch:  4\n","LQ image reconstruction loss: 0.6300250291824341 iteration:  260 epoch:  4\n","Same Different Classification loss: 0.5573570132255554 iteration:  260 epoch:  4\n","Male Female Classification loss: 0.6472216844558716 iteration:  260 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.2005555629730225 iteration:  265 epoch:  4\n","LQ image reconstruction loss: 0.5355067253112793 iteration:  265 epoch:  4\n","Same Different Classification loss: 0.4741640090942383 iteration:  265 epoch:  4\n","Male Female Classification loss: 0.6578118205070496 iteration:  265 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.763385772705078 iteration:  270 epoch:  4\n","LQ image reconstruction loss: 0.5962608456611633 iteration:  270 epoch:  4\n","Same Different Classification loss: 0.4862896800041199 iteration:  270 epoch:  4\n","Male Female Classification loss: 0.6053545475006104 iteration:  270 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.249361991882324 iteration:  275 epoch:  4\n","LQ image reconstruction loss: 0.5735076665878296 iteration:  275 epoch:  4\n","Same Different Classification loss: 0.507351815700531 iteration:  275 epoch:  4\n","Male Female Classification loss: 0.6421785950660706 iteration:  275 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.2684311866760254 iteration:  280 epoch:  4\n","LQ image reconstruction loss: 0.6333180665969849 iteration:  280 epoch:  4\n","Same Different Classification loss: 0.5299434661865234 iteration:  280 epoch:  4\n","Male Female Classification loss: 0.5643495917320251 iteration:  280 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.566899061203003 iteration:  285 epoch:  4\n","LQ image reconstruction loss: 0.611345648765564 iteration:  285 epoch:  4\n","Same Different Classification loss: 0.4891470968723297 iteration:  285 epoch:  4\n","Male Female Classification loss: 0.5880811214447021 iteration:  285 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.298039436340332 iteration:  290 epoch:  4\n","LQ image reconstruction loss: 0.5812486410140991 iteration:  290 epoch:  4\n","Same Different Classification loss: 0.4900745749473572 iteration:  290 epoch:  4\n","Male Female Classification loss: 0.6168361902236938 iteration:  290 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.893157958984375 iteration:  295 epoch:  4\n","LQ image reconstruction loss: 0.5843867063522339 iteration:  295 epoch:  4\n","Same Different Classification loss: 0.4881475865840912 iteration:  295 epoch:  4\n","Male Female Classification loss: 0.6249231100082397 iteration:  295 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.050891160964966 iteration:  300 epoch:  4\n","LQ image reconstruction loss: 0.5419749617576599 iteration:  300 epoch:  4\n","Same Different Classification loss: 0.5097070932388306 iteration:  300 epoch:  4\n","Male Female Classification loss: 0.6963069438934326 iteration:  300 epoch:  4\n","----------------------------------------------------\n","Accuracy of SD: 70 %\n","Accuracy of MF: 67 %\n","Mean Accuracy of SD: 60 %\n","Mean Accuracy of MF: 61 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.2218825817108154 iteration:  305 epoch:  4\n","LQ image reconstruction loss: 0.5924888849258423 iteration:  305 epoch:  4\n","Same Different Classification loss: 0.5052613615989685 iteration:  305 epoch:  4\n","Male Female Classification loss: 0.6140184998512268 iteration:  305 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.3436970710754395 iteration:  310 epoch:  4\n","LQ image reconstruction loss: 0.6148367524147034 iteration:  310 epoch:  4\n","Same Different Classification loss: 0.5087645053863525 iteration:  310 epoch:  4\n","Male Female Classification loss: 0.6794579029083252 iteration:  310 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.601349115371704 iteration:  315 epoch:  4\n","LQ image reconstruction loss: 0.5514426231384277 iteration:  315 epoch:  4\n","Same Different Classification loss: 0.45720088481903076 iteration:  315 epoch:  4\n","Male Female Classification loss: 0.6267637014389038 iteration:  315 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.7699317932128906 iteration:  320 epoch:  4\n","LQ image reconstruction loss: 0.6203677654266357 iteration:  320 epoch:  4\n","Same Different Classification loss: 0.4944494366645813 iteration:  320 epoch:  4\n","Male Female Classification loss: 0.6038774251937866 iteration:  320 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.3093743324279785 iteration:  325 epoch:  4\n","LQ image reconstruction loss: 0.786478579044342 iteration:  325 epoch:  4\n","Same Different Classification loss: 0.6504356861114502 iteration:  325 epoch:  4\n","Male Female Classification loss: 0.6184236407279968 iteration:  325 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.9074831008911133 iteration:  330 epoch:  4\n","LQ image reconstruction loss: 0.5604918003082275 iteration:  330 epoch:  4\n","Same Different Classification loss: 0.4937830865383148 iteration:  330 epoch:  4\n","Male Female Classification loss: 0.6374942660331726 iteration:  330 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.1596901416778564 iteration:  335 epoch:  4\n","LQ image reconstruction loss: 0.6683433651924133 iteration:  335 epoch:  4\n","Same Different Classification loss: 0.5214253067970276 iteration:  335 epoch:  4\n","Male Female Classification loss: 0.6023434996604919 iteration:  335 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.6865146160125732 iteration:  340 epoch:  4\n","LQ image reconstruction loss: 0.6079457998275757 iteration:  340 epoch:  4\n","Same Different Classification loss: 0.5529206991195679 iteration:  340 epoch:  4\n","Male Female Classification loss: 0.6681951284408569 iteration:  340 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.4882802963256836 iteration:  345 epoch:  4\n","LQ image reconstruction loss: 0.5920441150665283 iteration:  345 epoch:  4\n","Same Different Classification loss: 0.515419602394104 iteration:  345 epoch:  4\n","Male Female Classification loss: 0.6352470517158508 iteration:  345 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.6044931411743164 iteration:  350 epoch:  4\n","LQ image reconstruction loss: 0.5119017958641052 iteration:  350 epoch:  4\n","Same Different Classification loss: 0.4226992726325989 iteration:  350 epoch:  4\n","Male Female Classification loss: 0.5801661014556885 iteration:  350 epoch:  4\n","----------------------------------------------------\n","Accuracy of SD: 76 %\n","Accuracy of MF: 61 %\n","Mean Accuracy of SD: 60 %\n","Mean Accuracy of MF: 61 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.0848872661590576 iteration:  355 epoch:  4\n","LQ image reconstruction loss: 0.6129512786865234 iteration:  355 epoch:  4\n","Same Different Classification loss: 0.5415145754814148 iteration:  355 epoch:  4\n","Male Female Classification loss: 0.7038615942001343 iteration:  355 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.103437900543213 iteration:  360 epoch:  4\n","LQ image reconstruction loss: 0.632207453250885 iteration:  360 epoch:  4\n","Same Different Classification loss: 0.5427047610282898 iteration:  360 epoch:  4\n","Male Female Classification loss: 0.6434775590896606 iteration:  360 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.3473851680755615 iteration:  365 epoch:  4\n","LQ image reconstruction loss: 0.7020628452301025 iteration:  365 epoch:  4\n","Same Different Classification loss: 0.5879783034324646 iteration:  365 epoch:  4\n","Male Female Classification loss: 0.5681468844413757 iteration:  365 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.460456609725952 iteration:  370 epoch:  4\n","LQ image reconstruction loss: 0.5687023401260376 iteration:  370 epoch:  4\n","Same Different Classification loss: 0.5224246382713318 iteration:  370 epoch:  4\n","Male Female Classification loss: 0.6763554215431213 iteration:  370 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.7138900756835938 iteration:  375 epoch:  4\n","LQ image reconstruction loss: 0.5619799494743347 iteration:  375 epoch:  4\n","Same Different Classification loss: 0.49373653531074524 iteration:  375 epoch:  4\n","Male Female Classification loss: 0.6089810729026794 iteration:  375 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.7456417083740234 iteration:  380 epoch:  4\n","LQ image reconstruction loss: 0.5553402304649353 iteration:  380 epoch:  4\n","Same Different Classification loss: 0.4650071859359741 iteration:  380 epoch:  4\n","Male Female Classification loss: 0.5798135995864868 iteration:  380 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.931290626525879 iteration:  385 epoch:  4\n","LQ image reconstruction loss: 0.6964489817619324 iteration:  385 epoch:  4\n","Same Different Classification loss: 0.5781458020210266 iteration:  385 epoch:  4\n","Male Female Classification loss: 0.6478729844093323 iteration:  385 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.827983856201172 iteration:  390 epoch:  4\n","LQ image reconstruction loss: 0.5277926325798035 iteration:  390 epoch:  4\n","Same Different Classification loss: 0.4684935510158539 iteration:  390 epoch:  4\n","Male Female Classification loss: 0.639642596244812 iteration:  390 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.2717418670654297 iteration:  395 epoch:  4\n","LQ image reconstruction loss: 0.6397876739501953 iteration:  395 epoch:  4\n","Same Different Classification loss: 0.5666204690933228 iteration:  395 epoch:  4\n","Male Female Classification loss: 0.6146910786628723 iteration:  395 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.180969715118408 iteration:  400 epoch:  4\n","LQ image reconstruction loss: 0.5956462621688843 iteration:  400 epoch:  4\n","Same Different Classification loss: 0.48933932185173035 iteration:  400 epoch:  4\n","Male Female Classification loss: 0.5396034717559814 iteration:  400 epoch:  4\n","----------------------------------------------------\n","Accuracy of SD: 72 %\n","Accuracy of MF: 73 %\n","Mean Accuracy of SD: 60 %\n","Mean Accuracy of MF: 61 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.770798683166504 iteration:  405 epoch:  4\n","LQ image reconstruction loss: 0.5645325183868408 iteration:  405 epoch:  4\n","Same Different Classification loss: 0.47739142179489136 iteration:  405 epoch:  4\n","Male Female Classification loss: 0.5648212432861328 iteration:  405 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.9217262268066406 iteration:  410 epoch:  4\n","LQ image reconstruction loss: 0.592666506767273 iteration:  410 epoch:  4\n","Same Different Classification loss: 0.5174688696861267 iteration:  410 epoch:  4\n","Male Female Classification loss: 0.5905808806419373 iteration:  410 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.0999085903167725 iteration:  415 epoch:  4\n","LQ image reconstruction loss: 0.558277428150177 iteration:  415 epoch:  4\n","Same Different Classification loss: 0.49251991510391235 iteration:  415 epoch:  4\n","Male Female Classification loss: 0.6223636865615845 iteration:  415 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.347477674484253 iteration:  420 epoch:  4\n","LQ image reconstruction loss: 0.5245834589004517 iteration:  420 epoch:  4\n","Same Different Classification loss: 0.495869517326355 iteration:  420 epoch:  4\n","Male Female Classification loss: 0.6843013167381287 iteration:  420 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.6338770389556885 iteration:  425 epoch:  4\n","LQ image reconstruction loss: 0.511367917060852 iteration:  425 epoch:  4\n","Same Different Classification loss: 0.46275806427001953 iteration:  425 epoch:  4\n","Male Female Classification loss: 0.6505685448646545 iteration:  425 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.717156171798706 iteration:  430 epoch:  4\n","LQ image reconstruction loss: 0.5224116444587708 iteration:  430 epoch:  4\n","Same Different Classification loss: 0.45835795998573303 iteration:  430 epoch:  4\n","Male Female Classification loss: 0.615356981754303 iteration:  430 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.2084603309631348 iteration:  435 epoch:  4\n","LQ image reconstruction loss: 0.62403804063797 iteration:  435 epoch:  4\n","Same Different Classification loss: 0.5284247994422913 iteration:  435 epoch:  4\n","Male Female Classification loss: 0.5669204592704773 iteration:  435 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.439173936843872 iteration:  440 epoch:  4\n","LQ image reconstruction loss: 0.5612199902534485 iteration:  440 epoch:  4\n","Same Different Classification loss: 0.48901692032814026 iteration:  440 epoch:  4\n","Male Female Classification loss: 0.6113032698631287 iteration:  440 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.2857751846313477 iteration:  445 epoch:  4\n","LQ image reconstruction loss: 0.6130685806274414 iteration:  445 epoch:  4\n","Same Different Classification loss: 0.5397073030471802 iteration:  445 epoch:  4\n","Male Female Classification loss: 0.6068933010101318 iteration:  445 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.3852834701538086 iteration:  450 epoch:  4\n","LQ image reconstruction loss: 0.6900783777236938 iteration:  450 epoch:  4\n","Same Different Classification loss: 0.5861987471580505 iteration:  450 epoch:  4\n","Male Female Classification loss: 0.6714627742767334 iteration:  450 epoch:  4\n","----------------------------------------------------\n","Accuracy of SD: 78 %\n","Accuracy of MF: 64 %\n","Mean Accuracy of SD: 60 %\n","Mean Accuracy of MF: 61 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.125669002532959 iteration:  455 epoch:  4\n","LQ image reconstruction loss: 0.5809042453765869 iteration:  455 epoch:  4\n","Same Different Classification loss: 0.5083033442497253 iteration:  455 epoch:  4\n","Male Female Classification loss: 0.6184187531471252 iteration:  455 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.7757954597473145 iteration:  460 epoch:  4\n","LQ image reconstruction loss: 0.6750142574310303 iteration:  460 epoch:  4\n","Same Different Classification loss: 0.5581961870193481 iteration:  460 epoch:  4\n","Male Female Classification loss: 0.6320168972015381 iteration:  460 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.0763325691223145 iteration:  465 epoch:  4\n","LQ image reconstruction loss: 0.5996873378753662 iteration:  465 epoch:  4\n","Same Different Classification loss: 0.5041564106941223 iteration:  465 epoch:  4\n","Male Female Classification loss: 0.614457368850708 iteration:  465 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.2419376373291016 iteration:  470 epoch:  4\n","LQ image reconstruction loss: 0.556187093257904 iteration:  470 epoch:  4\n","Same Different Classification loss: 0.47676101326942444 iteration:  470 epoch:  4\n","Male Female Classification loss: 0.6084116101264954 iteration:  470 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.5698325634002686 iteration:  475 epoch:  4\n","LQ image reconstruction loss: 0.463918000459671 iteration:  475 epoch:  4\n","Same Different Classification loss: 0.4102973937988281 iteration:  475 epoch:  4\n","Male Female Classification loss: 0.6578584313392639 iteration:  475 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.313626527786255 iteration:  480 epoch:  4\n","LQ image reconstruction loss: 0.6385453343391418 iteration:  480 epoch:  4\n","Same Different Classification loss: 0.5730651021003723 iteration:  480 epoch:  4\n","Male Female Classification loss: 0.6154500246047974 iteration:  480 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.8619701862335205 iteration:  485 epoch:  4\n","LQ image reconstruction loss: 0.5428066253662109 iteration:  485 epoch:  4\n","Same Different Classification loss: 0.4824075996875763 iteration:  485 epoch:  4\n","Male Female Classification loss: 0.6331554651260376 iteration:  485 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.9689512252807617 iteration:  490 epoch:  4\n","LQ image reconstruction loss: 0.6570350527763367 iteration:  490 epoch:  4\n","Same Different Classification loss: 0.5388423800468445 iteration:  490 epoch:  4\n","Male Female Classification loss: 0.5520433187484741 iteration:  490 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.467076301574707 iteration:  495 epoch:  4\n","LQ image reconstruction loss: 0.6357175707817078 iteration:  495 epoch:  4\n","Same Different Classification loss: 0.5028455853462219 iteration:  495 epoch:  4\n","Male Female Classification loss: 0.5959330797195435 iteration:  495 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.5218231678009033 iteration:  500 epoch:  4\n","LQ image reconstruction loss: 0.582355797290802 iteration:  500 epoch:  4\n","Same Different Classification loss: 0.4593784809112549 iteration:  500 epoch:  4\n","Male Female Classification loss: 0.6084619760513306 iteration:  500 epoch:  4\n","----------------------------------------------------\n","Accuracy of SD: 70 %\n","Accuracy of MF: 75 %\n","Mean Accuracy of SD: 60 %\n","Mean Accuracy of MF: 61 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.4879143238067627 iteration:  505 epoch:  4\n","LQ image reconstruction loss: 0.5044825077056885 iteration:  505 epoch:  4\n","Same Different Classification loss: 0.4551416337490082 iteration:  505 epoch:  4\n","Male Female Classification loss: 0.6753240823745728 iteration:  505 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.3380250930786133 iteration:  510 epoch:  4\n","LQ image reconstruction loss: 0.513266921043396 iteration:  510 epoch:  4\n","Same Different Classification loss: 0.46612074971199036 iteration:  510 epoch:  4\n","Male Female Classification loss: 0.6581017971038818 iteration:  510 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.5830636024475098 iteration:  515 epoch:  4\n","LQ image reconstruction loss: 0.5594974160194397 iteration:  515 epoch:  4\n","Same Different Classification loss: 0.5134756565093994 iteration:  515 epoch:  4\n","Male Female Classification loss: 0.6122868657112122 iteration:  515 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.8589086532592773 iteration:  520 epoch:  4\n","LQ image reconstruction loss: 0.5641078948974609 iteration:  520 epoch:  4\n","Same Different Classification loss: 0.4891921281814575 iteration:  520 epoch:  4\n","Male Female Classification loss: 0.5993808507919312 iteration:  520 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.656008243560791 iteration:  525 epoch:  4\n","LQ image reconstruction loss: 0.5934643149375916 iteration:  525 epoch:  4\n","Same Different Classification loss: 0.4907110929489136 iteration:  525 epoch:  4\n","Male Female Classification loss: 0.6252382397651672 iteration:  525 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.640756130218506 iteration:  530 epoch:  4\n","LQ image reconstruction loss: 0.6211326718330383 iteration:  530 epoch:  4\n","Same Different Classification loss: 0.5035355091094971 iteration:  530 epoch:  4\n","Male Female Classification loss: 0.5997936129570007 iteration:  530 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.928968906402588 iteration:  535 epoch:  4\n","LQ image reconstruction loss: 0.5510436296463013 iteration:  535 epoch:  4\n","Same Different Classification loss: 0.5004371404647827 iteration:  535 epoch:  4\n","Male Female Classification loss: 0.6823744177818298 iteration:  535 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.4594714641571045 iteration:  540 epoch:  4\n","LQ image reconstruction loss: 0.6024996638298035 iteration:  540 epoch:  4\n","Same Different Classification loss: 0.5501002073287964 iteration:  540 epoch:  4\n","Male Female Classification loss: 0.6120375990867615 iteration:  540 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.8262672424316406 iteration:  545 epoch:  4\n","LQ image reconstruction loss: 0.6912286281585693 iteration:  545 epoch:  4\n","Same Different Classification loss: 0.5970226526260376 iteration:  545 epoch:  4\n","Male Female Classification loss: 0.6847527027130127 iteration:  545 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.591840982437134 iteration:  550 epoch:  4\n","LQ image reconstruction loss: 0.6048589944839478 iteration:  550 epoch:  4\n","Same Different Classification loss: 0.4749860465526581 iteration:  550 epoch:  4\n","Male Female Classification loss: 0.5854135751724243 iteration:  550 epoch:  4\n","----------------------------------------------------\n","Accuracy of SD: 77 %\n","Accuracy of MF: 72 %\n","Mean Accuracy of SD: 60 %\n","Mean Accuracy of MF: 61 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.685708522796631 iteration:  555 epoch:  4\n","LQ image reconstruction loss: 0.5402230620384216 iteration:  555 epoch:  4\n","Same Different Classification loss: 0.4922645092010498 iteration:  555 epoch:  4\n","Male Female Classification loss: 0.6332660913467407 iteration:  555 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.6714248657226562 iteration:  560 epoch:  4\n","LQ image reconstruction loss: 0.5668179392814636 iteration:  560 epoch:  4\n","Same Different Classification loss: 0.48637744784355164 iteration:  560 epoch:  4\n","Male Female Classification loss: 0.6326267123222351 iteration:  560 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.5566837787628174 iteration:  565 epoch:  4\n","LQ image reconstruction loss: 0.553231954574585 iteration:  565 epoch:  4\n","Same Different Classification loss: 0.4567164480686188 iteration:  565 epoch:  4\n","Male Female Classification loss: 0.5851563215255737 iteration:  565 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.588146209716797 iteration:  570 epoch:  4\n","LQ image reconstruction loss: 0.6223841905593872 iteration:  570 epoch:  4\n","Same Different Classification loss: 0.5236170887947083 iteration:  570 epoch:  4\n","Male Female Classification loss: 0.5556687712669373 iteration:  570 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.724344491958618 iteration:  575 epoch:  4\n","LQ image reconstruction loss: 0.6429212689399719 iteration:  575 epoch:  4\n","Same Different Classification loss: 0.535301685333252 iteration:  575 epoch:  4\n","Male Female Classification loss: 0.6344008445739746 iteration:  575 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.0612382888793945 iteration:  580 epoch:  4\n","LQ image reconstruction loss: 0.5982908010482788 iteration:  580 epoch:  4\n","Same Different Classification loss: 0.4853633642196655 iteration:  580 epoch:  4\n","Male Female Classification loss: 0.6298113465309143 iteration:  580 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.8684420585632324 iteration:  585 epoch:  4\n","LQ image reconstruction loss: 0.7046388387680054 iteration:  585 epoch:  4\n","Same Different Classification loss: 0.5808159112930298 iteration:  585 epoch:  4\n","Male Female Classification loss: 0.5554887652397156 iteration:  585 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.4331836700439453 iteration:  590 epoch:  4\n","LQ image reconstruction loss: 0.5135313272476196 iteration:  590 epoch:  4\n","Same Different Classification loss: 0.43092355132102966 iteration:  590 epoch:  4\n","Male Female Classification loss: 0.5539500117301941 iteration:  590 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.3110361099243164 iteration:  595 epoch:  4\n","LQ image reconstruction loss: 0.5336872339248657 iteration:  595 epoch:  4\n","Same Different Classification loss: 0.49281421303749084 iteration:  595 epoch:  4\n","Male Female Classification loss: 0.6817103028297424 iteration:  595 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 4.054919719696045 iteration:  600 epoch:  4\n","LQ image reconstruction loss: 0.656934380531311 iteration:  600 epoch:  4\n","Same Different Classification loss: 0.5352986454963684 iteration:  600 epoch:  4\n","Male Female Classification loss: 0.545855700969696 iteration:  600 epoch:  4\n","----------------------------------------------------\n","Accuracy of SD: 81 %\n","Accuracy of MF: 66 %\n","Mean Accuracy of SD: 60 %\n","Mean Accuracy of MF: 61 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.7962546348571777 iteration:  605 epoch:  4\n","LQ image reconstruction loss: 0.6261850595474243 iteration:  605 epoch:  4\n","Same Different Classification loss: 0.505249559879303 iteration:  605 epoch:  4\n","Male Female Classification loss: 0.602576732635498 iteration:  605 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.0322425365448 iteration:  610 epoch:  4\n","LQ image reconstruction loss: 0.5945719480514526 iteration:  610 epoch:  4\n","Same Different Classification loss: 0.5121830701828003 iteration:  610 epoch:  4\n","Male Female Classification loss: 0.6040624380111694 iteration:  610 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.7641570568084717 iteration:  615 epoch:  4\n","LQ image reconstruction loss: 0.6013047099113464 iteration:  615 epoch:  4\n","Same Different Classification loss: 0.5168559551239014 iteration:  615 epoch:  4\n","Male Female Classification loss: 0.5791633129119873 iteration:  615 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.7248833179473877 iteration:  620 epoch:  4\n","LQ image reconstruction loss: 0.5641751885414124 iteration:  620 epoch:  4\n","Same Different Classification loss: 0.486176460981369 iteration:  620 epoch:  4\n","Male Female Classification loss: 0.5942584276199341 iteration:  620 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.5265555381774902 iteration:  625 epoch:  4\n","LQ image reconstruction loss: 0.617816150188446 iteration:  625 epoch:  4\n","Same Different Classification loss: 0.5000911951065063 iteration:  625 epoch:  4\n","Male Female Classification loss: 0.5533455610275269 iteration:  625 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.687239170074463 iteration:  630 epoch:  4\n","LQ image reconstruction loss: 0.5738821029663086 iteration:  630 epoch:  4\n","Same Different Classification loss: 0.465074360370636 iteration:  630 epoch:  4\n","Male Female Classification loss: 0.5638664960861206 iteration:  630 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.625903844833374 iteration:  635 epoch:  4\n","LQ image reconstruction loss: 0.5316367149353027 iteration:  635 epoch:  4\n","Same Different Classification loss: 0.45376288890838623 iteration:  635 epoch:  4\n","Male Female Classification loss: 0.5878549814224243 iteration:  635 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.159045696258545 iteration:  640 epoch:  4\n","LQ image reconstruction loss: 0.5117456316947937 iteration:  640 epoch:  4\n","Same Different Classification loss: 0.45157745480537415 iteration:  640 epoch:  4\n","Male Female Classification loss: 0.622661828994751 iteration:  640 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.815767765045166 iteration:  645 epoch:  4\n","LQ image reconstruction loss: 0.5748964548110962 iteration:  645 epoch:  4\n","Same Different Classification loss: 0.48389914631843567 iteration:  645 epoch:  4\n","Male Female Classification loss: 0.5742132663726807 iteration:  645 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.692625045776367 iteration:  650 epoch:  4\n","LQ image reconstruction loss: 0.5938818454742432 iteration:  650 epoch:  4\n","Same Different Classification loss: 0.5034221410751343 iteration:  650 epoch:  4\n","Male Female Classification loss: 0.5765190720558167 iteration:  650 epoch:  4\n","----------------------------------------------------\n","Accuracy of SD: 80 %\n","Accuracy of MF: 77 %\n","Mean Accuracy of SD: 60 %\n","Mean Accuracy of MF: 61 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.4198622703552246 iteration:  655 epoch:  4\n","LQ image reconstruction loss: 0.5695542097091675 iteration:  655 epoch:  4\n","Same Different Classification loss: 0.4727485775947571 iteration:  655 epoch:  4\n","Male Female Classification loss: 0.5735704302787781 iteration:  655 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.9759445190429688 iteration:  660 epoch:  4\n","LQ image reconstruction loss: 0.6315804719924927 iteration:  660 epoch:  4\n","Same Different Classification loss: 0.5281203389167786 iteration:  660 epoch:  4\n","Male Female Classification loss: 0.562928318977356 iteration:  660 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.717454433441162 iteration:  665 epoch:  4\n","LQ image reconstruction loss: 0.5206060409545898 iteration:  665 epoch:  4\n","Same Different Classification loss: 0.4403994679450989 iteration:  665 epoch:  4\n","Male Female Classification loss: 0.6160833239555359 iteration:  665 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.341052532196045 iteration:  670 epoch:  4\n","LQ image reconstruction loss: 0.5991197228431702 iteration:  670 epoch:  4\n","Same Different Classification loss: 0.5118118524551392 iteration:  670 epoch:  4\n","Male Female Classification loss: 0.5935277342796326 iteration:  670 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.4933505058288574 iteration:  675 epoch:  4\n","LQ image reconstruction loss: 0.598946750164032 iteration:  675 epoch:  4\n","Same Different Classification loss: 0.5365949273109436 iteration:  675 epoch:  4\n","Male Female Classification loss: 0.5820463299751282 iteration:  675 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.54828143119812 iteration:  680 epoch:  4\n","LQ image reconstruction loss: 0.621762752532959 iteration:  680 epoch:  4\n","Same Different Classification loss: 0.5650184154510498 iteration:  680 epoch:  4\n","Male Female Classification loss: 0.6273200511932373 iteration:  680 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.828470468521118 iteration:  685 epoch:  4\n","LQ image reconstruction loss: 0.5287864208221436 iteration:  685 epoch:  4\n","Same Different Classification loss: 0.48612457513809204 iteration:  685 epoch:  4\n","Male Female Classification loss: 0.6441563963890076 iteration:  685 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.7519614696502686 iteration:  690 epoch:  4\n","LQ image reconstruction loss: 0.5446267127990723 iteration:  690 epoch:  4\n","Same Different Classification loss: 0.5129991769790649 iteration:  690 epoch:  4\n","Male Female Classification loss: 0.6039993166923523 iteration:  690 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.7401540279388428 iteration:  695 epoch:  4\n","LQ image reconstruction loss: 0.5602163672447205 iteration:  695 epoch:  4\n","Same Different Classification loss: 0.45216697454452515 iteration:  695 epoch:  4\n","Male Female Classification loss: 0.5687452554702759 iteration:  695 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.189774751663208 iteration:  700 epoch:  4\n","LQ image reconstruction loss: 0.5774965882301331 iteration:  700 epoch:  4\n","Same Different Classification loss: 0.5038700699806213 iteration:  700 epoch:  4\n","Male Female Classification loss: 0.5677648782730103 iteration:  700 epoch:  4\n","----------------------------------------------------\n","Accuracy of SD: 72 %\n","Accuracy of MF: 74 %\n","Mean Accuracy of SD: 60 %\n","Mean Accuracy of MF: 61 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.5697174072265625 iteration:  705 epoch:  4\n","LQ image reconstruction loss: 0.5557658672332764 iteration:  705 epoch:  4\n","Same Different Classification loss: 0.47604531049728394 iteration:  705 epoch:  4\n","Male Female Classification loss: 0.5893644094467163 iteration:  705 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.9602067470550537 iteration:  710 epoch:  4\n","LQ image reconstruction loss: 0.6770374774932861 iteration:  710 epoch:  4\n","Same Different Classification loss: 0.5669372081756592 iteration:  710 epoch:  4\n","Male Female Classification loss: 0.6047754883766174 iteration:  710 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.8824386596679688 iteration:  715 epoch:  4\n","LQ image reconstruction loss: 0.6223374605178833 iteration:  715 epoch:  4\n","Same Different Classification loss: 0.5103896856307983 iteration:  715 epoch:  4\n","Male Female Classification loss: 0.561231255531311 iteration:  715 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.3544812202453613 iteration:  720 epoch:  4\n","LQ image reconstruction loss: 0.5295870900154114 iteration:  720 epoch:  4\n","Same Different Classification loss: 0.4852496087551117 iteration:  720 epoch:  4\n","Male Female Classification loss: 0.6280006170272827 iteration:  720 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.9528493881225586 iteration:  725 epoch:  4\n","LQ image reconstruction loss: 0.5693045854568481 iteration:  725 epoch:  4\n","Same Different Classification loss: 0.5221639275550842 iteration:  725 epoch:  4\n","Male Female Classification loss: 0.6620075106620789 iteration:  725 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.8363542556762695 iteration:  730 epoch:  4\n","LQ image reconstruction loss: 0.5933231115341187 iteration:  730 epoch:  4\n","Same Different Classification loss: 0.4990238547325134 iteration:  730 epoch:  4\n","Male Female Classification loss: 0.5515850186347961 iteration:  730 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.4950549602508545 iteration:  735 epoch:  4\n","LQ image reconstruction loss: 0.5685843229293823 iteration:  735 epoch:  4\n","Same Different Classification loss: 0.4845646619796753 iteration:  735 epoch:  4\n","Male Female Classification loss: 0.548405110836029 iteration:  735 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.383530378341675 iteration:  740 epoch:  4\n","LQ image reconstruction loss: 0.5284397006034851 iteration:  740 epoch:  4\n","Same Different Classification loss: 0.4656020700931549 iteration:  740 epoch:  4\n","Male Female Classification loss: 0.5829377770423889 iteration:  740 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.183499336242676 iteration:  745 epoch:  4\n","LQ image reconstruction loss: 0.5725228190422058 iteration:  745 epoch:  4\n","Same Different Classification loss: 0.47495678067207336 iteration:  745 epoch:  4\n","Male Female Classification loss: 0.5721359252929688 iteration:  745 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.1683077812194824 iteration:  750 epoch:  4\n","LQ image reconstruction loss: 0.6710286140441895 iteration:  750 epoch:  4\n","Same Different Classification loss: 0.5629172325134277 iteration:  750 epoch:  4\n","Male Female Classification loss: 0.5945153832435608 iteration:  750 epoch:  4\n","----------------------------------------------------\n","Accuracy of SD: 80 %\n","Accuracy of MF: 66 %\n","Mean Accuracy of SD: 60 %\n","Mean Accuracy of MF: 61 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.2520439624786377 iteration:  755 epoch:  4\n","LQ image reconstruction loss: 0.503244161605835 iteration:  755 epoch:  4\n","Same Different Classification loss: 0.4639820456504822 iteration:  755 epoch:  4\n","Male Female Classification loss: 0.6723897457122803 iteration:  755 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.4554290771484375 iteration:  760 epoch:  4\n","LQ image reconstruction loss: 0.5743764638900757 iteration:  760 epoch:  4\n","Same Different Classification loss: 0.5187259912490845 iteration:  760 epoch:  4\n","Male Female Classification loss: 0.6267247200012207 iteration:  760 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.960386276245117 iteration:  765 epoch:  4\n","LQ image reconstruction loss: 0.5674532651901245 iteration:  765 epoch:  4\n","Same Different Classification loss: 0.5210285782814026 iteration:  765 epoch:  4\n","Male Female Classification loss: 0.6260764598846436 iteration:  765 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.843506336212158 iteration:  770 epoch:  4\n","LQ image reconstruction loss: 0.5618857741355896 iteration:  770 epoch:  4\n","Same Different Classification loss: 0.45127761363983154 iteration:  770 epoch:  4\n","Male Female Classification loss: 0.6040908098220825 iteration:  770 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.870435953140259 iteration:  775 epoch:  4\n","LQ image reconstruction loss: 0.5417693257331848 iteration:  775 epoch:  4\n","Same Different Classification loss: 0.4708716869354248 iteration:  775 epoch:  4\n","Male Female Classification loss: 0.6003500819206238 iteration:  775 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.9731881618499756 iteration:  780 epoch:  4\n","LQ image reconstruction loss: 0.5453092455863953 iteration:  780 epoch:  4\n","Same Different Classification loss: 0.4623942971229553 iteration:  780 epoch:  4\n","Male Female Classification loss: 0.616199254989624 iteration:  780 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.920030355453491 iteration:  785 epoch:  4\n","LQ image reconstruction loss: 0.5672988891601562 iteration:  785 epoch:  4\n","Same Different Classification loss: 0.478914350271225 iteration:  785 epoch:  4\n","Male Female Classification loss: 0.6547913551330566 iteration:  785 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.0744519233703613 iteration:  790 epoch:  4\n","LQ image reconstruction loss: 0.4765678644180298 iteration:  790 epoch:  4\n","Same Different Classification loss: 0.4296033978462219 iteration:  790 epoch:  4\n","Male Female Classification loss: 0.6021410226821899 iteration:  790 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.124896764755249 iteration:  795 epoch:  4\n","LQ image reconstruction loss: 0.6220705509185791 iteration:  795 epoch:  4\n","Same Different Classification loss: 0.531320333480835 iteration:  795 epoch:  4\n","Male Female Classification loss: 0.6149846911430359 iteration:  795 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.604860782623291 iteration:  800 epoch:  4\n","LQ image reconstruction loss: 0.6273977756500244 iteration:  800 epoch:  4\n","Same Different Classification loss: 0.5189660787582397 iteration:  800 epoch:  4\n","Male Female Classification loss: 0.5461693406105042 iteration:  800 epoch:  4\n","----------------------------------------------------\n","Accuracy of SD: 80 %\n","Accuracy of MF: 77 %\n","Mean Accuracy of SD: 60 %\n","Mean Accuracy of MF: 61 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.3135986328125 iteration:  805 epoch:  4\n","LQ image reconstruction loss: 0.4472864866256714 iteration:  805 epoch:  4\n","Same Different Classification loss: 0.39505699276924133 iteration:  805 epoch:  4\n","Male Female Classification loss: 0.5852639675140381 iteration:  805 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.9792957305908203 iteration:  810 epoch:  4\n","LQ image reconstruction loss: 0.5816817283630371 iteration:  810 epoch:  4\n","Same Different Classification loss: 0.4851231276988983 iteration:  810 epoch:  4\n","Male Female Classification loss: 0.551956295967102 iteration:  810 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.5132863521575928 iteration:  815 epoch:  4\n","LQ image reconstruction loss: 0.6042690277099609 iteration:  815 epoch:  4\n","Same Different Classification loss: 0.4928057789802551 iteration:  815 epoch:  4\n","Male Female Classification loss: 0.5771750211715698 iteration:  815 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 4.717607021331787 iteration:  820 epoch:  4\n","LQ image reconstruction loss: 0.6350659132003784 iteration:  820 epoch:  4\n","Same Different Classification loss: 0.5850206613540649 iteration:  820 epoch:  4\n","Male Female Classification loss: 0.6082869172096252 iteration:  820 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.561194896697998 iteration:  825 epoch:  4\n","LQ image reconstruction loss: 0.6095849275588989 iteration:  825 epoch:  4\n","Same Different Classification loss: 0.5081392526626587 iteration:  825 epoch:  4\n","Male Female Classification loss: 0.5726966261863708 iteration:  825 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.702331066131592 iteration:  830 epoch:  4\n","LQ image reconstruction loss: 0.5586093068122864 iteration:  830 epoch:  4\n","Same Different Classification loss: 0.47928687930107117 iteration:  830 epoch:  4\n","Male Female Classification loss: 0.5397533178329468 iteration:  830 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 1.7942359447479248 iteration:  835 epoch:  4\n","LQ image reconstruction loss: 0.5082199573516846 iteration:  835 epoch:  4\n","Same Different Classification loss: 0.4369491636753082 iteration:  835 epoch:  4\n","Male Female Classification loss: 0.6381570100784302 iteration:  835 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.474630832672119 iteration:  840 epoch:  4\n","LQ image reconstruction loss: 0.5955246686935425 iteration:  840 epoch:  4\n","Same Different Classification loss: 0.5132291913032532 iteration:  840 epoch:  4\n","Male Female Classification loss: 0.5704721212387085 iteration:  840 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.5253899097442627 iteration:  845 epoch:  4\n","LQ image reconstruction loss: 0.5066423416137695 iteration:  845 epoch:  4\n","Same Different Classification loss: 0.4089571237564087 iteration:  845 epoch:  4\n","Male Female Classification loss: 0.5474705100059509 iteration:  845 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.2944438457489014 iteration:  850 epoch:  4\n","LQ image reconstruction loss: 0.5462048649787903 iteration:  850 epoch:  4\n","Same Different Classification loss: 0.4270797669887543 iteration:  850 epoch:  4\n","Male Female Classification loss: 0.5666651129722595 iteration:  850 epoch:  4\n","----------------------------------------------------\n","Accuracy of SD: 79 %\n","Accuracy of MF: 79 %\n","Mean Accuracy of SD: 60 %\n","Mean Accuracy of MF: 62 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.135119915008545 iteration:  855 epoch:  4\n","LQ image reconstruction loss: 0.5349922180175781 iteration:  855 epoch:  4\n","Same Different Classification loss: 0.4676312506198883 iteration:  855 epoch:  4\n","Male Female Classification loss: 0.5763028264045715 iteration:  855 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.359480857849121 iteration:  860 epoch:  4\n","LQ image reconstruction loss: 0.5067028999328613 iteration:  860 epoch:  4\n","Same Different Classification loss: 0.43999752402305603 iteration:  860 epoch:  4\n","Male Female Classification loss: 0.624098539352417 iteration:  860 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.5677781105041504 iteration:  865 epoch:  4\n","LQ image reconstruction loss: 0.519929051399231 iteration:  865 epoch:  4\n","Same Different Classification loss: 0.45565539598464966 iteration:  865 epoch:  4\n","Male Female Classification loss: 0.5781744718551636 iteration:  865 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.7393798828125 iteration:  870 epoch:  4\n","LQ image reconstruction loss: 0.5626742839813232 iteration:  870 epoch:  4\n","Same Different Classification loss: 0.4549984037876129 iteration:  870 epoch:  4\n","Male Female Classification loss: 0.5829247832298279 iteration:  870 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.7552907466888428 iteration:  875 epoch:  4\n","LQ image reconstruction loss: 0.5815203785896301 iteration:  875 epoch:  4\n","Same Different Classification loss: 0.5215039849281311 iteration:  875 epoch:  4\n","Male Female Classification loss: 0.6263301968574524 iteration:  875 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.717379331588745 iteration:  880 epoch:  4\n","LQ image reconstruction loss: 0.47747722268104553 iteration:  880 epoch:  4\n","Same Different Classification loss: 0.4326229989528656 iteration:  880 epoch:  4\n","Male Female Classification loss: 0.59983229637146 iteration:  880 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.2490127086639404 iteration:  885 epoch:  4\n","LQ image reconstruction loss: 0.5045778751373291 iteration:  885 epoch:  4\n","Same Different Classification loss: 0.4360678493976593 iteration:  885 epoch:  4\n","Male Female Classification loss: 0.6532549262046814 iteration:  885 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.0206525325775146 iteration:  890 epoch:  4\n","LQ image reconstruction loss: 0.5259807109832764 iteration:  890 epoch:  4\n","Same Different Classification loss: 0.436140775680542 iteration:  890 epoch:  4\n","Male Female Classification loss: 0.5298299193382263 iteration:  890 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.205451488494873 iteration:  895 epoch:  4\n","LQ image reconstruction loss: 0.5316729545593262 iteration:  895 epoch:  4\n","Same Different Classification loss: 0.46065571904182434 iteration:  895 epoch:  4\n","Male Female Classification loss: 0.6036618947982788 iteration:  895 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 1.993778944015503 iteration:  900 epoch:  4\n","LQ image reconstruction loss: 0.4810939431190491 iteration:  900 epoch:  4\n","Same Different Classification loss: 0.42295846343040466 iteration:  900 epoch:  4\n","Male Female Classification loss: 0.6565965414047241 iteration:  900 epoch:  4\n","----------------------------------------------------\n","Accuracy of SD: 76 %\n","Accuracy of MF: 76 %\n","Mean Accuracy of SD: 60 %\n","Mean Accuracy of MF: 62 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 1.8343530893325806 iteration:  905 epoch:  4\n","LQ image reconstruction loss: 0.5239335894584656 iteration:  905 epoch:  4\n","Same Different Classification loss: 0.44255146384239197 iteration:  905 epoch:  4\n","Male Female Classification loss: 0.6109465956687927 iteration:  905 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.272871971130371 iteration:  910 epoch:  4\n","LQ image reconstruction loss: 0.5347049832344055 iteration:  910 epoch:  4\n","Same Different Classification loss: 0.42843395471572876 iteration:  910 epoch:  4\n","Male Female Classification loss: 0.581315279006958 iteration:  910 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.7799994945526123 iteration:  915 epoch:  4\n","LQ image reconstruction loss: 0.582115888595581 iteration:  915 epoch:  4\n","Same Different Classification loss: 0.4901508390903473 iteration:  915 epoch:  4\n","Male Female Classification loss: 0.5462200045585632 iteration:  915 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.0091145038604736 iteration:  920 epoch:  4\n","LQ image reconstruction loss: 0.6382762789726257 iteration:  920 epoch:  4\n","Same Different Classification loss: 0.5164908766746521 iteration:  920 epoch:  4\n","Male Female Classification loss: 0.5161194205284119 iteration:  920 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.4358644485473633 iteration:  925 epoch:  4\n","LQ image reconstruction loss: 0.5668725371360779 iteration:  925 epoch:  4\n","Same Different Classification loss: 0.4538497030735016 iteration:  925 epoch:  4\n","Male Female Classification loss: 0.5519260764122009 iteration:  925 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.279491901397705 iteration:  930 epoch:  4\n","LQ image reconstruction loss: 0.5118690133094788 iteration:  930 epoch:  4\n","Same Different Classification loss: 0.4311404526233673 iteration:  930 epoch:  4\n","Male Female Classification loss: 0.6407020688056946 iteration:  930 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.6340384483337402 iteration:  935 epoch:  4\n","LQ image reconstruction loss: 0.5349003672599792 iteration:  935 epoch:  4\n","Same Different Classification loss: 0.475362628698349 iteration:  935 epoch:  4\n","Male Female Classification loss: 0.6112356781959534 iteration:  935 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.6545846462249756 iteration:  940 epoch:  4\n","LQ image reconstruction loss: 0.6300198435783386 iteration:  940 epoch:  4\n","Same Different Classification loss: 0.5077524781227112 iteration:  940 epoch:  4\n","Male Female Classification loss: 0.5991294384002686 iteration:  940 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.144157886505127 iteration:  945 epoch:  4\n","LQ image reconstruction loss: 0.5460588932037354 iteration:  945 epoch:  4\n","Same Different Classification loss: 0.4471043646335602 iteration:  945 epoch:  4\n","Male Female Classification loss: 0.5936947464942932 iteration:  945 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.2194106578826904 iteration:  950 epoch:  4\n","LQ image reconstruction loss: 0.47413283586502075 iteration:  950 epoch:  4\n","Same Different Classification loss: 0.45860302448272705 iteration:  950 epoch:  4\n","Male Female Classification loss: 0.6399230360984802 iteration:  950 epoch:  4\n","----------------------------------------------------\n","Accuracy of SD: 75 %\n","Accuracy of MF: 70 %\n","Mean Accuracy of SD: 60 %\n","Mean Accuracy of MF: 62 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.1012966632843018 iteration:  955 epoch:  4\n","LQ image reconstruction loss: 0.5623140335083008 iteration:  955 epoch:  4\n","Same Different Classification loss: 0.49170634150505066 iteration:  955 epoch:  4\n","Male Female Classification loss: 0.6193145513534546 iteration:  955 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.353739023208618 iteration:  960 epoch:  4\n","LQ image reconstruction loss: 0.5547580122947693 iteration:  960 epoch:  4\n","Same Different Classification loss: 0.46910330653190613 iteration:  960 epoch:  4\n","Male Female Classification loss: 0.5353097319602966 iteration:  960 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.4276716709136963 iteration:  965 epoch:  4\n","LQ image reconstruction loss: 0.5627892017364502 iteration:  965 epoch:  4\n","Same Different Classification loss: 0.489751398563385 iteration:  965 epoch:  4\n","Male Female Classification loss: 0.6380658745765686 iteration:  965 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.2607316970825195 iteration:  970 epoch:  4\n","LQ image reconstruction loss: 0.526740312576294 iteration:  970 epoch:  4\n","Same Different Classification loss: 0.4149254560470581 iteration:  970 epoch:  4\n","Male Female Classification loss: 0.621624231338501 iteration:  970 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.2920756340026855 iteration:  975 epoch:  4\n","LQ image reconstruction loss: 0.4316222071647644 iteration:  975 epoch:  4\n","Same Different Classification loss: 0.37175652384757996 iteration:  975 epoch:  4\n","Male Female Classification loss: 0.5501614212989807 iteration:  975 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.265817165374756 iteration:  980 epoch:  4\n","LQ image reconstruction loss: 0.5073183178901672 iteration:  980 epoch:  4\n","Same Different Classification loss: 0.453329473733902 iteration:  980 epoch:  4\n","Male Female Classification loss: 0.6135444045066833 iteration:  980 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.289045572280884 iteration:  985 epoch:  4\n","LQ image reconstruction loss: 0.48895829916000366 iteration:  985 epoch:  4\n","Same Different Classification loss: 0.42943352460861206 iteration:  985 epoch:  4\n","Male Female Classification loss: 0.6143991947174072 iteration:  985 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.7272493839263916 iteration:  990 epoch:  4\n","LQ image reconstruction loss: 0.4237014949321747 iteration:  990 epoch:  4\n","Same Different Classification loss: 0.3778513967990875 iteration:  990 epoch:  4\n","Male Female Classification loss: 0.6482254862785339 iteration:  990 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.5350608825683594 iteration:  995 epoch:  4\n","LQ image reconstruction loss: 0.4784318208694458 iteration:  995 epoch:  4\n","Same Different Classification loss: 0.4138108193874359 iteration:  995 epoch:  4\n","Male Female Classification loss: 0.6158926486968994 iteration:  995 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.572605609893799 iteration:  1000 epoch:  4\n","LQ image reconstruction loss: 0.6005759835243225 iteration:  1000 epoch:  4\n","Same Different Classification loss: 0.5061759948730469 iteration:  1000 epoch:  4\n","Male Female Classification loss: 0.5428553819656372 iteration:  1000 epoch:  4\n","----------------------------------------------------\n","Accuracy of SD: 88 %\n","Accuracy of MF: 74 %\n","Mean Accuracy of SD: 61 %\n","Mean Accuracy of MF: 62 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.6353557109832764 iteration:  1005 epoch:  4\n","LQ image reconstruction loss: 0.49907344579696655 iteration:  1005 epoch:  4\n","Same Different Classification loss: 0.4171766936779022 iteration:  1005 epoch:  4\n","Male Female Classification loss: 0.6055269837379456 iteration:  1005 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.962376356124878 iteration:  1010 epoch:  4\n","LQ image reconstruction loss: 0.43383726477622986 iteration:  1010 epoch:  4\n","Same Different Classification loss: 0.40599799156188965 iteration:  1010 epoch:  4\n","Male Female Classification loss: 0.6666020154953003 iteration:  1010 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.2904396057128906 iteration:  1015 epoch:  4\n","LQ image reconstruction loss: 0.5115653276443481 iteration:  1015 epoch:  4\n","Same Different Classification loss: 0.4438547194004059 iteration:  1015 epoch:  4\n","Male Female Classification loss: 0.585894763469696 iteration:  1015 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.331773281097412 iteration:  1020 epoch:  4\n","LQ image reconstruction loss: 0.5737285017967224 iteration:  1020 epoch:  4\n","Same Different Classification loss: 0.5087531208992004 iteration:  1020 epoch:  4\n","Male Female Classification loss: 0.6510652303695679 iteration:  1020 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.5568270683288574 iteration:  1025 epoch:  4\n","LQ image reconstruction loss: 0.5533488392829895 iteration:  1025 epoch:  4\n","Same Different Classification loss: 0.4481257200241089 iteration:  1025 epoch:  4\n","Male Female Classification loss: 0.562181293964386 iteration:  1025 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.551668405532837 iteration:  1030 epoch:  4\n","LQ image reconstruction loss: 0.5478663444519043 iteration:  1030 epoch:  4\n","Same Different Classification loss: 0.4442994296550751 iteration:  1030 epoch:  4\n","Male Female Classification loss: 0.5613666772842407 iteration:  1030 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.5332748889923096 iteration:  1035 epoch:  4\n","LQ image reconstruction loss: 0.42489194869995117 iteration:  1035 epoch:  4\n","Same Different Classification loss: 0.40332040190696716 iteration:  1035 epoch:  4\n","Male Female Classification loss: 0.6592617630958557 iteration:  1035 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.5091633796691895 iteration:  1040 epoch:  4\n","LQ image reconstruction loss: 0.5747666358947754 iteration:  1040 epoch:  4\n","Same Different Classification loss: 0.48773202300071716 iteration:  1040 epoch:  4\n","Male Female Classification loss: 0.5555012226104736 iteration:  1040 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.3262431621551514 iteration:  1045 epoch:  4\n","LQ image reconstruction loss: 0.5298040509223938 iteration:  1045 epoch:  4\n","Same Different Classification loss: 0.49221861362457275 iteration:  1045 epoch:  4\n","Male Female Classification loss: 0.6086845993995667 iteration:  1045 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 1.9396347999572754 iteration:  1050 epoch:  4\n","LQ image reconstruction loss: 0.45171457529067993 iteration:  1050 epoch:  4\n","Same Different Classification loss: 0.40577325224876404 iteration:  1050 epoch:  4\n","Male Female Classification loss: 0.6364811658859253 iteration:  1050 epoch:  4\n","----------------------------------------------------\n","Accuracy of SD: 85 %\n","Accuracy of MF: 75 %\n","Mean Accuracy of SD: 61 %\n","Mean Accuracy of MF: 62 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.381469964981079 iteration:  1055 epoch:  4\n","LQ image reconstruction loss: 0.49183031916618347 iteration:  1055 epoch:  4\n","Same Different Classification loss: 0.445417582988739 iteration:  1055 epoch:  4\n","Male Female Classification loss: 0.6307620406150818 iteration:  1055 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.5540714263916016 iteration:  1060 epoch:  4\n","LQ image reconstruction loss: 0.5183072090148926 iteration:  1060 epoch:  4\n","Same Different Classification loss: 0.41794586181640625 iteration:  1060 epoch:  4\n","Male Female Classification loss: 0.601780116558075 iteration:  1060 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.5538811683654785 iteration:  1065 epoch:  4\n","LQ image reconstruction loss: 0.5285691022872925 iteration:  1065 epoch:  4\n","Same Different Classification loss: 0.4632101058959961 iteration:  1065 epoch:  4\n","Male Female Classification loss: 0.6031615734100342 iteration:  1065 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.3109796047210693 iteration:  1070 epoch:  4\n","LQ image reconstruction loss: 0.5758782029151917 iteration:  1070 epoch:  4\n","Same Different Classification loss: 0.4646061062812805 iteration:  1070 epoch:  4\n","Male Female Classification loss: 0.4980829656124115 iteration:  1070 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.380150318145752 iteration:  1075 epoch:  4\n","LQ image reconstruction loss: 0.5212490558624268 iteration:  1075 epoch:  4\n","Same Different Classification loss: 0.453231543302536 iteration:  1075 epoch:  4\n","Male Female Classification loss: 0.5830678343772888 iteration:  1075 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.669154167175293 iteration:  1080 epoch:  4\n","LQ image reconstruction loss: 0.4580201506614685 iteration:  1080 epoch:  4\n","Same Different Classification loss: 0.3949563503265381 iteration:  1080 epoch:  4\n","Male Female Classification loss: 0.6258554458618164 iteration:  1080 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.4232709407806396 iteration:  1085 epoch:  4\n","LQ image reconstruction loss: 0.5167781114578247 iteration:  1085 epoch:  4\n","Same Different Classification loss: 0.44742077589035034 iteration:  1085 epoch:  4\n","Male Female Classification loss: 0.5367440581321716 iteration:  1085 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.2333526611328125 iteration:  1090 epoch:  4\n","LQ image reconstruction loss: 0.468693345785141 iteration:  1090 epoch:  4\n","Same Different Classification loss: 0.4084976315498352 iteration:  1090 epoch:  4\n","Male Female Classification loss: 0.5888081192970276 iteration:  1090 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.540328025817871 iteration:  1095 epoch:  4\n","LQ image reconstruction loss: 0.6291905641555786 iteration:  1095 epoch:  4\n","Same Different Classification loss: 0.5643640756607056 iteration:  1095 epoch:  4\n","Male Female Classification loss: 0.5802964568138123 iteration:  1095 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.6931679248809814 iteration:  1100 epoch:  4\n","LQ image reconstruction loss: 0.6398677825927734 iteration:  1100 epoch:  4\n","Same Different Classification loss: 0.5690473318099976 iteration:  1100 epoch:  4\n","Male Female Classification loss: 0.5622989535331726 iteration:  1100 epoch:  4\n","----------------------------------------------------\n","Accuracy of SD: 81 %\n","Accuracy of MF: 65 %\n","Mean Accuracy of SD: 61 %\n","Mean Accuracy of MF: 62 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.086257219314575 iteration:  1105 epoch:  4\n","LQ image reconstruction loss: 0.4799039959907532 iteration:  1105 epoch:  4\n","Same Different Classification loss: 0.43388107419013977 iteration:  1105 epoch:  4\n","Male Female Classification loss: 0.6143967509269714 iteration:  1105 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.9396958351135254 iteration:  1110 epoch:  4\n","LQ image reconstruction loss: 0.5665364265441895 iteration:  1110 epoch:  4\n","Same Different Classification loss: 0.45579203963279724 iteration:  1110 epoch:  4\n","Male Female Classification loss: 0.5860865712165833 iteration:  1110 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.1442840099334717 iteration:  1115 epoch:  4\n","LQ image reconstruction loss: 0.3954668939113617 iteration:  1115 epoch:  4\n","Same Different Classification loss: 0.35841986536979675 iteration:  1115 epoch:  4\n","Male Female Classification loss: 0.6421119570732117 iteration:  1115 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.455155372619629 iteration:  1120 epoch:  4\n","LQ image reconstruction loss: 0.4628365635871887 iteration:  1120 epoch:  4\n","Same Different Classification loss: 0.4112454354763031 iteration:  1120 epoch:  4\n","Male Female Classification loss: 0.5724519491195679 iteration:  1120 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 1.8728293180465698 iteration:  1125 epoch:  4\n","LQ image reconstruction loss: 0.5218645930290222 iteration:  1125 epoch:  4\n","Same Different Classification loss: 0.4709276556968689 iteration:  1125 epoch:  4\n","Male Female Classification loss: 0.5192027688026428 iteration:  1125 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.4459292888641357 iteration:  1130 epoch:  4\n","LQ image reconstruction loss: 0.4862099289894104 iteration:  1130 epoch:  4\n","Same Different Classification loss: 0.4108577072620392 iteration:  1130 epoch:  4\n","Male Female Classification loss: 0.5751012563705444 iteration:  1130 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.338033437728882 iteration:  1135 epoch:  4\n","LQ image reconstruction loss: 0.43914467096328735 iteration:  1135 epoch:  4\n","Same Different Classification loss: 0.4032593071460724 iteration:  1135 epoch:  4\n","Male Female Classification loss: 0.6173755526542664 iteration:  1135 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.753903865814209 iteration:  1140 epoch:  4\n","LQ image reconstruction loss: 0.47796666622161865 iteration:  1140 epoch:  4\n","Same Different Classification loss: 0.4068388044834137 iteration:  1140 epoch:  4\n","Male Female Classification loss: 0.5987645387649536 iteration:  1140 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.3090269565582275 iteration:  1145 epoch:  4\n","LQ image reconstruction loss: 0.5147096514701843 iteration:  1145 epoch:  4\n","Same Different Classification loss: 0.4177003502845764 iteration:  1145 epoch:  4\n","Male Female Classification loss: 0.552258312702179 iteration:  1145 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.573910713195801 iteration:  1150 epoch:  4\n","LQ image reconstruction loss: 0.47353479266166687 iteration:  1150 epoch:  4\n","Same Different Classification loss: 0.4037700593471527 iteration:  1150 epoch:  4\n","Male Female Classification loss: 0.610559344291687 iteration:  1150 epoch:  4\n","----------------------------------------------------\n","Accuracy of SD: 89 %\n","Accuracy of MF: 76 %\n","Mean Accuracy of SD: 61 %\n","Mean Accuracy of MF: 62 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.1881203651428223 iteration:  1155 epoch:  4\n","LQ image reconstruction loss: 0.4350273013114929 iteration:  1155 epoch:  4\n","Same Different Classification loss: 0.3737392723560333 iteration:  1155 epoch:  4\n","Male Female Classification loss: 0.5582274794578552 iteration:  1155 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.0674991607666016 iteration:  1160 epoch:  4\n","LQ image reconstruction loss: 0.532682478427887 iteration:  1160 epoch:  4\n","Same Different Classification loss: 0.4570864737033844 iteration:  1160 epoch:  4\n","Male Female Classification loss: 0.6093041896820068 iteration:  1160 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.3494505882263184 iteration:  1165 epoch:  4\n","LQ image reconstruction loss: 0.4155992269515991 iteration:  1165 epoch:  4\n","Same Different Classification loss: 0.3715450167655945 iteration:  1165 epoch:  4\n","Male Female Classification loss: 0.624152660369873 iteration:  1165 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.6421258449554443 iteration:  1170 epoch:  4\n","LQ image reconstruction loss: 0.5639686584472656 iteration:  1170 epoch:  4\n","Same Different Classification loss: 0.4658430814743042 iteration:  1170 epoch:  4\n","Male Female Classification loss: 0.5291206240653992 iteration:  1170 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.2403786182403564 iteration:  1175 epoch:  4\n","LQ image reconstruction loss: 0.4705572724342346 iteration:  1175 epoch:  4\n","Same Different Classification loss: 0.3964850902557373 iteration:  1175 epoch:  4\n","Male Female Classification loss: 0.5912898778915405 iteration:  1175 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.5320916175842285 iteration:  1180 epoch:  4\n","LQ image reconstruction loss: 0.5068271160125732 iteration:  1180 epoch:  4\n","Same Different Classification loss: 0.4613291919231415 iteration:  1180 epoch:  4\n","Male Female Classification loss: 0.5723744630813599 iteration:  1180 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.742379903793335 iteration:  1185 epoch:  4\n","LQ image reconstruction loss: 0.5036624670028687 iteration:  1185 epoch:  4\n","Same Different Classification loss: 0.4619360864162445 iteration:  1185 epoch:  4\n","Male Female Classification loss: 0.5873042941093445 iteration:  1185 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.4657275676727295 iteration:  1190 epoch:  4\n","LQ image reconstruction loss: 0.4991985261440277 iteration:  1190 epoch:  4\n","Same Different Classification loss: 0.4377323389053345 iteration:  1190 epoch:  4\n","Male Female Classification loss: 0.6066242456436157 iteration:  1190 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.3529181480407715 iteration:  1195 epoch:  4\n","LQ image reconstruction loss: 0.5436311364173889 iteration:  1195 epoch:  4\n","Same Different Classification loss: 0.4739772081375122 iteration:  1195 epoch:  4\n","Male Female Classification loss: 0.5928371548652649 iteration:  1195 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 1.9696160554885864 iteration:  1200 epoch:  4\n","LQ image reconstruction loss: 0.43323537707328796 iteration:  1200 epoch:  4\n","Same Different Classification loss: 0.42793700098991394 iteration:  1200 epoch:  4\n","Male Female Classification loss: 0.6740210056304932 iteration:  1200 epoch:  4\n","----------------------------------------------------\n","Accuracy of SD: 84 %\n","Accuracy of MF: 84 %\n","Mean Accuracy of SD: 61 %\n","Mean Accuracy of MF: 62 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.0679163932800293 iteration:  1205 epoch:  4\n","LQ image reconstruction loss: 0.46645891666412354 iteration:  1205 epoch:  4\n","Same Different Classification loss: 0.3974403142929077 iteration:  1205 epoch:  4\n","Male Female Classification loss: 0.604245662689209 iteration:  1205 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 1.8055223226547241 iteration:  1210 epoch:  4\n","LQ image reconstruction loss: 0.42445018887519836 iteration:  1210 epoch:  4\n","Same Different Classification loss: 0.355567067861557 iteration:  1210 epoch:  4\n","Male Female Classification loss: 0.5723927021026611 iteration:  1210 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.0262746810913086 iteration:  1215 epoch:  4\n","LQ image reconstruction loss: 0.47934016585350037 iteration:  1215 epoch:  4\n","Same Different Classification loss: 0.4112218916416168 iteration:  1215 epoch:  4\n","Male Female Classification loss: 0.6026134490966797 iteration:  1215 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.22281551361084 iteration:  1220 epoch:  4\n","LQ image reconstruction loss: 0.4743906557559967 iteration:  1220 epoch:  4\n","Same Different Classification loss: 0.43371134996414185 iteration:  1220 epoch:  4\n","Male Female Classification loss: 0.6810509562492371 iteration:  1220 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.565608263015747 iteration:  1225 epoch:  4\n","LQ image reconstruction loss: 0.47493842244148254 iteration:  1225 epoch:  4\n","Same Different Classification loss: 0.4062289893627167 iteration:  1225 epoch:  4\n","Male Female Classification loss: 0.5990824699401855 iteration:  1225 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.715268135070801 iteration:  1230 epoch:  4\n","LQ image reconstruction loss: 0.5303692817687988 iteration:  1230 epoch:  4\n","Same Different Classification loss: 0.44533705711364746 iteration:  1230 epoch:  4\n","Male Female Classification loss: 0.5730535984039307 iteration:  1230 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.8220431804656982 iteration:  1235 epoch:  4\n","LQ image reconstruction loss: 0.5048444271087646 iteration:  1235 epoch:  4\n","Same Different Classification loss: 0.45676612854003906 iteration:  1235 epoch:  4\n","Male Female Classification loss: 0.6418566703796387 iteration:  1235 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.5654358863830566 iteration:  1240 epoch:  4\n","LQ image reconstruction loss: 0.5251979231834412 iteration:  1240 epoch:  4\n","Same Different Classification loss: 0.4605425000190735 iteration:  1240 epoch:  4\n","Male Female Classification loss: 0.59140545129776 iteration:  1240 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.3297266960144043 iteration:  1245 epoch:  4\n","LQ image reconstruction loss: 0.545170247554779 iteration:  1245 epoch:  4\n","Same Different Classification loss: 0.4500845670700073 iteration:  1245 epoch:  4\n","Male Female Classification loss: 0.5653109550476074 iteration:  1245 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.28401255607605 iteration:  1250 epoch:  4\n","LQ image reconstruction loss: 0.43246719241142273 iteration:  1250 epoch:  4\n","Same Different Classification loss: 0.39757493138313293 iteration:  1250 epoch:  4\n","Male Female Classification loss: 0.5981261134147644 iteration:  1250 epoch:  4\n","----------------------------------------------------\n","Accuracy of SD: 79 %\n","Accuracy of MF: 82 %\n","Mean Accuracy of SD: 61 %\n","Mean Accuracy of MF: 62 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.4633803367614746 iteration:  1255 epoch:  4\n","LQ image reconstruction loss: 0.5090729594230652 iteration:  1255 epoch:  4\n","Same Different Classification loss: 0.49606484174728394 iteration:  1255 epoch:  4\n","Male Female Classification loss: 0.6154859066009521 iteration:  1255 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.151921510696411 iteration:  1260 epoch:  4\n","LQ image reconstruction loss: 0.5214054584503174 iteration:  1260 epoch:  4\n","Same Different Classification loss: 0.4495255649089813 iteration:  1260 epoch:  4\n","Male Female Classification loss: 0.51554274559021 iteration:  1260 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.3185582160949707 iteration:  1265 epoch:  4\n","LQ image reconstruction loss: 0.5026983618736267 iteration:  1265 epoch:  4\n","Same Different Classification loss: 0.42768189311027527 iteration:  1265 epoch:  4\n","Male Female Classification loss: 0.5382303595542908 iteration:  1265 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.652366876602173 iteration:  1270 epoch:  4\n","LQ image reconstruction loss: 0.4924684166908264 iteration:  1270 epoch:  4\n","Same Different Classification loss: 0.4179028570652008 iteration:  1270 epoch:  4\n","Male Female Classification loss: 0.5970413684844971 iteration:  1270 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.5425572395324707 iteration:  1275 epoch:  4\n","LQ image reconstruction loss: 0.4919782876968384 iteration:  1275 epoch:  4\n","Same Different Classification loss: 0.410762757062912 iteration:  1275 epoch:  4\n","Male Female Classification loss: 0.5681869387626648 iteration:  1275 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.5292611122131348 iteration:  1280 epoch:  4\n","LQ image reconstruction loss: 0.4438956677913666 iteration:  1280 epoch:  4\n","Same Different Classification loss: 0.39465445280075073 iteration:  1280 epoch:  4\n","Male Female Classification loss: 0.622175395488739 iteration:  1280 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.182518482208252 iteration:  1285 epoch:  4\n","LQ image reconstruction loss: 0.4048125743865967 iteration:  1285 epoch:  4\n","Same Different Classification loss: 0.38869479298591614 iteration:  1285 epoch:  4\n","Male Female Classification loss: 0.6232455372810364 iteration:  1285 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 1.8897371292114258 iteration:  1290 epoch:  4\n","LQ image reconstruction loss: 0.39722350239753723 iteration:  1290 epoch:  4\n","Same Different Classification loss: 0.3255199193954468 iteration:  1290 epoch:  4\n","Male Female Classification loss: 0.6472135186195374 iteration:  1290 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 1.712316632270813 iteration:  1295 epoch:  4\n","LQ image reconstruction loss: 0.4578377902507782 iteration:  1295 epoch:  4\n","Same Different Classification loss: 0.43963631987571716 iteration:  1295 epoch:  4\n","Male Female Classification loss: 0.6119692921638489 iteration:  1295 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 1.797568678855896 iteration:  1300 epoch:  4\n","LQ image reconstruction loss: 0.45534807443618774 iteration:  1300 epoch:  4\n","Same Different Classification loss: 0.3845367729663849 iteration:  1300 epoch:  4\n","Male Female Classification loss: 0.5593119263648987 iteration:  1300 epoch:  4\n","----------------------------------------------------\n","Accuracy of SD: 86 %\n","Accuracy of MF: 61 %\n","Mean Accuracy of SD: 61 %\n","Mean Accuracy of MF: 62 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.6392035484313965 iteration:  1305 epoch:  4\n","LQ image reconstruction loss: 0.4297133982181549 iteration:  1305 epoch:  4\n","Same Different Classification loss: 0.3823419213294983 iteration:  1305 epoch:  4\n","Male Female Classification loss: 0.6261815428733826 iteration:  1305 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.559196710586548 iteration:  1310 epoch:  4\n","LQ image reconstruction loss: 0.44700169563293457 iteration:  1310 epoch:  4\n","Same Different Classification loss: 0.3910931348800659 iteration:  1310 epoch:  4\n","Male Female Classification loss: 0.5933409333229065 iteration:  1310 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.055356025695801 iteration:  1315 epoch:  4\n","LQ image reconstruction loss: 0.45773494243621826 iteration:  1315 epoch:  4\n","Same Different Classification loss: 0.4177890717983246 iteration:  1315 epoch:  4\n","Male Female Classification loss: 0.606018602848053 iteration:  1315 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.6262893676757812 iteration:  1320 epoch:  4\n","LQ image reconstruction loss: 0.48508644104003906 iteration:  1320 epoch:  4\n","Same Different Classification loss: 0.42173057794570923 iteration:  1320 epoch:  4\n","Male Female Classification loss: 0.6007415056228638 iteration:  1320 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.294844627380371 iteration:  1325 epoch:  4\n","LQ image reconstruction loss: 0.4465949237346649 iteration:  1325 epoch:  4\n","Same Different Classification loss: 0.4073038101196289 iteration:  1325 epoch:  4\n","Male Female Classification loss: 0.6634829044342041 iteration:  1325 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.3846116065979004 iteration:  1330 epoch:  4\n","LQ image reconstruction loss: 0.608893871307373 iteration:  1330 epoch:  4\n","Same Different Classification loss: 0.512071430683136 iteration:  1330 epoch:  4\n","Male Female Classification loss: 0.54811030626297 iteration:  1330 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.2530577182769775 iteration:  1335 epoch:  4\n","LQ image reconstruction loss: 0.46063801646232605 iteration:  1335 epoch:  4\n","Same Different Classification loss: 0.42865294218063354 iteration:  1335 epoch:  4\n","Male Female Classification loss: 0.6253969073295593 iteration:  1335 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.3915746212005615 iteration:  1340 epoch:  4\n","LQ image reconstruction loss: 0.5027831196784973 iteration:  1340 epoch:  4\n","Same Different Classification loss: 0.46131694316864014 iteration:  1340 epoch:  4\n","Male Female Classification loss: 0.5614869594573975 iteration:  1340 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.591036319732666 iteration:  1345 epoch:  4\n","LQ image reconstruction loss: 0.5237160325050354 iteration:  1345 epoch:  4\n","Same Different Classification loss: 0.485387921333313 iteration:  1345 epoch:  4\n","Male Female Classification loss: 0.5880154967308044 iteration:  1345 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.6937026977539062 iteration:  1350 epoch:  4\n","LQ image reconstruction loss: 0.4602113962173462 iteration:  1350 epoch:  4\n","Same Different Classification loss: 0.42338797450065613 iteration:  1350 epoch:  4\n","Male Female Classification loss: 0.6068950295448303 iteration:  1350 epoch:  4\n","----------------------------------------------------\n","Accuracy of SD: 91 %\n","Accuracy of MF: 81 %\n","Mean Accuracy of SD: 61 %\n","Mean Accuracy of MF: 62 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.317904472351074 iteration:  1355 epoch:  4\n","LQ image reconstruction loss: 0.5251320004463196 iteration:  1355 epoch:  4\n","Same Different Classification loss: 0.4317176043987274 iteration:  1355 epoch:  4\n","Male Female Classification loss: 0.5739354491233826 iteration:  1355 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.4581122398376465 iteration:  1360 epoch:  4\n","LQ image reconstruction loss: 0.5153104066848755 iteration:  1360 epoch:  4\n","Same Different Classification loss: 0.46904173493385315 iteration:  1360 epoch:  4\n","Male Female Classification loss: 0.534890353679657 iteration:  1360 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.012117385864258 iteration:  1365 epoch:  4\n","LQ image reconstruction loss: 0.531680703163147 iteration:  1365 epoch:  4\n","Same Different Classification loss: 0.46653038263320923 iteration:  1365 epoch:  4\n","Male Female Classification loss: 0.6025878190994263 iteration:  1365 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.5859768390655518 iteration:  1370 epoch:  4\n","LQ image reconstruction loss: 0.4472546875476837 iteration:  1370 epoch:  4\n","Same Different Classification loss: 0.4318384528160095 iteration:  1370 epoch:  4\n","Male Female Classification loss: 0.6264873147010803 iteration:  1370 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.9449586868286133 iteration:  1375 epoch:  4\n","LQ image reconstruction loss: 0.4430572986602783 iteration:  1375 epoch:  4\n","Same Different Classification loss: 0.4272838234901428 iteration:  1375 epoch:  4\n","Male Female Classification loss: 0.6106245517730713 iteration:  1375 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.335205554962158 iteration:  1380 epoch:  4\n","LQ image reconstruction loss: 0.5592632293701172 iteration:  1380 epoch:  4\n","Same Different Classification loss: 0.47100940346717834 iteration:  1380 epoch:  4\n","Male Female Classification loss: 0.5691549181938171 iteration:  1380 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.14949107170105 iteration:  1385 epoch:  4\n","LQ image reconstruction loss: 0.48767340183258057 iteration:  1385 epoch:  4\n","Same Different Classification loss: 0.44081175327301025 iteration:  1385 epoch:  4\n","Male Female Classification loss: 0.6404870748519897 iteration:  1385 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 1.9977821111679077 iteration:  1390 epoch:  4\n","LQ image reconstruction loss: 0.48518919944763184 iteration:  1390 epoch:  4\n","Same Different Classification loss: 0.41331028938293457 iteration:  1390 epoch:  4\n","Male Female Classification loss: 0.5476870536804199 iteration:  1390 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.4236772060394287 iteration:  1395 epoch:  4\n","LQ image reconstruction loss: 0.5568050146102905 iteration:  1395 epoch:  4\n","Same Different Classification loss: 0.45379412174224854 iteration:  1395 epoch:  4\n","Male Female Classification loss: 0.5496231317520142 iteration:  1395 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 1.9060193300247192 iteration:  1400 epoch:  4\n","LQ image reconstruction loss: 0.4440285563468933 iteration:  1400 epoch:  4\n","Same Different Classification loss: 0.4104000926017761 iteration:  1400 epoch:  4\n","Male Female Classification loss: 0.6074889898300171 iteration:  1400 epoch:  4\n","----------------------------------------------------\n","Accuracy of SD: 87 %\n","Accuracy of MF: 72 %\n","Mean Accuracy of SD: 61 %\n","Mean Accuracy of MF: 62 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 1.944104552268982 iteration:  1405 epoch:  4\n","LQ image reconstruction loss: 0.5275295972824097 iteration:  1405 epoch:  4\n","Same Different Classification loss: 0.44422996044158936 iteration:  1405 epoch:  4\n","Male Female Classification loss: 0.5076942443847656 iteration:  1405 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.364593982696533 iteration:  1410 epoch:  4\n","LQ image reconstruction loss: 0.4552789032459259 iteration:  1410 epoch:  4\n","Same Different Classification loss: 0.4439973831176758 iteration:  1410 epoch:  4\n","Male Female Classification loss: 0.6474851369857788 iteration:  1410 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.287618637084961 iteration:  1415 epoch:  4\n","LQ image reconstruction loss: 0.5213528275489807 iteration:  1415 epoch:  4\n","Same Different Classification loss: 0.46105533838272095 iteration:  1415 epoch:  4\n","Male Female Classification loss: 0.625251829624176 iteration:  1415 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.101318836212158 iteration:  1420 epoch:  4\n","LQ image reconstruction loss: 0.4227426052093506 iteration:  1420 epoch:  4\n","Same Different Classification loss: 0.3367784023284912 iteration:  1420 epoch:  4\n","Male Female Classification loss: 0.5355918407440186 iteration:  1420 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.306804895401001 iteration:  1425 epoch:  4\n","LQ image reconstruction loss: 0.4569922983646393 iteration:  1425 epoch:  4\n","Same Different Classification loss: 0.4453190863132477 iteration:  1425 epoch:  4\n","Male Female Classification loss: 0.6776397824287415 iteration:  1425 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.2577452659606934 iteration:  1430 epoch:  4\n","LQ image reconstruction loss: 0.6046799421310425 iteration:  1430 epoch:  4\n","Same Different Classification loss: 0.4995996356010437 iteration:  1430 epoch:  4\n","Male Female Classification loss: 0.5668509006500244 iteration:  1430 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.502791404724121 iteration:  1435 epoch:  4\n","LQ image reconstruction loss: 0.44113779067993164 iteration:  1435 epoch:  4\n","Same Different Classification loss: 0.4137125313282013 iteration:  1435 epoch:  4\n","Male Female Classification loss: 0.5979118943214417 iteration:  1435 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.354731321334839 iteration:  1440 epoch:  4\n","LQ image reconstruction loss: 0.4474319815635681 iteration:  1440 epoch:  4\n","Same Different Classification loss: 0.4096868634223938 iteration:  1440 epoch:  4\n","Male Female Classification loss: 0.5336844325065613 iteration:  1440 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.5224359035491943 iteration:  1445 epoch:  4\n","LQ image reconstruction loss: 0.4619387984275818 iteration:  1445 epoch:  4\n","Same Different Classification loss: 0.4050702154636383 iteration:  1445 epoch:  4\n","Male Female Classification loss: 0.5309186577796936 iteration:  1445 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.716364860534668 iteration:  1450 epoch:  4\n","LQ image reconstruction loss: 0.45574480295181274 iteration:  1450 epoch:  4\n","Same Different Classification loss: 0.40891575813293457 iteration:  1450 epoch:  4\n","Male Female Classification loss: 0.6235908269882202 iteration:  1450 epoch:  4\n","----------------------------------------------------\n","Accuracy of SD: 84 %\n","Accuracy of MF: 66 %\n","Mean Accuracy of SD: 61 %\n","Mean Accuracy of MF: 62 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.097935914993286 iteration:  1455 epoch:  4\n","LQ image reconstruction loss: 0.4576755166053772 iteration:  1455 epoch:  4\n","Same Different Classification loss: 0.37112781405448914 iteration:  1455 epoch:  4\n","Male Female Classification loss: 0.578442394733429 iteration:  1455 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.47864031791687 iteration:  1460 epoch:  4\n","LQ image reconstruction loss: 0.4906497895717621 iteration:  1460 epoch:  4\n","Same Different Classification loss: 0.4383910000324249 iteration:  1460 epoch:  4\n","Male Female Classification loss: 0.6060916781425476 iteration:  1460 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.459566354751587 iteration:  1465 epoch:  4\n","LQ image reconstruction loss: 0.5502539873123169 iteration:  1465 epoch:  4\n","Same Different Classification loss: 0.5014488697052002 iteration:  1465 epoch:  4\n","Male Female Classification loss: 0.6118255257606506 iteration:  1465 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 1.9193484783172607 iteration:  1470 epoch:  4\n","LQ image reconstruction loss: 0.3809375762939453 iteration:  1470 epoch:  4\n","Same Different Classification loss: 0.34011468291282654 iteration:  1470 epoch:  4\n","Male Female Classification loss: 0.6267971992492676 iteration:  1470 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.4629249572753906 iteration:  1475 epoch:  4\n","LQ image reconstruction loss: 0.45996198058128357 iteration:  1475 epoch:  4\n","Same Different Classification loss: 0.4269773066043854 iteration:  1475 epoch:  4\n","Male Female Classification loss: 0.5688214898109436 iteration:  1475 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 1.730757474899292 iteration:  1480 epoch:  4\n","LQ image reconstruction loss: 0.41871950030326843 iteration:  1480 epoch:  4\n","Same Different Classification loss: 0.352334201335907 iteration:  1480 epoch:  4\n","Male Female Classification loss: 0.624188244342804 iteration:  1480 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.1128599643707275 iteration:  1485 epoch:  4\n","LQ image reconstruction loss: 0.4428175091743469 iteration:  1485 epoch:  4\n","Same Different Classification loss: 0.3768843710422516 iteration:  1485 epoch:  4\n","Male Female Classification loss: 0.5675967931747437 iteration:  1485 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.4148712158203125 iteration:  1490 epoch:  4\n","LQ image reconstruction loss: 0.5077635049819946 iteration:  1490 epoch:  4\n","Same Different Classification loss: 0.40863358974456787 iteration:  1490 epoch:  4\n","Male Female Classification loss: 0.5075839161872864 iteration:  1490 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.1298274993896484 iteration:  1495 epoch:  4\n","LQ image reconstruction loss: 0.5108509659767151 iteration:  1495 epoch:  4\n","Same Different Classification loss: 0.42192089557647705 iteration:  1495 epoch:  4\n","Male Female Classification loss: 0.5453130602836609 iteration:  1495 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 1.9908185005187988 iteration:  1500 epoch:  4\n","LQ image reconstruction loss: 0.47290924191474915 iteration:  1500 epoch:  4\n","Same Different Classification loss: 0.4224274456501007 iteration:  1500 epoch:  4\n","Male Female Classification loss: 0.6151408553123474 iteration:  1500 epoch:  4\n","----------------------------------------------------\n","Accuracy of SD: 88 %\n","Accuracy of MF: 74 %\n","Mean Accuracy of SD: 62 %\n","Mean Accuracy of MF: 62 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.4583165645599365 iteration:  1505 epoch:  4\n","LQ image reconstruction loss: 0.4887263774871826 iteration:  1505 epoch:  4\n","Same Different Classification loss: 0.45696690678596497 iteration:  1505 epoch:  4\n","Male Female Classification loss: 0.563890814781189 iteration:  1505 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.5560340881347656 iteration:  1510 epoch:  4\n","LQ image reconstruction loss: 0.45303624868392944 iteration:  1510 epoch:  4\n","Same Different Classification loss: 0.40457960963249207 iteration:  1510 epoch:  4\n","Male Female Classification loss: 0.5926045775413513 iteration:  1510 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.399322271347046 iteration:  1515 epoch:  4\n","LQ image reconstruction loss: 0.517708420753479 iteration:  1515 epoch:  4\n","Same Different Classification loss: 0.4173232913017273 iteration:  1515 epoch:  4\n","Male Female Classification loss: 0.5799273252487183 iteration:  1515 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.1938865184783936 iteration:  1520 epoch:  4\n","LQ image reconstruction loss: 0.4521252512931824 iteration:  1520 epoch:  4\n","Same Different Classification loss: 0.4016963839530945 iteration:  1520 epoch:  4\n","Male Female Classification loss: 0.6122882962226868 iteration:  1520 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.1393933296203613 iteration:  1525 epoch:  4\n","LQ image reconstruction loss: 0.3757037818431854 iteration:  1525 epoch:  4\n","Same Different Classification loss: 0.3393780589103699 iteration:  1525 epoch:  4\n","Male Female Classification loss: 0.6351901888847351 iteration:  1525 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.3431246280670166 iteration:  1530 epoch:  4\n","LQ image reconstruction loss: 0.49077677726745605 iteration:  1530 epoch:  4\n","Same Different Classification loss: 0.39820998907089233 iteration:  1530 epoch:  4\n","Male Female Classification loss: 0.4988880455493927 iteration:  1530 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.792283773422241 iteration:  1535 epoch:  4\n","LQ image reconstruction loss: 0.6763365268707275 iteration:  1535 epoch:  4\n","Same Different Classification loss: 0.5979413390159607 iteration:  1535 epoch:  4\n","Male Female Classification loss: 0.5792746543884277 iteration:  1535 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.3142495155334473 iteration:  1540 epoch:  4\n","LQ image reconstruction loss: 0.5565732717514038 iteration:  1540 epoch:  4\n","Same Different Classification loss: 0.4541527032852173 iteration:  1540 epoch:  4\n","Male Female Classification loss: 0.5334938764572144 iteration:  1540 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.2499828338623047 iteration:  1545 epoch:  4\n","LQ image reconstruction loss: 0.4109482169151306 iteration:  1545 epoch:  4\n","Same Different Classification loss: 0.37452080845832825 iteration:  1545 epoch:  4\n","Male Female Classification loss: 0.5583186149597168 iteration:  1545 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.746084451675415 iteration:  1550 epoch:  4\n","LQ image reconstruction loss: 0.4982340633869171 iteration:  1550 epoch:  4\n","Same Different Classification loss: 0.47713905572891235 iteration:  1550 epoch:  4\n","Male Female Classification loss: 0.6358312368392944 iteration:  1550 epoch:  4\n","----------------------------------------------------\n","Accuracy of SD: 83 %\n","Accuracy of MF: 68 %\n","Mean Accuracy of SD: 62 %\n","Mean Accuracy of MF: 62 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.111809492111206 iteration:  1555 epoch:  4\n","LQ image reconstruction loss: 0.4364188611507416 iteration:  1555 epoch:  4\n","Same Different Classification loss: 0.397719144821167 iteration:  1555 epoch:  4\n","Male Female Classification loss: 0.638471782207489 iteration:  1555 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.7982265949249268 iteration:  1560 epoch:  4\n","LQ image reconstruction loss: 0.48143887519836426 iteration:  1560 epoch:  4\n","Same Different Classification loss: 0.39302435517311096 iteration:  1560 epoch:  4\n","Male Female Classification loss: 0.5017231106758118 iteration:  1560 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.194254159927368 iteration:  1565 epoch:  4\n","LQ image reconstruction loss: 0.4542143940925598 iteration:  1565 epoch:  4\n","Same Different Classification loss: 0.3912220895290375 iteration:  1565 epoch:  4\n","Male Female Classification loss: 0.584802508354187 iteration:  1565 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.9187684059143066 iteration:  1570 epoch:  4\n","LQ image reconstruction loss: 0.49764400720596313 iteration:  1570 epoch:  4\n","Same Different Classification loss: 0.38448500633239746 iteration:  1570 epoch:  4\n","Male Female Classification loss: 0.5849641561508179 iteration:  1570 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.3141441345214844 iteration:  1575 epoch:  4\n","LQ image reconstruction loss: 0.41752898693084717 iteration:  1575 epoch:  4\n","Same Different Classification loss: 0.3632507026195526 iteration:  1575 epoch:  4\n","Male Female Classification loss: 0.5784436464309692 iteration:  1575 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 1.93405282497406 iteration:  1580 epoch:  4\n","LQ image reconstruction loss: 0.42759254574775696 iteration:  1580 epoch:  4\n","Same Different Classification loss: 0.3794058859348297 iteration:  1580 epoch:  4\n","Male Female Classification loss: 0.5868402719497681 iteration:  1580 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.3138129711151123 iteration:  1585 epoch:  4\n","LQ image reconstruction loss: 0.4561101198196411 iteration:  1585 epoch:  4\n","Same Different Classification loss: 0.4112916588783264 iteration:  1585 epoch:  4\n","Male Female Classification loss: 0.557396411895752 iteration:  1585 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.641655921936035 iteration:  1590 epoch:  4\n","LQ image reconstruction loss: 0.4386189579963684 iteration:  1590 epoch:  4\n","Same Different Classification loss: 0.3856894373893738 iteration:  1590 epoch:  4\n","Male Female Classification loss: 0.611508309841156 iteration:  1590 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.9300782680511475 iteration:  1595 epoch:  4\n","LQ image reconstruction loss: 0.647173285484314 iteration:  1595 epoch:  4\n","Same Different Classification loss: 0.5849693417549133 iteration:  1595 epoch:  4\n","Male Female Classification loss: 0.5348088145256042 iteration:  1595 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.5901026725769043 iteration:  1600 epoch:  4\n","LQ image reconstruction loss: 0.49308091402053833 iteration:  1600 epoch:  4\n","Same Different Classification loss: 0.39534232020378113 iteration:  1600 epoch:  4\n","Male Female Classification loss: 0.5376663208007812 iteration:  1600 epoch:  4\n","----------------------------------------------------\n","Accuracy of SD: 86 %\n","Accuracy of MF: 75 %\n","Mean Accuracy of SD: 62 %\n","Mean Accuracy of MF: 62 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.4645626544952393 iteration:  1605 epoch:  4\n","LQ image reconstruction loss: 0.4525982737541199 iteration:  1605 epoch:  4\n","Same Different Classification loss: 0.3865495026111603 iteration:  1605 epoch:  4\n","Male Female Classification loss: 0.5295414924621582 iteration:  1605 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.1048476696014404 iteration:  1610 epoch:  4\n","LQ image reconstruction loss: 0.40288108587265015 iteration:  1610 epoch:  4\n","Same Different Classification loss: 0.38448888063430786 iteration:  1610 epoch:  4\n","Male Female Classification loss: 0.6162362098693848 iteration:  1610 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 1.8955204486846924 iteration:  1615 epoch:  4\n","LQ image reconstruction loss: 0.4113461375236511 iteration:  1615 epoch:  4\n","Same Different Classification loss: 0.3806796967983246 iteration:  1615 epoch:  4\n","Male Female Classification loss: 0.5604373216629028 iteration:  1615 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.266948699951172 iteration:  1620 epoch:  4\n","LQ image reconstruction loss: 0.48587560653686523 iteration:  1620 epoch:  4\n","Same Different Classification loss: 0.44044196605682373 iteration:  1620 epoch:  4\n","Male Female Classification loss: 0.5952920317649841 iteration:  1620 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.226386308670044 iteration:  1625 epoch:  4\n","LQ image reconstruction loss: 0.5125649571418762 iteration:  1625 epoch:  4\n","Same Different Classification loss: 0.4417811632156372 iteration:  1625 epoch:  4\n","Male Female Classification loss: 0.5630843639373779 iteration:  1625 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.131953477859497 iteration:  1630 epoch:  4\n","LQ image reconstruction loss: 0.5669392943382263 iteration:  1630 epoch:  4\n","Same Different Classification loss: 0.4662289321422577 iteration:  1630 epoch:  4\n","Male Female Classification loss: 0.6243784427642822 iteration:  1630 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 1.8236181735992432 iteration:  1635 epoch:  4\n","LQ image reconstruction loss: 0.4168006479740143 iteration:  1635 epoch:  4\n","Same Different Classification loss: 0.36452987790107727 iteration:  1635 epoch:  4\n","Male Female Classification loss: 0.6356715559959412 iteration:  1635 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.2414474487304688 iteration:  1640 epoch:  4\n","LQ image reconstruction loss: 0.4306829571723938 iteration:  1640 epoch:  4\n","Same Different Classification loss: 0.3925887644290924 iteration:  1640 epoch:  4\n","Male Female Classification loss: 0.5861327052116394 iteration:  1640 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.267122268676758 iteration:  1645 epoch:  4\n","LQ image reconstruction loss: 0.4291200339794159 iteration:  1645 epoch:  4\n","Same Different Classification loss: 0.4098164737224579 iteration:  1645 epoch:  4\n","Male Female Classification loss: 0.636436402797699 iteration:  1645 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.2965519428253174 iteration:  1650 epoch:  4\n","LQ image reconstruction loss: 0.5002009272575378 iteration:  1650 epoch:  4\n","Same Different Classification loss: 0.4172849953174591 iteration:  1650 epoch:  4\n","Male Female Classification loss: 0.5540152192115784 iteration:  1650 epoch:  4\n","----------------------------------------------------\n","Accuracy of SD: 76 %\n","Accuracy of MF: 77 %\n","Mean Accuracy of SD: 62 %\n","Mean Accuracy of MF: 62 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.400151491165161 iteration:  1655 epoch:  4\n","LQ image reconstruction loss: 0.4577196538448334 iteration:  1655 epoch:  4\n","Same Different Classification loss: 0.4092073142528534 iteration:  1655 epoch:  4\n","Male Female Classification loss: 0.6024328470230103 iteration:  1655 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.1697113513946533 iteration:  1660 epoch:  4\n","LQ image reconstruction loss: 0.3940337896347046 iteration:  1660 epoch:  4\n","Same Different Classification loss: 0.36444011330604553 iteration:  1660 epoch:  4\n","Male Female Classification loss: 0.5976753830909729 iteration:  1660 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.0474274158477783 iteration:  1665 epoch:  4\n","LQ image reconstruction loss: 0.3859599232673645 iteration:  1665 epoch:  4\n","Same Different Classification loss: 0.3668826222419739 iteration:  1665 epoch:  4\n","Male Female Classification loss: 0.642866313457489 iteration:  1665 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.606415033340454 iteration:  1670 epoch:  4\n","LQ image reconstruction loss: 0.47487860918045044 iteration:  1670 epoch:  4\n","Same Different Classification loss: 0.4389289915561676 iteration:  1670 epoch:  4\n","Male Female Classification loss: 0.5964226722717285 iteration:  1670 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.1370084285736084 iteration:  1675 epoch:  4\n","LQ image reconstruction loss: 0.47181278467178345 iteration:  1675 epoch:  4\n","Same Different Classification loss: 0.4209916591644287 iteration:  1675 epoch:  4\n","Male Female Classification loss: 0.5891746282577515 iteration:  1675 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.1900441646575928 iteration:  1680 epoch:  4\n","LQ image reconstruction loss: 0.4597129821777344 iteration:  1680 epoch:  4\n","Same Different Classification loss: 0.3766333758831024 iteration:  1680 epoch:  4\n","Male Female Classification loss: 0.4941403567790985 iteration:  1680 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 1.8034350872039795 iteration:  1685 epoch:  4\n","LQ image reconstruction loss: 0.39439254999160767 iteration:  1685 epoch:  4\n","Same Different Classification loss: 0.3855687379837036 iteration:  1685 epoch:  4\n","Male Female Classification loss: 0.6160836219787598 iteration:  1685 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.0881614685058594 iteration:  1690 epoch:  4\n","LQ image reconstruction loss: 0.427623987197876 iteration:  1690 epoch:  4\n","Same Different Classification loss: 0.34881800413131714 iteration:  1690 epoch:  4\n","Male Female Classification loss: 0.5720439553260803 iteration:  1690 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.1226370334625244 iteration:  1695 epoch:  4\n","LQ image reconstruction loss: 0.4768521785736084 iteration:  1695 epoch:  4\n","Same Different Classification loss: 0.4129618704319 iteration:  1695 epoch:  4\n","Male Female Classification loss: 0.5533701181411743 iteration:  1695 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.083427667617798 iteration:  1700 epoch:  4\n","LQ image reconstruction loss: 0.45598381757736206 iteration:  1700 epoch:  4\n","Same Different Classification loss: 0.41374844312667847 iteration:  1700 epoch:  4\n","Male Female Classification loss: 0.5855354070663452 iteration:  1700 epoch:  4\n","----------------------------------------------------\n","Accuracy of SD: 82 %\n","Accuracy of MF: 80 %\n","Mean Accuracy of SD: 62 %\n","Mean Accuracy of MF: 62 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.9670510292053223 iteration:  1705 epoch:  4\n","LQ image reconstruction loss: 0.4375796318054199 iteration:  1705 epoch:  4\n","Same Different Classification loss: 0.402968168258667 iteration:  1705 epoch:  4\n","Male Female Classification loss: 0.5869459509849548 iteration:  1705 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.6856698989868164 iteration:  1710 epoch:  4\n","LQ image reconstruction loss: 0.5963177680969238 iteration:  1710 epoch:  4\n","Same Different Classification loss: 0.5419878363609314 iteration:  1710 epoch:  4\n","Male Female Classification loss: 0.6035962700843811 iteration:  1710 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.079943895339966 iteration:  1715 epoch:  4\n","LQ image reconstruction loss: 0.42813462018966675 iteration:  1715 epoch:  4\n","Same Different Classification loss: 0.39713355898857117 iteration:  1715 epoch:  4\n","Male Female Classification loss: 0.5955756902694702 iteration:  1715 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.317688226699829 iteration:  1720 epoch:  4\n","LQ image reconstruction loss: 0.4429778456687927 iteration:  1720 epoch:  4\n","Same Different Classification loss: 0.38079047203063965 iteration:  1720 epoch:  4\n","Male Female Classification loss: 0.5559377670288086 iteration:  1720 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.3834376335144043 iteration:  1725 epoch:  4\n","LQ image reconstruction loss: 0.4598805904388428 iteration:  1725 epoch:  4\n","Same Different Classification loss: 0.38625118136405945 iteration:  1725 epoch:  4\n","Male Female Classification loss: 0.5178859829902649 iteration:  1725 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 1.982743263244629 iteration:  1730 epoch:  4\n","LQ image reconstruction loss: 0.368583619594574 iteration:  1730 epoch:  4\n","Same Different Classification loss: 0.32307887077331543 iteration:  1730 epoch:  4\n","Male Female Classification loss: 0.620869517326355 iteration:  1730 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.1086666584014893 iteration:  1735 epoch:  4\n","LQ image reconstruction loss: 0.4801405966281891 iteration:  1735 epoch:  4\n","Same Different Classification loss: 0.44703006744384766 iteration:  1735 epoch:  4\n","Male Female Classification loss: 0.6114596724510193 iteration:  1735 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.069822311401367 iteration:  1740 epoch:  4\n","LQ image reconstruction loss: 0.5052120685577393 iteration:  1740 epoch:  4\n","Same Different Classification loss: 0.41352254152297974 iteration:  1740 epoch:  4\n","Male Female Classification loss: 0.5103421807289124 iteration:  1740 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.0785839557647705 iteration:  1745 epoch:  4\n","LQ image reconstruction loss: 0.5053033232688904 iteration:  1745 epoch:  4\n","Same Different Classification loss: 0.4220898747444153 iteration:  1745 epoch:  4\n","Male Female Classification loss: 0.611980140209198 iteration:  1745 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.322871208190918 iteration:  1750 epoch:  4\n","LQ image reconstruction loss: 0.42824119329452515 iteration:  1750 epoch:  4\n","Same Different Classification loss: 0.3636602461338043 iteration:  1750 epoch:  4\n","Male Female Classification loss: 0.6043139100074768 iteration:  1750 epoch:  4\n","----------------------------------------------------\n","Accuracy of SD: 75 %\n","Accuracy of MF: 75 %\n","Mean Accuracy of SD: 62 %\n","Mean Accuracy of MF: 62 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.0838327407836914 iteration:  1755 epoch:  4\n","LQ image reconstruction loss: 0.4924185872077942 iteration:  1755 epoch:  4\n","Same Different Classification loss: 0.4335392713546753 iteration:  1755 epoch:  4\n","Male Female Classification loss: 0.502089262008667 iteration:  1755 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.1811771392822266 iteration:  1760 epoch:  4\n","LQ image reconstruction loss: 0.43738698959350586 iteration:  1760 epoch:  4\n","Same Different Classification loss: 0.411551296710968 iteration:  1760 epoch:  4\n","Male Female Classification loss: 0.5874982476234436 iteration:  1760 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.8769922256469727 iteration:  1765 epoch:  4\n","LQ image reconstruction loss: 0.45507535338401794 iteration:  1765 epoch:  4\n","Same Different Classification loss: 0.4074598252773285 iteration:  1765 epoch:  4\n","Male Female Classification loss: 0.526868462562561 iteration:  1765 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.193673610687256 iteration:  1770 epoch:  4\n","LQ image reconstruction loss: 0.4065867066383362 iteration:  1770 epoch:  4\n","Same Different Classification loss: 0.3641168475151062 iteration:  1770 epoch:  4\n","Male Female Classification loss: 0.5496156215667725 iteration:  1770 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.404221296310425 iteration:  1775 epoch:  4\n","LQ image reconstruction loss: 0.458335280418396 iteration:  1775 epoch:  4\n","Same Different Classification loss: 0.41685882210731506 iteration:  1775 epoch:  4\n","Male Female Classification loss: 0.5939418077468872 iteration:  1775 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.0246856212615967 iteration:  1780 epoch:  4\n","LQ image reconstruction loss: 0.4181288480758667 iteration:  1780 epoch:  4\n","Same Different Classification loss: 0.3978065848350525 iteration:  1780 epoch:  4\n","Male Female Classification loss: 0.5676789879798889 iteration:  1780 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.530395030975342 iteration:  1785 epoch:  4\n","LQ image reconstruction loss: 0.40144652128219604 iteration:  1785 epoch:  4\n","Same Different Classification loss: 0.35871386528015137 iteration:  1785 epoch:  4\n","Male Female Classification loss: 0.5835403203964233 iteration:  1785 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.9685823917388916 iteration:  1790 epoch:  4\n","LQ image reconstruction loss: 0.5211724042892456 iteration:  1790 epoch:  4\n","Same Different Classification loss: 0.4677925109863281 iteration:  1790 epoch:  4\n","Male Female Classification loss: 0.5948379039764404 iteration:  1790 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.650319814682007 iteration:  1795 epoch:  4\n","LQ image reconstruction loss: 0.5445348620414734 iteration:  1795 epoch:  4\n","Same Different Classification loss: 0.4981038272380829 iteration:  1795 epoch:  4\n","Male Female Classification loss: 0.5472527146339417 iteration:  1795 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.559209108352661 iteration:  1800 epoch:  4\n","LQ image reconstruction loss: 0.500065803527832 iteration:  1800 epoch:  4\n","Same Different Classification loss: 0.4377080798149109 iteration:  1800 epoch:  4\n","Male Female Classification loss: 0.6392961144447327 iteration:  1800 epoch:  4\n","----------------------------------------------------\n","Accuracy of SD: 82 %\n","Accuracy of MF: 75 %\n","Mean Accuracy of SD: 62 %\n","Mean Accuracy of MF: 62 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.933171033859253 iteration:  1805 epoch:  4\n","LQ image reconstruction loss: 0.445276141166687 iteration:  1805 epoch:  4\n","Same Different Classification loss: 0.4032210409641266 iteration:  1805 epoch:  4\n","Male Female Classification loss: 0.5686962604522705 iteration:  1805 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.7973766326904297 iteration:  1810 epoch:  4\n","LQ image reconstruction loss: 0.5207914113998413 iteration:  1810 epoch:  4\n","Same Different Classification loss: 0.47665414214134216 iteration:  1810 epoch:  4\n","Male Female Classification loss: 0.578421413898468 iteration:  1810 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.797591209411621 iteration:  1815 epoch:  4\n","LQ image reconstruction loss: 0.410400390625 iteration:  1815 epoch:  4\n","Same Different Classification loss: 0.37583935260772705 iteration:  1815 epoch:  4\n","Male Female Classification loss: 0.6186010837554932 iteration:  1815 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.1290628910064697 iteration:  1820 epoch:  4\n","LQ image reconstruction loss: 0.4315311014652252 iteration:  1820 epoch:  4\n","Same Different Classification loss: 0.3998059034347534 iteration:  1820 epoch:  4\n","Male Female Classification loss: 0.6403713226318359 iteration:  1820 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.5612683296203613 iteration:  1825 epoch:  4\n","LQ image reconstruction loss: 0.4300939738750458 iteration:  1825 epoch:  4\n","Same Different Classification loss: 0.41139766573905945 iteration:  1825 epoch:  4\n","Male Female Classification loss: 0.6256141066551208 iteration:  1825 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 1.9266860485076904 iteration:  1830 epoch:  4\n","LQ image reconstruction loss: 0.46462762355804443 iteration:  1830 epoch:  4\n","Same Different Classification loss: 0.4307367205619812 iteration:  1830 epoch:  4\n","Male Female Classification loss: 0.5375491976737976 iteration:  1830 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.1604106426239014 iteration:  1835 epoch:  4\n","LQ image reconstruction loss: 0.5152201652526855 iteration:  1835 epoch:  4\n","Same Different Classification loss: 0.46560215950012207 iteration:  1835 epoch:  4\n","Male Female Classification loss: 0.5432667136192322 iteration:  1835 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.161597728729248 iteration:  1840 epoch:  4\n","LQ image reconstruction loss: 0.49561887979507446 iteration:  1840 epoch:  4\n","Same Different Classification loss: 0.44083839654922485 iteration:  1840 epoch:  4\n","Male Female Classification loss: 0.5703757405281067 iteration:  1840 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 1.6838195323944092 iteration:  1845 epoch:  4\n","LQ image reconstruction loss: 0.39570152759552 iteration:  1845 epoch:  4\n","Same Different Classification loss: 0.3613845407962799 iteration:  1845 epoch:  4\n","Male Female Classification loss: 0.6565423011779785 iteration:  1845 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 1.9365477561950684 iteration:  1850 epoch:  4\n","LQ image reconstruction loss: 0.44278985261917114 iteration:  1850 epoch:  4\n","Same Different Classification loss: 0.39873775839805603 iteration:  1850 epoch:  4\n","Male Female Classification loss: 0.5654745697975159 iteration:  1850 epoch:  4\n","----------------------------------------------------\n","Accuracy of SD: 83 %\n","Accuracy of MF: 80 %\n","Mean Accuracy of SD: 62 %\n","Mean Accuracy of MF: 62 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.0300354957580566 iteration:  1855 epoch:  4\n","LQ image reconstruction loss: 0.4141782522201538 iteration:  1855 epoch:  4\n","Same Different Classification loss: 0.36744770407676697 iteration:  1855 epoch:  4\n","Male Female Classification loss: 0.579618513584137 iteration:  1855 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.4721593856811523 iteration:  1860 epoch:  4\n","LQ image reconstruction loss: 0.44939348101615906 iteration:  1860 epoch:  4\n","Same Different Classification loss: 0.4244929850101471 iteration:  1860 epoch:  4\n","Male Female Classification loss: 0.5957347750663757 iteration:  1860 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 1.9599045515060425 iteration:  1865 epoch:  4\n","LQ image reconstruction loss: 0.45434248447418213 iteration:  1865 epoch:  4\n","Same Different Classification loss: 0.44087693095207214 iteration:  1865 epoch:  4\n","Male Female Classification loss: 0.6056036949157715 iteration:  1865 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 1.6939150094985962 iteration:  1870 epoch:  4\n","LQ image reconstruction loss: 0.4413849115371704 iteration:  1870 epoch:  4\n","Same Different Classification loss: 0.40045270323753357 iteration:  1870 epoch:  4\n","Male Female Classification loss: 0.5557101368904114 iteration:  1870 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 1.931229829788208 iteration:  1875 epoch:  4\n","LQ image reconstruction loss: 0.40624749660491943 iteration:  1875 epoch:  4\n","Same Different Classification loss: 0.32426917552948 iteration:  1875 epoch:  4\n","Male Female Classification loss: 0.5317240357398987 iteration:  1875 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 1.9111629724502563 iteration:  1880 epoch:  4\n","LQ image reconstruction loss: 0.43668079376220703 iteration:  1880 epoch:  4\n","Same Different Classification loss: 0.405671089887619 iteration:  1880 epoch:  4\n","Male Female Classification loss: 0.585039496421814 iteration:  1880 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.0185844898223877 iteration:  1885 epoch:  4\n","LQ image reconstruction loss: 0.4103626012802124 iteration:  1885 epoch:  4\n","Same Different Classification loss: 0.40044254064559937 iteration:  1885 epoch:  4\n","Male Female Classification loss: 0.6015158891677856 iteration:  1885 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.30301570892334 iteration:  1890 epoch:  4\n","LQ image reconstruction loss: 0.5042899250984192 iteration:  1890 epoch:  4\n","Same Different Classification loss: 0.4228701889514923 iteration:  1890 epoch:  4\n","Male Female Classification loss: 0.5337175726890564 iteration:  1890 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.9031455516815186 iteration:  1895 epoch:  4\n","LQ image reconstruction loss: 0.49101120233535767 iteration:  1895 epoch:  4\n","Same Different Classification loss: 0.4394252896308899 iteration:  1895 epoch:  4\n","Male Female Classification loss: 0.534625232219696 iteration:  1895 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.435906171798706 iteration:  1900 epoch:  4\n","LQ image reconstruction loss: 0.4438309371471405 iteration:  1900 epoch:  4\n","Same Different Classification loss: 0.4063393771648407 iteration:  1900 epoch:  4\n","Male Female Classification loss: 0.5528188347816467 iteration:  1900 epoch:  4\n","----------------------------------------------------\n","Accuracy of SD: 80 %\n","Accuracy of MF: 65 %\n","Mean Accuracy of SD: 62 %\n","Mean Accuracy of MF: 62 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.80651593208313 iteration:  1905 epoch:  4\n","LQ image reconstruction loss: 0.5111241340637207 iteration:  1905 epoch:  4\n","Same Different Classification loss: 0.469241738319397 iteration:  1905 epoch:  4\n","Male Female Classification loss: 0.6475525498390198 iteration:  1905 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.8532042503356934 iteration:  1910 epoch:  4\n","LQ image reconstruction loss: 0.49032527208328247 iteration:  1910 epoch:  4\n","Same Different Classification loss: 0.440698504447937 iteration:  1910 epoch:  4\n","Male Female Classification loss: 0.5951654314994812 iteration:  1910 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.908080577850342 iteration:  1915 epoch:  4\n","LQ image reconstruction loss: 0.410815954208374 iteration:  1915 epoch:  4\n","Same Different Classification loss: 0.4075002372264862 iteration:  1915 epoch:  4\n","Male Female Classification loss: 0.6424659490585327 iteration:  1915 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.793837070465088 iteration:  1920 epoch:  4\n","LQ image reconstruction loss: 0.4194526970386505 iteration:  1920 epoch:  4\n","Same Different Classification loss: 0.4004395604133606 iteration:  1920 epoch:  4\n","Male Female Classification loss: 0.6400146484375 iteration:  1920 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.7702832221984863 iteration:  1925 epoch:  4\n","LQ image reconstruction loss: 0.5039005279541016 iteration:  1925 epoch:  4\n","Same Different Classification loss: 0.46113407611846924 iteration:  1925 epoch:  4\n","Male Female Classification loss: 0.6168541312217712 iteration:  1925 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.861969470977783 iteration:  1930 epoch:  4\n","LQ image reconstruction loss: 0.45335614681243896 iteration:  1930 epoch:  4\n","Same Different Classification loss: 0.42977845668792725 iteration:  1930 epoch:  4\n","Male Female Classification loss: 0.5841577053070068 iteration:  1930 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.199359655380249 iteration:  1935 epoch:  4\n","LQ image reconstruction loss: 0.40628302097320557 iteration:  1935 epoch:  4\n","Same Different Classification loss: 0.40083834528923035 iteration:  1935 epoch:  4\n","Male Female Classification loss: 0.6424648761749268 iteration:  1935 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.7485339641571045 iteration:  1940 epoch:  4\n","LQ image reconstruction loss: 0.44462141394615173 iteration:  1940 epoch:  4\n","Same Different Classification loss: 0.3916124701499939 iteration:  1940 epoch:  4\n","Male Female Classification loss: 0.5097371935844421 iteration:  1940 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.9845478534698486 iteration:  1945 epoch:  4\n","LQ image reconstruction loss: 0.4304404854774475 iteration:  1945 epoch:  4\n","Same Different Classification loss: 0.4209217429161072 iteration:  1945 epoch:  4\n","Male Female Classification loss: 0.5699618458747864 iteration:  1945 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.441657066345215 iteration:  1950 epoch:  4\n","LQ image reconstruction loss: 0.382670521736145 iteration:  1950 epoch:  4\n","Same Different Classification loss: 0.3717910647392273 iteration:  1950 epoch:  4\n","Male Female Classification loss: 0.5823298692703247 iteration:  1950 epoch:  4\n","----------------------------------------------------\n","Accuracy of SD: 81 %\n","Accuracy of MF: 72 %\n","Mean Accuracy of SD: 62 %\n","Mean Accuracy of MF: 63 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.4222397804260254 iteration:  1955 epoch:  4\n","LQ image reconstruction loss: 0.4407300353050232 iteration:  1955 epoch:  4\n","Same Different Classification loss: 0.37580639123916626 iteration:  1955 epoch:  4\n","Male Female Classification loss: 0.546830415725708 iteration:  1955 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.346928596496582 iteration:  1960 epoch:  4\n","LQ image reconstruction loss: 0.467343270778656 iteration:  1960 epoch:  4\n","Same Different Classification loss: 0.40376776456832886 iteration:  1960 epoch:  4\n","Male Female Classification loss: 0.567089319229126 iteration:  1960 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.2345924377441406 iteration:  1965 epoch:  4\n","LQ image reconstruction loss: 0.437004417181015 iteration:  1965 epoch:  4\n","Same Different Classification loss: 0.3829255700111389 iteration:  1965 epoch:  4\n","Male Female Classification loss: 0.5867592096328735 iteration:  1965 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.3231449127197266 iteration:  1970 epoch:  4\n","LQ image reconstruction loss: 0.4465104937553406 iteration:  1970 epoch:  4\n","Same Different Classification loss: 0.3820900321006775 iteration:  1970 epoch:  4\n","Male Female Classification loss: 0.5810521841049194 iteration:  1970 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.110766649246216 iteration:  1975 epoch:  4\n","LQ image reconstruction loss: 0.48145657777786255 iteration:  1975 epoch:  4\n","Same Different Classification loss: 0.4104342758655548 iteration:  1975 epoch:  4\n","Male Female Classification loss: 0.5475156903266907 iteration:  1975 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 1.8377025127410889 iteration:  1980 epoch:  4\n","LQ image reconstruction loss: 0.43671661615371704 iteration:  1980 epoch:  4\n","Same Different Classification loss: 0.3752359449863434 iteration:  1980 epoch:  4\n","Male Female Classification loss: 0.562177300453186 iteration:  1980 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 1.9041908979415894 iteration:  1985 epoch:  4\n","LQ image reconstruction loss: 0.42516785860061646 iteration:  1985 epoch:  4\n","Same Different Classification loss: 0.38864627480506897 iteration:  1985 epoch:  4\n","Male Female Classification loss: 0.5803863406181335 iteration:  1985 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.0829668045043945 iteration:  1990 epoch:  4\n","LQ image reconstruction loss: 0.4129853844642639 iteration:  1990 epoch:  4\n","Same Different Classification loss: 0.3811987638473511 iteration:  1990 epoch:  4\n","Male Female Classification loss: 0.5919994115829468 iteration:  1990 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.1098287105560303 iteration:  1995 epoch:  4\n","LQ image reconstruction loss: 0.3603496551513672 iteration:  1995 epoch:  4\n","Same Different Classification loss: 0.35208848118782043 iteration:  1995 epoch:  4\n","Male Female Classification loss: 0.5893635749816895 iteration:  1995 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.9139952659606934 iteration:  2000 epoch:  4\n","LQ image reconstruction loss: 0.43836456537246704 iteration:  2000 epoch:  4\n","Same Different Classification loss: 0.4137873351573944 iteration:  2000 epoch:  4\n","Male Female Classification loss: 0.605704128742218 iteration:  2000 epoch:  4\n","----------------------------------------------------\n","Accuracy of SD: 82 %\n","Accuracy of MF: 71 %\n","Mean Accuracy of SD: 62 %\n","Mean Accuracy of MF: 63 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.7943379878997803 iteration:  2005 epoch:  4\n","LQ image reconstruction loss: 0.4898560047149658 iteration:  2005 epoch:  4\n","Same Different Classification loss: 0.4502633810043335 iteration:  2005 epoch:  4\n","Male Female Classification loss: 0.5896692872047424 iteration:  2005 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.2538087368011475 iteration:  2010 epoch:  4\n","LQ image reconstruction loss: 0.4420074224472046 iteration:  2010 epoch:  4\n","Same Different Classification loss: 0.4099535346031189 iteration:  2010 epoch:  4\n","Male Female Classification loss: 0.582318127155304 iteration:  2010 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.0681443214416504 iteration:  2015 epoch:  4\n","LQ image reconstruction loss: 0.4612559676170349 iteration:  2015 epoch:  4\n","Same Different Classification loss: 0.4256526231765747 iteration:  2015 epoch:  4\n","Male Female Classification loss: 0.6049109697341919 iteration:  2015 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.4026739597320557 iteration:  2020 epoch:  4\n","LQ image reconstruction loss: 0.3571718633174896 iteration:  2020 epoch:  4\n","Same Different Classification loss: 0.3630006015300751 iteration:  2020 epoch:  4\n","Male Female Classification loss: 0.654783308506012 iteration:  2020 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.0429325103759766 iteration:  2025 epoch:  4\n","LQ image reconstruction loss: 0.41232943534851074 iteration:  2025 epoch:  4\n","Same Different Classification loss: 0.4189094603061676 iteration:  2025 epoch:  4\n","Male Female Classification loss: 0.6343337893486023 iteration:  2025 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.7563860416412354 iteration:  2030 epoch:  4\n","LQ image reconstruction loss: 0.4649806022644043 iteration:  2030 epoch:  4\n","Same Different Classification loss: 0.43622061610221863 iteration:  2030 epoch:  4\n","Male Female Classification loss: 0.5984307527542114 iteration:  2030 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.1364710330963135 iteration:  2035 epoch:  4\n","LQ image reconstruction loss: 0.457363486289978 iteration:  2035 epoch:  4\n","Same Different Classification loss: 0.4186437726020813 iteration:  2035 epoch:  4\n","Male Female Classification loss: 0.620948076248169 iteration:  2035 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.1915836334228516 iteration:  2040 epoch:  4\n","LQ image reconstruction loss: 0.4136495590209961 iteration:  2040 epoch:  4\n","Same Different Classification loss: 0.36353805661201477 iteration:  2040 epoch:  4\n","Male Female Classification loss: 0.5345218777656555 iteration:  2040 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.708430290222168 iteration:  2045 epoch:  4\n","LQ image reconstruction loss: 0.4497268795967102 iteration:  2045 epoch:  4\n","Same Different Classification loss: 0.41291874647140503 iteration:  2045 epoch:  4\n","Male Female Classification loss: 0.5273399949073792 iteration:  2045 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.8721060752868652 iteration:  2050 epoch:  4\n","LQ image reconstruction loss: 0.48235446214675903 iteration:  2050 epoch:  4\n","Same Different Classification loss: 0.42172515392303467 iteration:  2050 epoch:  4\n","Male Female Classification loss: 0.5343713164329529 iteration:  2050 epoch:  4\n","----------------------------------------------------\n","Accuracy of SD: 80 %\n","Accuracy of MF: 76 %\n","Mean Accuracy of SD: 62 %\n","Mean Accuracy of MF: 63 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 1.9845398664474487 iteration:  2055 epoch:  4\n","LQ image reconstruction loss: 0.3594149351119995 iteration:  2055 epoch:  4\n","Same Different Classification loss: 0.3701893985271454 iteration:  2055 epoch:  4\n","Male Female Classification loss: 0.6671896576881409 iteration:  2055 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.4494497776031494 iteration:  2060 epoch:  4\n","LQ image reconstruction loss: 0.3988962769508362 iteration:  2060 epoch:  4\n","Same Different Classification loss: 0.3479553163051605 iteration:  2060 epoch:  4\n","Male Female Classification loss: 0.5652378797531128 iteration:  2060 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.57234525680542 iteration:  2065 epoch:  4\n","LQ image reconstruction loss: 0.43840453028678894 iteration:  2065 epoch:  4\n","Same Different Classification loss: 0.42438212037086487 iteration:  2065 epoch:  4\n","Male Female Classification loss: 0.5747315883636475 iteration:  2065 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.5678000450134277 iteration:  2070 epoch:  4\n","LQ image reconstruction loss: 0.4771351218223572 iteration:  2070 epoch:  4\n","Same Different Classification loss: 0.39296913146972656 iteration:  2070 epoch:  4\n","Male Female Classification loss: 0.4860151708126068 iteration:  2070 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 1.809500813484192 iteration:  2075 epoch:  4\n","LQ image reconstruction loss: 0.4464358687400818 iteration:  2075 epoch:  4\n","Same Different Classification loss: 0.4232015907764435 iteration:  2075 epoch:  4\n","Male Female Classification loss: 0.5651589035987854 iteration:  2075 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.829915761947632 iteration:  2080 epoch:  4\n","LQ image reconstruction loss: 0.35735827684402466 iteration:  2080 epoch:  4\n","Same Different Classification loss: 0.33269020915031433 iteration:  2080 epoch:  4\n","Male Female Classification loss: 0.5704338550567627 iteration:  2080 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.4332611560821533 iteration:  2085 epoch:  4\n","LQ image reconstruction loss: 0.4883366525173187 iteration:  2085 epoch:  4\n","Same Different Classification loss: 0.4859028160572052 iteration:  2085 epoch:  4\n","Male Female Classification loss: 0.6325710415840149 iteration:  2085 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 1.7503061294555664 iteration:  2090 epoch:  4\n","LQ image reconstruction loss: 0.40642639994621277 iteration:  2090 epoch:  4\n","Same Different Classification loss: 0.3636401891708374 iteration:  2090 epoch:  4\n","Male Female Classification loss: 0.5802608132362366 iteration:  2090 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.482119083404541 iteration:  2095 epoch:  4\n","LQ image reconstruction loss: 0.3752676248550415 iteration:  2095 epoch:  4\n","Same Different Classification loss: 0.35447102785110474 iteration:  2095 epoch:  4\n","Male Female Classification loss: 0.6110789775848389 iteration:  2095 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.3422820568084717 iteration:  2100 epoch:  4\n","LQ image reconstruction loss: 0.3780428171157837 iteration:  2100 epoch:  4\n","Same Different Classification loss: 0.3358911871910095 iteration:  2100 epoch:  4\n","Male Female Classification loss: 0.5830591320991516 iteration:  2100 epoch:  4\n","----------------------------------------------------\n","Accuracy of SD: 85 %\n","Accuracy of MF: 74 %\n","Mean Accuracy of SD: 62 %\n","Mean Accuracy of MF: 63 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.4872121810913086 iteration:  2105 epoch:  4\n","LQ image reconstruction loss: 0.42877310514450073 iteration:  2105 epoch:  4\n","Same Different Classification loss: 0.3709322214126587 iteration:  2105 epoch:  4\n","Male Female Classification loss: 0.5786930322647095 iteration:  2105 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.5402863025665283 iteration:  2110 epoch:  4\n","LQ image reconstruction loss: 0.5280346870422363 iteration:  2110 epoch:  4\n","Same Different Classification loss: 0.4967580735683441 iteration:  2110 epoch:  4\n","Male Female Classification loss: 0.5506340861320496 iteration:  2110 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.637756109237671 iteration:  2115 epoch:  4\n","LQ image reconstruction loss: 0.4113992750644684 iteration:  2115 epoch:  4\n","Same Different Classification loss: 0.3703535795211792 iteration:  2115 epoch:  4\n","Male Female Classification loss: 0.5584961771965027 iteration:  2115 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.770592212677002 iteration:  2120 epoch:  4\n","LQ image reconstruction loss: 0.4362964630126953 iteration:  2120 epoch:  4\n","Same Different Classification loss: 0.35933253169059753 iteration:  2120 epoch:  4\n","Male Female Classification loss: 0.5018754005432129 iteration:  2120 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.2661960124969482 iteration:  2125 epoch:  4\n","LQ image reconstruction loss: 0.3977385461330414 iteration:  2125 epoch:  4\n","Same Different Classification loss: 0.353989839553833 iteration:  2125 epoch:  4\n","Male Female Classification loss: 0.5882734656333923 iteration:  2125 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.323967218399048 iteration:  2130 epoch:  4\n","LQ image reconstruction loss: 0.4208395481109619 iteration:  2130 epoch:  4\n","Same Different Classification loss: 0.40769317746162415 iteration:  2130 epoch:  4\n","Male Female Classification loss: 0.6306672096252441 iteration:  2130 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.142698049545288 iteration:  2135 epoch:  4\n","LQ image reconstruction loss: 0.36974793672561646 iteration:  2135 epoch:  4\n","Same Different Classification loss: 0.3224612772464752 iteration:  2135 epoch:  4\n","Male Female Classification loss: 0.5606572031974792 iteration:  2135 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.3281428813934326 iteration:  2140 epoch:  4\n","LQ image reconstruction loss: 0.5789919495582581 iteration:  2140 epoch:  4\n","Same Different Classification loss: 0.5005972981452942 iteration:  2140 epoch:  4\n","Male Female Classification loss: 0.5710470676422119 iteration:  2140 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.7241909503936768 iteration:  2145 epoch:  4\n","LQ image reconstruction loss: 0.4190089702606201 iteration:  2145 epoch:  4\n","Same Different Classification loss: 0.4168046712875366 iteration:  2145 epoch:  4\n","Male Female Classification loss: 0.6070679426193237 iteration:  2145 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.4513816833496094 iteration:  2150 epoch:  4\n","LQ image reconstruction loss: 0.43216201663017273 iteration:  2150 epoch:  4\n","Same Different Classification loss: 0.4116905629634857 iteration:  2150 epoch:  4\n","Male Female Classification loss: 0.5840752124786377 iteration:  2150 epoch:  4\n","----------------------------------------------------\n","Accuracy of SD: 87 %\n","Accuracy of MF: 81 %\n","Mean Accuracy of SD: 62 %\n","Mean Accuracy of MF: 63 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.349625825881958 iteration:  2155 epoch:  4\n","LQ image reconstruction loss: 0.45099276304244995 iteration:  2155 epoch:  4\n","Same Different Classification loss: 0.44490447640419006 iteration:  2155 epoch:  4\n","Male Female Classification loss: 0.601963222026825 iteration:  2155 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.044766426086426 iteration:  2160 epoch:  4\n","LQ image reconstruction loss: 0.36477959156036377 iteration:  2160 epoch:  4\n","Same Different Classification loss: 0.3418726623058319 iteration:  2160 epoch:  4\n","Male Female Classification loss: 0.5897189974784851 iteration:  2160 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.489147424697876 iteration:  2165 epoch:  4\n","LQ image reconstruction loss: 0.5086134672164917 iteration:  2165 epoch:  4\n","Same Different Classification loss: 0.45605307817459106 iteration:  2165 epoch:  4\n","Male Female Classification loss: 0.5834965705871582 iteration:  2165 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.1433889865875244 iteration:  2170 epoch:  4\n","LQ image reconstruction loss: 0.44034063816070557 iteration:  2170 epoch:  4\n","Same Different Classification loss: 0.38287413120269775 iteration:  2170 epoch:  4\n","Male Female Classification loss: 0.5167242288589478 iteration:  2170 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.450690984725952 iteration:  2175 epoch:  4\n","LQ image reconstruction loss: 0.4199722409248352 iteration:  2175 epoch:  4\n","Same Different Classification loss: 0.3901670575141907 iteration:  2175 epoch:  4\n","Male Female Classification loss: 0.5486201643943787 iteration:  2175 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.566592216491699 iteration:  2180 epoch:  4\n","LQ image reconstruction loss: 0.4621276259422302 iteration:  2180 epoch:  4\n","Same Different Classification loss: 0.4376833438873291 iteration:  2180 epoch:  4\n","Male Female Classification loss: 0.5563935041427612 iteration:  2180 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.555419445037842 iteration:  2185 epoch:  4\n","LQ image reconstruction loss: 0.41481471061706543 iteration:  2185 epoch:  4\n","Same Different Classification loss: 0.3847995400428772 iteration:  2185 epoch:  4\n","Male Female Classification loss: 0.547644317150116 iteration:  2185 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 1.9568296670913696 iteration:  2190 epoch:  4\n","LQ image reconstruction loss: 0.356516569852829 iteration:  2190 epoch:  4\n","Same Different Classification loss: 0.3353467583656311 iteration:  2190 epoch:  4\n","Male Female Classification loss: 0.5925414562225342 iteration:  2190 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 1.808327317237854 iteration:  2195 epoch:  4\n","LQ image reconstruction loss: 0.38617390394210815 iteration:  2195 epoch:  4\n","Same Different Classification loss: 0.35425853729248047 iteration:  2195 epoch:  4\n","Male Female Classification loss: 0.6191213726997375 iteration:  2195 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.085258960723877 iteration:  2200 epoch:  4\n","LQ image reconstruction loss: 0.4308718144893646 iteration:  2200 epoch:  4\n","Same Different Classification loss: 0.39489519596099854 iteration:  2200 epoch:  4\n","Male Female Classification loss: 0.5755866169929504 iteration:  2200 epoch:  4\n","----------------------------------------------------\n","Accuracy of SD: 83 %\n","Accuracy of MF: 76 %\n","Mean Accuracy of SD: 63 %\n","Mean Accuracy of MF: 63 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.3264670372009277 iteration:  2205 epoch:  4\n","LQ image reconstruction loss: 0.39713239669799805 iteration:  2205 epoch:  4\n","Same Different Classification loss: 0.3767598271369934 iteration:  2205 epoch:  4\n","Male Female Classification loss: 0.5506623983383179 iteration:  2205 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.783280849456787 iteration:  2210 epoch:  4\n","LQ image reconstruction loss: 0.5038046836853027 iteration:  2210 epoch:  4\n","Same Different Classification loss: 0.45659640431404114 iteration:  2210 epoch:  4\n","Male Female Classification loss: 0.5472062826156616 iteration:  2210 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.05625057220459 iteration:  2215 epoch:  4\n","LQ image reconstruction loss: 0.318650484085083 iteration:  2215 epoch:  4\n","Same Different Classification loss: 0.3065008819103241 iteration:  2215 epoch:  4\n","Male Female Classification loss: 0.5635210275650024 iteration:  2215 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.1305789947509766 iteration:  2220 epoch:  4\n","LQ image reconstruction loss: 0.45949268341064453 iteration:  2220 epoch:  4\n","Same Different Classification loss: 0.40551263093948364 iteration:  2220 epoch:  4\n","Male Female Classification loss: 0.5073829889297485 iteration:  2220 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.0977120399475098 iteration:  2225 epoch:  4\n","LQ image reconstruction loss: 0.4071107506752014 iteration:  2225 epoch:  4\n","Same Different Classification loss: 0.3602868616580963 iteration:  2225 epoch:  4\n","Male Female Classification loss: 0.5838196873664856 iteration:  2225 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.0326528549194336 iteration:  2230 epoch:  4\n","LQ image reconstruction loss: 0.3696213960647583 iteration:  2230 epoch:  4\n","Same Different Classification loss: 0.325763463973999 iteration:  2230 epoch:  4\n","Male Female Classification loss: 0.5528357625007629 iteration:  2230 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.2649471759796143 iteration:  2235 epoch:  4\n","LQ image reconstruction loss: 0.4453318417072296 iteration:  2235 epoch:  4\n","Same Different Classification loss: 0.39912837743759155 iteration:  2235 epoch:  4\n","Male Female Classification loss: 0.611850380897522 iteration:  2235 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.0971689224243164 iteration:  2240 epoch:  4\n","LQ image reconstruction loss: 0.34035569429397583 iteration:  2240 epoch:  4\n","Same Different Classification loss: 0.3296741843223572 iteration:  2240 epoch:  4\n","Male Female Classification loss: 0.5763891339302063 iteration:  2240 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.5690371990203857 iteration:  2245 epoch:  4\n","LQ image reconstruction loss: 0.41728466749191284 iteration:  2245 epoch:  4\n","Same Different Classification loss: 0.36408448219299316 iteration:  2245 epoch:  4\n","Male Female Classification loss: 0.5701906681060791 iteration:  2245 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.2970712184906006 iteration:  2250 epoch:  4\n","LQ image reconstruction loss: 0.37261420488357544 iteration:  2250 epoch:  4\n","Same Different Classification loss: 0.3392125070095062 iteration:  2250 epoch:  4\n","Male Female Classification loss: 0.5835439562797546 iteration:  2250 epoch:  4\n","----------------------------------------------------\n","Accuracy of SD: 89 %\n","Accuracy of MF: 84 %\n","Mean Accuracy of SD: 63 %\n","Mean Accuracy of MF: 63 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.060070753097534 iteration:  2255 epoch:  4\n","LQ image reconstruction loss: 0.3839191198348999 iteration:  2255 epoch:  4\n","Same Different Classification loss: 0.34538716077804565 iteration:  2255 epoch:  4\n","Male Female Classification loss: 0.5976766347885132 iteration:  2255 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 1.789859652519226 iteration:  2260 epoch:  4\n","LQ image reconstruction loss: 0.3158738613128662 iteration:  2260 epoch:  4\n","Same Different Classification loss: 0.29633864760398865 iteration:  2260 epoch:  4\n","Male Female Classification loss: 0.5840368866920471 iteration:  2260 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 1.8991552591323853 iteration:  2265 epoch:  4\n","LQ image reconstruction loss: 0.3508645296096802 iteration:  2265 epoch:  4\n","Same Different Classification loss: 0.32881778478622437 iteration:  2265 epoch:  4\n","Male Female Classification loss: 0.594587504863739 iteration:  2265 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 1.8068619966506958 iteration:  2270 epoch:  4\n","LQ image reconstruction loss: 0.375640332698822 iteration:  2270 epoch:  4\n","Same Different Classification loss: 0.3511998653411865 iteration:  2270 epoch:  4\n","Male Female Classification loss: 0.5911789536476135 iteration:  2270 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.2860054969787598 iteration:  2275 epoch:  4\n","LQ image reconstruction loss: 0.5006279945373535 iteration:  2275 epoch:  4\n","Same Different Classification loss: 0.4338536858558655 iteration:  2275 epoch:  4\n","Male Female Classification loss: 0.5563801527023315 iteration:  2275 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 1.780871868133545 iteration:  2280 epoch:  4\n","LQ image reconstruction loss: 0.36071276664733887 iteration:  2280 epoch:  4\n","Same Different Classification loss: 0.3300230801105499 iteration:  2280 epoch:  4\n","Male Female Classification loss: 0.5939226150512695 iteration:  2280 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.09136962890625 iteration:  2285 epoch:  4\n","LQ image reconstruction loss: 0.4603227972984314 iteration:  2285 epoch:  4\n","Same Different Classification loss: 0.3773321807384491 iteration:  2285 epoch:  4\n","Male Female Classification loss: 0.47262394428253174 iteration:  2285 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.0118696689605713 iteration:  2290 epoch:  4\n","LQ image reconstruction loss: 0.43146324157714844 iteration:  2290 epoch:  4\n","Same Different Classification loss: 0.3802429139614105 iteration:  2290 epoch:  4\n","Male Female Classification loss: 0.565333902835846 iteration:  2290 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.2236289978027344 iteration:  2295 epoch:  4\n","LQ image reconstruction loss: 0.3221435546875 iteration:  2295 epoch:  4\n","Same Different Classification loss: 0.29659098386764526 iteration:  2295 epoch:  4\n","Male Female Classification loss: 0.520874559879303 iteration:  2295 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.2192978858947754 iteration:  2300 epoch:  4\n","LQ image reconstruction loss: 0.3827406167984009 iteration:  2300 epoch:  4\n","Same Different Classification loss: 0.34453609585762024 iteration:  2300 epoch:  4\n","Male Female Classification loss: 0.5671855211257935 iteration:  2300 epoch:  4\n","----------------------------------------------------\n","Accuracy of SD: 87 %\n","Accuracy of MF: 64 %\n","Mean Accuracy of SD: 63 %\n","Mean Accuracy of MF: 63 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 1.7443459033966064 iteration:  2305 epoch:  4\n","LQ image reconstruction loss: 0.4031795263290405 iteration:  2305 epoch:  4\n","Same Different Classification loss: 0.36583051085472107 iteration:  2305 epoch:  4\n","Male Female Classification loss: 0.572171151638031 iteration:  2305 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.0539841651916504 iteration:  2310 epoch:  4\n","LQ image reconstruction loss: 0.4091651439666748 iteration:  2310 epoch:  4\n","Same Different Classification loss: 0.3553958535194397 iteration:  2310 epoch:  4\n","Male Female Classification loss: 0.534188985824585 iteration:  2310 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.231142997741699 iteration:  2315 epoch:  4\n","LQ image reconstruction loss: 0.48937487602233887 iteration:  2315 epoch:  4\n","Same Different Classification loss: 0.4579445719718933 iteration:  2315 epoch:  4\n","Male Female Classification loss: 0.5590861439704895 iteration:  2315 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.0016138553619385 iteration:  2320 epoch:  4\n","LQ image reconstruction loss: 0.38458895683288574 iteration:  2320 epoch:  4\n","Same Different Classification loss: 0.33035343885421753 iteration:  2320 epoch:  4\n","Male Female Classification loss: 0.500030517578125 iteration:  2320 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.207031726837158 iteration:  2325 epoch:  4\n","LQ image reconstruction loss: 0.39835429191589355 iteration:  2325 epoch:  4\n","Same Different Classification loss: 0.347245454788208 iteration:  2325 epoch:  4\n","Male Female Classification loss: 0.509244978427887 iteration:  2325 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.5112483501434326 iteration:  2330 epoch:  4\n","LQ image reconstruction loss: 0.46160727739334106 iteration:  2330 epoch:  4\n","Same Different Classification loss: 0.40415528416633606 iteration:  2330 epoch:  4\n","Male Female Classification loss: 0.516840934753418 iteration:  2330 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.255103588104248 iteration:  2335 epoch:  4\n","LQ image reconstruction loss: 0.36350858211517334 iteration:  2335 epoch:  4\n","Same Different Classification loss: 0.3380123972892761 iteration:  2335 epoch:  4\n","Male Female Classification loss: 0.6156602501869202 iteration:  2335 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.2256548404693604 iteration:  2340 epoch:  4\n","LQ image reconstruction loss: 0.39715585112571716 iteration:  2340 epoch:  4\n","Same Different Classification loss: 0.3854930102825165 iteration:  2340 epoch:  4\n","Male Female Classification loss: 0.5722669363021851 iteration:  2340 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.4516210556030273 iteration:  2345 epoch:  4\n","LQ image reconstruction loss: 0.39724433422088623 iteration:  2345 epoch:  4\n","Same Different Classification loss: 0.3431001603603363 iteration:  2345 epoch:  4\n","Male Female Classification loss: 0.551164448261261 iteration:  2345 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.125027894973755 iteration:  2350 epoch:  4\n","LQ image reconstruction loss: 0.4105756878852844 iteration:  2350 epoch:  4\n","Same Different Classification loss: 0.36150360107421875 iteration:  2350 epoch:  4\n","Male Female Classification loss: 0.5544416904449463 iteration:  2350 epoch:  4\n","----------------------------------------------------\n","Accuracy of SD: 87 %\n","Accuracy of MF: 74 %\n","Mean Accuracy of SD: 63 %\n","Mean Accuracy of MF: 63 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.565568447113037 iteration:  2355 epoch:  4\n","LQ image reconstruction loss: 0.44267821311950684 iteration:  2355 epoch:  4\n","Same Different Classification loss: 0.40759342908859253 iteration:  2355 epoch:  4\n","Male Female Classification loss: 0.552390456199646 iteration:  2355 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.708359956741333 iteration:  2360 epoch:  4\n","LQ image reconstruction loss: 0.43381771445274353 iteration:  2360 epoch:  4\n","Same Different Classification loss: 0.4188491106033325 iteration:  2360 epoch:  4\n","Male Female Classification loss: 0.6454271674156189 iteration:  2360 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.9699625968933105 iteration:  2365 epoch:  4\n","LQ image reconstruction loss: 0.4513295590877533 iteration:  2365 epoch:  4\n","Same Different Classification loss: 0.4319431781768799 iteration:  2365 epoch:  4\n","Male Female Classification loss: 0.6085917949676514 iteration:  2365 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.6752803325653076 iteration:  2370 epoch:  4\n","LQ image reconstruction loss: 0.4808141589164734 iteration:  2370 epoch:  4\n","Same Different Classification loss: 0.4188011884689331 iteration:  2370 epoch:  4\n","Male Female Classification loss: 0.5656679272651672 iteration:  2370 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 3.0228443145751953 iteration:  2375 epoch:  4\n","LQ image reconstruction loss: 0.5164945125579834 iteration:  2375 epoch:  4\n","Same Different Classification loss: 0.4946185052394867 iteration:  2375 epoch:  4\n","Male Female Classification loss: 0.5718823671340942 iteration:  2375 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.790557861328125 iteration:  2380 epoch:  4\n","LQ image reconstruction loss: 0.385237455368042 iteration:  2380 epoch:  4\n","Same Different Classification loss: 0.38721829652786255 iteration:  2380 epoch:  4\n","Male Female Classification loss: 0.6279087066650391 iteration:  2380 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.6849985122680664 iteration:  2385 epoch:  4\n","LQ image reconstruction loss: 0.3655661344528198 iteration:  2385 epoch:  4\n","Same Different Classification loss: 0.3613067865371704 iteration:  2385 epoch:  4\n","Male Female Classification loss: 0.6143688559532166 iteration:  2385 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.28741192817688 iteration:  2390 epoch:  4\n","LQ image reconstruction loss: 0.37312716245651245 iteration:  2390 epoch:  4\n","Same Different Classification loss: 0.33866721391677856 iteration:  2390 epoch:  4\n","Male Female Classification loss: 0.5728548169136047 iteration:  2390 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.0671112537384033 iteration:  2395 epoch:  4\n","LQ image reconstruction loss: 0.4448733329772949 iteration:  2395 epoch:  4\n","Same Different Classification loss: 0.46288707852363586 iteration:  2395 epoch:  4\n","Male Female Classification loss: 0.6160371899604797 iteration:  2395 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.2500455379486084 iteration:  2400 epoch:  4\n","LQ image reconstruction loss: 0.4390365779399872 iteration:  2400 epoch:  4\n","Same Different Classification loss: 0.4185692071914673 iteration:  2400 epoch:  4\n","Male Female Classification loss: 0.5483583211898804 iteration:  2400 epoch:  4\n","----------------------------------------------------\n","Accuracy of SD: 81 %\n","Accuracy of MF: 74 %\n","Mean Accuracy of SD: 63 %\n","Mean Accuracy of MF: 63 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.678784132003784 iteration:  2405 epoch:  4\n","LQ image reconstruction loss: 0.3289814591407776 iteration:  2405 epoch:  4\n","Same Different Classification loss: 0.30478614568710327 iteration:  2405 epoch:  4\n","Male Female Classification loss: 0.6003456115722656 iteration:  2405 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.1274573802948 iteration:  2410 epoch:  4\n","LQ image reconstruction loss: 0.36447328329086304 iteration:  2410 epoch:  4\n","Same Different Classification loss: 0.3478245437145233 iteration:  2410 epoch:  4\n","Male Female Classification loss: 0.5385341048240662 iteration:  2410 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.59763503074646 iteration:  2415 epoch:  4\n","LQ image reconstruction loss: 0.4154680669307709 iteration:  2415 epoch:  4\n","Same Different Classification loss: 0.38549214601516724 iteration:  2415 epoch:  4\n","Male Female Classification loss: 0.5614108443260193 iteration:  2415 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.1951119899749756 iteration:  2420 epoch:  4\n","LQ image reconstruction loss: 0.34882867336273193 iteration:  2420 epoch:  4\n","Same Different Classification loss: 0.32953986525535583 iteration:  2420 epoch:  4\n","Male Female Classification loss: 0.5494880080223083 iteration:  2420 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.670722723007202 iteration:  2425 epoch:  4\n","LQ image reconstruction loss: 0.3872734308242798 iteration:  2425 epoch:  4\n","Same Different Classification loss: 0.3721112310886383 iteration:  2425 epoch:  4\n","Male Female Classification loss: 0.5649728775024414 iteration:  2425 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 1.9312660694122314 iteration:  2430 epoch:  4\n","LQ image reconstruction loss: 0.44725626707077026 iteration:  2430 epoch:  4\n","Same Different Classification loss: 0.41793063282966614 iteration:  2430 epoch:  4\n","Male Female Classification loss: 0.5731440782546997 iteration:  2430 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.130169153213501 iteration:  2435 epoch:  4\n","LQ image reconstruction loss: 0.40775448083877563 iteration:  2435 epoch:  4\n","Same Different Classification loss: 0.33949267864227295 iteration:  2435 epoch:  4\n","Male Female Classification loss: 0.468171089887619 iteration:  2435 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.090312957763672 iteration:  2440 epoch:  4\n","LQ image reconstruction loss: 0.35430750250816345 iteration:  2440 epoch:  4\n","Same Different Classification loss: 0.3228517770767212 iteration:  2440 epoch:  4\n","Male Female Classification loss: 0.5914733409881592 iteration:  2440 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.156191825866699 iteration:  2445 epoch:  4\n","LQ image reconstruction loss: 0.39262592792510986 iteration:  2445 epoch:  4\n","Same Different Classification loss: 0.36295968294143677 iteration:  2445 epoch:  4\n","Male Female Classification loss: 0.5476469993591309 iteration:  2445 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.3060243129730225 iteration:  2450 epoch:  4\n","LQ image reconstruction loss: 0.3771958351135254 iteration:  2450 epoch:  4\n","Same Different Classification loss: 0.3356844186782837 iteration:  2450 epoch:  4\n","Male Female Classification loss: 0.5604285001754761 iteration:  2450 epoch:  4\n","----------------------------------------------------\n","Accuracy of SD: 84 %\n","Accuracy of MF: 75 %\n","Mean Accuracy of SD: 63 %\n","Mean Accuracy of MF: 63 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 1.9061554670333862 iteration:  2455 epoch:  4\n","LQ image reconstruction loss: 0.4750116169452667 iteration:  2455 epoch:  4\n","Same Different Classification loss: 0.42783665657043457 iteration:  2455 epoch:  4\n","Male Female Classification loss: 0.5457607507705688 iteration:  2455 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.2500290870666504 iteration:  2460 epoch:  4\n","LQ image reconstruction loss: 0.47009724378585815 iteration:  2460 epoch:  4\n","Same Different Classification loss: 0.42817869782447815 iteration:  2460 epoch:  4\n","Male Female Classification loss: 0.567569375038147 iteration:  2460 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 1.7948241233825684 iteration:  2465 epoch:  4\n","LQ image reconstruction loss: 0.42747437953948975 iteration:  2465 epoch:  4\n","Same Different Classification loss: 0.3742205500602722 iteration:  2465 epoch:  4\n","Male Female Classification loss: 0.5984413623809814 iteration:  2465 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 1.8142116069793701 iteration:  2470 epoch:  4\n","LQ image reconstruction loss: 0.43443331122398376 iteration:  2470 epoch:  4\n","Same Different Classification loss: 0.3602757751941681 iteration:  2470 epoch:  4\n","Male Female Classification loss: 0.5410862565040588 iteration:  2470 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.038760185241699 iteration:  2475 epoch:  4\n","LQ image reconstruction loss: 0.3670087456703186 iteration:  2475 epoch:  4\n","Same Different Classification loss: 0.3217938244342804 iteration:  2475 epoch:  4\n","Male Female Classification loss: 0.48952171206474304 iteration:  2475 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 1.7623180150985718 iteration:  2480 epoch:  4\n","LQ image reconstruction loss: 0.4918290376663208 iteration:  2480 epoch:  4\n","Same Different Classification loss: 0.44451481103897095 iteration:  2480 epoch:  4\n","Male Female Classification loss: 0.5861320495605469 iteration:  2480 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 1.8675899505615234 iteration:  2485 epoch:  4\n","LQ image reconstruction loss: 0.3553844690322876 iteration:  2485 epoch:  4\n","Same Different Classification loss: 0.35048821568489075 iteration:  2485 epoch:  4\n","Male Female Classification loss: 0.6650733947753906 iteration:  2485 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.0993359088897705 iteration:  2490 epoch:  4\n","LQ image reconstruction loss: 0.37380096316337585 iteration:  2490 epoch:  4\n","Same Different Classification loss: 0.38773223757743835 iteration:  2490 epoch:  4\n","Male Female Classification loss: 0.6169346570968628 iteration:  2490 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 4.157406330108643 iteration:  2495 epoch:  4\n","LQ image reconstruction loss: 0.5310190320014954 iteration:  2495 epoch:  4\n","Same Different Classification loss: 0.47022873163223267 iteration:  2495 epoch:  4\n","Male Female Classification loss: 0.49975135922431946 iteration:  2495 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.049281120300293 iteration:  2500 epoch:  4\n","LQ image reconstruction loss: 0.37925952672958374 iteration:  2500 epoch:  4\n","Same Different Classification loss: 0.33452263474464417 iteration:  2500 epoch:  4\n","Male Female Classification loss: 0.55354905128479 iteration:  2500 epoch:  4\n","----------------------------------------------------\n","Accuracy of SD: 85 %\n","Accuracy of MF: 59 %\n","Mean Accuracy of SD: 63 %\n","Mean Accuracy of MF: 63 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.4240305423736572 iteration:  2505 epoch:  4\n","LQ image reconstruction loss: 0.37639424204826355 iteration:  2505 epoch:  4\n","Same Different Classification loss: 0.3692091405391693 iteration:  2505 epoch:  4\n","Male Female Classification loss: 0.6604001522064209 iteration:  2505 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.827155113220215 iteration:  2510 epoch:  4\n","LQ image reconstruction loss: 0.44414055347442627 iteration:  2510 epoch:  4\n","Same Different Classification loss: 0.4296889901161194 iteration:  2510 epoch:  4\n","Male Female Classification loss: 0.6281208992004395 iteration:  2510 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.5152602195739746 iteration:  2515 epoch:  4\n","LQ image reconstruction loss: 0.3539210855960846 iteration:  2515 epoch:  4\n","Same Different Classification loss: 0.34908244013786316 iteration:  2515 epoch:  4\n","Male Female Classification loss: 0.5894500613212585 iteration:  2515 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.5564568042755127 iteration:  2520 epoch:  4\n","LQ image reconstruction loss: 0.3887430429458618 iteration:  2520 epoch:  4\n","Same Different Classification loss: 0.38653573393821716 iteration:  2520 epoch:  4\n","Male Female Classification loss: 0.6042435169219971 iteration:  2520 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.3797340393066406 iteration:  2525 epoch:  4\n","LQ image reconstruction loss: 0.35215944051742554 iteration:  2525 epoch:  4\n","Same Different Classification loss: 0.34584277868270874 iteration:  2525 epoch:  4\n","Male Female Classification loss: 0.6462898850440979 iteration:  2525 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.180164337158203 iteration:  2530 epoch:  4\n","LQ image reconstruction loss: 0.37122005224227905 iteration:  2530 epoch:  4\n","Same Different Classification loss: 0.3323524594306946 iteration:  2530 epoch:  4\n","Male Female Classification loss: 0.579769492149353 iteration:  2530 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.662951946258545 iteration:  2535 epoch:  4\n","LQ image reconstruction loss: 0.4018452763557434 iteration:  2535 epoch:  4\n","Same Different Classification loss: 0.36002904176712036 iteration:  2535 epoch:  4\n","Male Female Classification loss: 0.5574472546577454 iteration:  2535 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.6914994716644287 iteration:  2540 epoch:  4\n","LQ image reconstruction loss: 0.35719603300094604 iteration:  2540 epoch:  4\n","Same Different Classification loss: 0.3473680913448334 iteration:  2540 epoch:  4\n","Male Female Classification loss: 0.5763517618179321 iteration:  2540 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.0505552291870117 iteration:  2545 epoch:  4\n","LQ image reconstruction loss: 0.39829128980636597 iteration:  2545 epoch:  4\n","Same Different Classification loss: 0.3595144748687744 iteration:  2545 epoch:  4\n","Male Female Classification loss: 0.5777838230133057 iteration:  2545 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.444359302520752 iteration:  2550 epoch:  4\n","LQ image reconstruction loss: 0.40648317337036133 iteration:  2550 epoch:  4\n","Same Different Classification loss: 0.37893569469451904 iteration:  2550 epoch:  4\n","Male Female Classification loss: 0.5636464357376099 iteration:  2550 epoch:  4\n","----------------------------------------------------\n","Accuracy of SD: 91 %\n","Accuracy of MF: 74 %\n","Mean Accuracy of SD: 63 %\n","Mean Accuracy of MF: 63 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 1.8150956630706787 iteration:  2555 epoch:  4\n","LQ image reconstruction loss: 0.42794397473335266 iteration:  2555 epoch:  4\n","Same Different Classification loss: 0.4081799387931824 iteration:  2555 epoch:  4\n","Male Female Classification loss: 0.641785204410553 iteration:  2555 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.047757863998413 iteration:  2560 epoch:  4\n","LQ image reconstruction loss: 0.3133239150047302 iteration:  2560 epoch:  4\n","Same Different Classification loss: 0.30734312534332275 iteration:  2560 epoch:  4\n","Male Female Classification loss: 0.6023069620132446 iteration:  2560 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.8958723545074463 iteration:  2565 epoch:  4\n","LQ image reconstruction loss: 0.4160858988761902 iteration:  2565 epoch:  4\n","Same Different Classification loss: 0.3972523808479309 iteration:  2565 epoch:  4\n","Male Female Classification loss: 0.5710413455963135 iteration:  2565 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.594296455383301 iteration:  2570 epoch:  4\n","LQ image reconstruction loss: 0.3702937960624695 iteration:  2570 epoch:  4\n","Same Different Classification loss: 0.36746451258659363 iteration:  2570 epoch:  4\n","Male Female Classification loss: 0.5848772525787354 iteration:  2570 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.524510383605957 iteration:  2575 epoch:  4\n","LQ image reconstruction loss: 0.3260155916213989 iteration:  2575 epoch:  4\n","Same Different Classification loss: 0.3217611610889435 iteration:  2575 epoch:  4\n","Male Female Classification loss: 0.5726569890975952 iteration:  2575 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.990461826324463 iteration:  2580 epoch:  4\n","LQ image reconstruction loss: 0.45996272563934326 iteration:  2580 epoch:  4\n","Same Different Classification loss: 0.43571093678474426 iteration:  2580 epoch:  4\n","Male Female Classification loss: 0.5915588140487671 iteration:  2580 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.4748144149780273 iteration:  2585 epoch:  4\n","LQ image reconstruction loss: 0.35591909289360046 iteration:  2585 epoch:  4\n","Same Different Classification loss: 0.3392305076122284 iteration:  2585 epoch:  4\n","Male Female Classification loss: 0.5721122622489929 iteration:  2585 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 1.9117556810379028 iteration:  2590 epoch:  4\n","LQ image reconstruction loss: 0.3586745858192444 iteration:  2590 epoch:  4\n","Same Different Classification loss: 0.3480747640132904 iteration:  2590 epoch:  4\n","Male Female Classification loss: 0.6170843243598938 iteration:  2590 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 1.6537156105041504 iteration:  2595 epoch:  4\n","LQ image reconstruction loss: 0.360420823097229 iteration:  2595 epoch:  4\n","Same Different Classification loss: 0.32887402176856995 iteration:  2595 epoch:  4\n","Male Female Classification loss: 0.547858715057373 iteration:  2595 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.2011730670928955 iteration:  2600 epoch:  4\n","LQ image reconstruction loss: 0.44906681776046753 iteration:  2600 epoch:  4\n","Same Different Classification loss: 0.3938811123371124 iteration:  2600 epoch:  4\n","Male Female Classification loss: 0.5432490706443787 iteration:  2600 epoch:  4\n","----------------------------------------------------\n","Accuracy of SD: 85 %\n","Accuracy of MF: 78 %\n","Mean Accuracy of SD: 63 %\n","Mean Accuracy of MF: 63 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.4418888092041016 iteration:  2605 epoch:  4\n","LQ image reconstruction loss: 0.3345233201980591 iteration:  2605 epoch:  4\n","Same Different Classification loss: 0.308369517326355 iteration:  2605 epoch:  4\n","Male Female Classification loss: 0.5635878443717957 iteration:  2605 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.6164917945861816 iteration:  2610 epoch:  4\n","LQ image reconstruction loss: 0.44986188411712646 iteration:  2610 epoch:  4\n","Same Different Classification loss: 0.3929319381713867 iteration:  2610 epoch:  4\n","Male Female Classification loss: 0.5957125425338745 iteration:  2610 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.1646881103515625 iteration:  2615 epoch:  4\n","LQ image reconstruction loss: 0.34267446398735046 iteration:  2615 epoch:  4\n","Same Different Classification loss: 0.30315861105918884 iteration:  2615 epoch:  4\n","Male Female Classification loss: 0.5740476250648499 iteration:  2615 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 1.848117470741272 iteration:  2620 epoch:  4\n","LQ image reconstruction loss: 0.3878961205482483 iteration:  2620 epoch:  4\n","Same Different Classification loss: 0.36607933044433594 iteration:  2620 epoch:  4\n","Male Female Classification loss: 0.5968143343925476 iteration:  2620 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.283118724822998 iteration:  2625 epoch:  4\n","LQ image reconstruction loss: 0.39683806896209717 iteration:  2625 epoch:  4\n","Same Different Classification loss: 0.3691343665122986 iteration:  2625 epoch:  4\n","Male Female Classification loss: 0.5539991855621338 iteration:  2625 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.4463255405426025 iteration:  2630 epoch:  4\n","LQ image reconstruction loss: 0.3914759159088135 iteration:  2630 epoch:  4\n","Same Different Classification loss: 0.3556361496448517 iteration:  2630 epoch:  4\n","Male Female Classification loss: 0.5782266855239868 iteration:  2630 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.1789658069610596 iteration:  2635 epoch:  4\n","LQ image reconstruction loss: 0.3486166000366211 iteration:  2635 epoch:  4\n","Same Different Classification loss: 0.32125377655029297 iteration:  2635 epoch:  4\n","Male Female Classification loss: 0.5651321411132812 iteration:  2635 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.872499465942383 iteration:  2640 epoch:  4\n","LQ image reconstruction loss: 0.5022785663604736 iteration:  2640 epoch:  4\n","Same Different Classification loss: 0.4273184537887573 iteration:  2640 epoch:  4\n","Male Female Classification loss: 0.46014413237571716 iteration:  2640 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.0169060230255127 iteration:  2645 epoch:  4\n","LQ image reconstruction loss: 0.4504542350769043 iteration:  2645 epoch:  4\n","Same Different Classification loss: 0.4000649154186249 iteration:  2645 epoch:  4\n","Male Female Classification loss: 0.5679526329040527 iteration:  2645 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 1.9622843265533447 iteration:  2650 epoch:  4\n","LQ image reconstruction loss: 0.37870723009109497 iteration:  2650 epoch:  4\n","Same Different Classification loss: 0.31184113025665283 iteration:  2650 epoch:  4\n","Male Female Classification loss: 0.49849820137023926 iteration:  2650 epoch:  4\n","----------------------------------------------------\n","Accuracy of SD: 83 %\n","Accuracy of MF: 79 %\n","Mean Accuracy of SD: 63 %\n","Mean Accuracy of MF: 63 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 1.8972536325454712 iteration:  2655 epoch:  4\n","LQ image reconstruction loss: 0.35300254821777344 iteration:  2655 epoch:  4\n","Same Different Classification loss: 0.33105987310409546 iteration:  2655 epoch:  4\n","Male Female Classification loss: 0.5883499383926392 iteration:  2655 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 1.8177683353424072 iteration:  2660 epoch:  4\n","LQ image reconstruction loss: 0.4453141391277313 iteration:  2660 epoch:  4\n","Same Different Classification loss: 0.4288461208343506 iteration:  2660 epoch:  4\n","Male Female Classification loss: 0.5280352234840393 iteration:  2660 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.0688514709472656 iteration:  2665 epoch:  4\n","LQ image reconstruction loss: 0.39251816272735596 iteration:  2665 epoch:  4\n","Same Different Classification loss: 0.36224955320358276 iteration:  2665 epoch:  4\n","Male Female Classification loss: 0.5290197730064392 iteration:  2665 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.184359550476074 iteration:  2670 epoch:  4\n","LQ image reconstruction loss: 0.391059935092926 iteration:  2670 epoch:  4\n","Same Different Classification loss: 0.3813653588294983 iteration:  2670 epoch:  4\n","Male Female Classification loss: 0.5242182612419128 iteration:  2670 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.188457489013672 iteration:  2675 epoch:  4\n","LQ image reconstruction loss: 0.34539973735809326 iteration:  2675 epoch:  4\n","Same Different Classification loss: 0.32988807559013367 iteration:  2675 epoch:  4\n","Male Female Classification loss: 0.6181734800338745 iteration:  2675 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 1.8327218294143677 iteration:  2680 epoch:  4\n","LQ image reconstruction loss: 0.36539921164512634 iteration:  2680 epoch:  4\n","Same Different Classification loss: 0.3577437698841095 iteration:  2680 epoch:  4\n","Male Female Classification loss: 0.583965539932251 iteration:  2680 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.1723341941833496 iteration:  2685 epoch:  4\n","LQ image reconstruction loss: 0.4195791184902191 iteration:  2685 epoch:  4\n","Same Different Classification loss: 0.41137629747390747 iteration:  2685 epoch:  4\n","Male Female Classification loss: 0.6033501625061035 iteration:  2685 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 1.8826243877410889 iteration:  2690 epoch:  4\n","LQ image reconstruction loss: 0.3283001780509949 iteration:  2690 epoch:  4\n","Same Different Classification loss: 0.29941633343696594 iteration:  2690 epoch:  4\n","Male Female Classification loss: 0.5544143915176392 iteration:  2690 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 1.702897071838379 iteration:  2695 epoch:  4\n","LQ image reconstruction loss: 0.356173574924469 iteration:  2695 epoch:  4\n","Same Different Classification loss: 0.33794283866882324 iteration:  2695 epoch:  4\n","Male Female Classification loss: 0.5664184093475342 iteration:  2695 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.4386606216430664 iteration:  2700 epoch:  4\n","LQ image reconstruction loss: 0.45880594849586487 iteration:  2700 epoch:  4\n","Same Different Classification loss: 0.4680291712284088 iteration:  2700 epoch:  4\n","Male Female Classification loss: 0.6018755435943604 iteration:  2700 epoch:  4\n","----------------------------------------------------\n","Accuracy of SD: 91 %\n","Accuracy of MF: 68 %\n","Mean Accuracy of SD: 63 %\n","Mean Accuracy of MF: 63 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.024055242538452 iteration:  2705 epoch:  4\n","LQ image reconstruction loss: 0.37458643317222595 iteration:  2705 epoch:  4\n","Same Different Classification loss: 0.3589506149291992 iteration:  2705 epoch:  4\n","Male Female Classification loss: 0.610409140586853 iteration:  2705 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.2405354976654053 iteration:  2710 epoch:  4\n","LQ image reconstruction loss: 0.40889793634414673 iteration:  2710 epoch:  4\n","Same Different Classification loss: 0.4008246958255768 iteration:  2710 epoch:  4\n","Male Female Classification loss: 0.6233177781105042 iteration:  2710 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.258901834487915 iteration:  2715 epoch:  4\n","LQ image reconstruction loss: 0.47684726119041443 iteration:  2715 epoch:  4\n","Same Different Classification loss: 0.41072383522987366 iteration:  2715 epoch:  4\n","Male Female Classification loss: 0.566655158996582 iteration:  2715 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 1.8582229614257812 iteration:  2720 epoch:  4\n","LQ image reconstruction loss: 0.3273199796676636 iteration:  2720 epoch:  4\n","Same Different Classification loss: 0.3301067650318146 iteration:  2720 epoch:  4\n","Male Female Classification loss: 0.6089119911193848 iteration:  2720 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.7158870697021484 iteration:  2725 epoch:  4\n","LQ image reconstruction loss: 0.3430495262145996 iteration:  2725 epoch:  4\n","Same Different Classification loss: 0.3249298334121704 iteration:  2725 epoch:  4\n","Male Female Classification loss: 0.5389379858970642 iteration:  2725 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.3060338497161865 iteration:  2730 epoch:  4\n","LQ image reconstruction loss: 0.37701746821403503 iteration:  2730 epoch:  4\n","Same Different Classification loss: 0.3798161745071411 iteration:  2730 epoch:  4\n","Male Female Classification loss: 0.5979708433151245 iteration:  2730 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.5233092308044434 iteration:  2735 epoch:  4\n","LQ image reconstruction loss: 0.4181223511695862 iteration:  2735 epoch:  4\n","Same Different Classification loss: 0.3729608952999115 iteration:  2735 epoch:  4\n","Male Female Classification loss: 0.5392838716506958 iteration:  2735 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.035691738128662 iteration:  2740 epoch:  4\n","LQ image reconstruction loss: 0.32276642322540283 iteration:  2740 epoch:  4\n","Same Different Classification loss: 0.2770192623138428 iteration:  2740 epoch:  4\n","Male Female Classification loss: 0.5396701693534851 iteration:  2740 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.127840518951416 iteration:  2745 epoch:  4\n","LQ image reconstruction loss: 0.3354591727256775 iteration:  2745 epoch:  4\n","Same Different Classification loss: 0.3461197018623352 iteration:  2745 epoch:  4\n","Male Female Classification loss: 0.5702010989189148 iteration:  2745 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.1351876258850098 iteration:  2750 epoch:  4\n","LQ image reconstruction loss: 0.33149346709251404 iteration:  2750 epoch:  4\n","Same Different Classification loss: 0.33425620198249817 iteration:  2750 epoch:  4\n","Male Female Classification loss: 0.6141889691352844 iteration:  2750 epoch:  4\n","----------------------------------------------------\n","Accuracy of SD: 90 %\n","Accuracy of MF: 75 %\n","Mean Accuracy of SD: 63 %\n","Mean Accuracy of MF: 63 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.3297722339630127 iteration:  2755 epoch:  4\n","LQ image reconstruction loss: 0.3959660530090332 iteration:  2755 epoch:  4\n","Same Different Classification loss: 0.40343791246414185 iteration:  2755 epoch:  4\n","Male Female Classification loss: 0.588083803653717 iteration:  2755 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 1.914279818534851 iteration:  2760 epoch:  4\n","LQ image reconstruction loss: 0.3166750371456146 iteration:  2760 epoch:  4\n","Same Different Classification loss: 0.3201991021633148 iteration:  2760 epoch:  4\n","Male Female Classification loss: 0.6136802434921265 iteration:  2760 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 1.8096064329147339 iteration:  2765 epoch:  4\n","LQ image reconstruction loss: 0.39682555198669434 iteration:  2765 epoch:  4\n","Same Different Classification loss: 0.3862045407295227 iteration:  2765 epoch:  4\n","Male Female Classification loss: 0.5815067291259766 iteration:  2765 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.387434244155884 iteration:  2770 epoch:  4\n","LQ image reconstruction loss: 0.3921629786491394 iteration:  2770 epoch:  4\n","Same Different Classification loss: 0.3638758361339569 iteration:  2770 epoch:  4\n","Male Female Classification loss: 0.5426563024520874 iteration:  2770 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.253748893737793 iteration:  2775 epoch:  4\n","LQ image reconstruction loss: 0.45751094818115234 iteration:  2775 epoch:  4\n","Same Different Classification loss: 0.4161895513534546 iteration:  2775 epoch:  4\n","Male Female Classification loss: 0.5651770234107971 iteration:  2775 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 1.888264775276184 iteration:  2780 epoch:  4\n","LQ image reconstruction loss: 0.338584303855896 iteration:  2780 epoch:  4\n","Same Different Classification loss: 0.3197455108165741 iteration:  2780 epoch:  4\n","Male Female Classification loss: 0.5886337161064148 iteration:  2780 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.5996780395507812 iteration:  2785 epoch:  4\n","LQ image reconstruction loss: 0.37779784202575684 iteration:  2785 epoch:  4\n","Same Different Classification loss: 0.3374241888523102 iteration:  2785 epoch:  4\n","Male Female Classification loss: 0.5973765254020691 iteration:  2785 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.054349184036255 iteration:  2790 epoch:  4\n","LQ image reconstruction loss: 0.3315924108028412 iteration:  2790 epoch:  4\n","Same Different Classification loss: 0.31336671113967896 iteration:  2790 epoch:  4\n","Male Female Classification loss: 0.5433509349822998 iteration:  2790 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.1475682258605957 iteration:  2795 epoch:  4\n","LQ image reconstruction loss: 0.3348284363746643 iteration:  2795 epoch:  4\n","Same Different Classification loss: 0.3589521050453186 iteration:  2795 epoch:  4\n","Male Female Classification loss: 0.5845810174942017 iteration:  2795 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 1.969420313835144 iteration:  2800 epoch:  4\n","LQ image reconstruction loss: 0.36432263255119324 iteration:  2800 epoch:  4\n","Same Different Classification loss: 0.3295601010322571 iteration:  2800 epoch:  4\n","Male Female Classification loss: 0.5364004373550415 iteration:  2800 epoch:  4\n","----------------------------------------------------\n","Accuracy of SD: 91 %\n","Accuracy of MF: 75 %\n","Mean Accuracy of SD: 64 %\n","Mean Accuracy of MF: 63 %\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.2350590229034424 iteration:  2805 epoch:  4\n","LQ image reconstruction loss: 0.312483549118042 iteration:  2805 epoch:  4\n","Same Different Classification loss: 0.3105897605419159 iteration:  2805 epoch:  4\n","Male Female Classification loss: 0.6244051456451416 iteration:  2805 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.3977904319763184 iteration:  2810 epoch:  4\n","LQ image reconstruction loss: 0.4551982879638672 iteration:  2810 epoch:  4\n","Same Different Classification loss: 0.39403387904167175 iteration:  2810 epoch:  4\n","Male Female Classification loss: 0.5008388757705688 iteration:  2810 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.5661869049072266 iteration:  2815 epoch:  4\n","LQ image reconstruction loss: 0.38572531938552856 iteration:  2815 epoch:  4\n","Same Different Classification loss: 0.3432609438896179 iteration:  2815 epoch:  4\n","Male Female Classification loss: 0.5493854284286499 iteration:  2815 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.624863624572754 iteration:  2820 epoch:  4\n","LQ image reconstruction loss: 0.34567952156066895 iteration:  2820 epoch:  4\n","Same Different Classification loss: 0.3546122610569 iteration:  2820 epoch:  4\n","Male Female Classification loss: 0.5759925842285156 iteration:  2820 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.6543569564819336 iteration:  2825 epoch:  4\n","LQ image reconstruction loss: 0.41640567779541016 iteration:  2825 epoch:  4\n","Same Different Classification loss: 0.4505196511745453 iteration:  2825 epoch:  4\n","Male Female Classification loss: 0.673176646232605 iteration:  2825 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 2.8038384914398193 iteration:  2830 epoch:  4\n","LQ image reconstruction loss: 0.29915207624435425 iteration:  2830 epoch:  4\n","Same Different Classification loss: 0.3004791736602783 iteration:  2830 epoch:  4\n","Male Female Classification loss: 0.593187689781189 iteration:  2830 epoch:  4\n","----------------------------------------------------\n","HQ image reconstruction loss: 1.8290644884109497 iteration:  2835 epoch:  4\n","LQ image reconstruction loss: 0.3076781630516052 iteration:  2835 epoch:  4\n","Same Different Classification loss: 0.2854173481464386 iteration:  2835 epoch:  4\n","Male Female Classification loss: 0.5638261437416077 iteration:  2835 epoch:  4\n","----------------------------------------------------\n"]},{"data":{"text/plain":["-1"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["class Net_A(nn.Module):\n","\n","    def __init__(self):\n","        super(Net_A,self).__init__()\n","\n","        self.ip3 = nn.Linear(2*560,2000,True)   # change the first parameter in case you change the size of the small image\n","        self.n3 = nn.BatchNorm1d(2000)\n","        self.n1 = nn.BatchNorm1d(2*560)\n","        self.ip4 = nn.Linear(2000,1000,True)\n","        self.n4 = nn.BatchNorm1d(1000)\n","        self.ip5 = nn.Linear(1000,128,True)\n","        self.n5 = nn.BatchNorm1d(128)\n","        self.ip6 = nn.Linear(128,2,True)\n","\n","    def forward(self,x):\n","        x = self.n1(x)\n","        x2 = self.ip3(x)\n","        x2 = F.relu(x2)\n","        x2 = self.n3(x2)\n","\n","        x2 = self.ip4(x2)\n","        x2 = F.relu(x2)\n","        x2 = self.n4(x2)\n","\n","        x2 = self.ip5(x2)\n","        x2 = F.relu(x2)\n","        x2 = self.n5(x2)\n","\n","        x2 = self.ip6(x2)\n","        x2 = F.relu(x2)\n","\n","\n","\n","\n","        # x2 = x2.mul(-1)\n","        # x = F.relu(x)\n","        # x = F.softmax(x,1)\n","        return x2\n","\n","\n","class Male_Female_dataset(Dataset):\n","\n","\n","   def __init__(self,root_dir,shape,transform=None):\n","     self.root_dir = root_dir\n","     self.transform = transform\n","     self.shape = shape\n","\n","   def __len__(self):\n","     return len(cele_attrib)\n","\n","   def __getitem__(self,idx):\n","    img1_name = os.path.join(self.root_dir,(\"{:06d}.png\".format(idx+1)))\n","    image1 = io.imread(img1_name)\n","    image1 = transform.resize(image1,ip_shape)\n","    #print(\"Got image 1: {0}\".format(image1))\n","\n","\n","    t = torch.rand(1)\n","    if t \u003e 0.5:\n","        h = torch.randint(0,len(self),(1,1))\n","        img2_name = os.path.join(self.root_dir,(\"{:06d}.png\".format(int(h)+1)))\n","        image2 = io.imread(img2_name)\n","        #print(\"Got image 2: {0}\".format(image2))\n","        image2 = transform.resize(image2,self.shape)\n","        image2 = torch.Tensor.float(torch.from_numpy(image2))\n","        image2 = (torch.Tensor.permute(image2,(2,0,1)))\n","        image2 = image2/256.0\n","        image1 = torch.Tensor.float(torch.from_numpy(image1))\n","        image1 = (torch.Tensor.permute(image1,(2,0,1)))\n","        image1 = image1/256.0\n","        annot = 0\n","        gender = cele_attrib['Male'][idx]\n","\n","    else:\n","        image2 = image1\n","        image2 = transform.resize(image2,self.shape)\n","        image2 = torch.Tensor.float(torch.from_numpy(image2))\n","        image2 = (torch.Tensor.permute(image2,(2,0,1)))\n","        image2 = image2/256.0\n","        image1 = torch.Tensor.float(torch.from_numpy(image1))\n","        image1 = (torch.Tensor.permute(image1,(2,0,1)))\n","        image1 = image1/256.0\n","        annot = 1\n","        gender = cele_attrib['Male'][idx]\n","\n","    sample = {'image1' : image1,\n","                'image2' : image2,\n","                'same'   : annot,\n","                'gender' : gender}\n","    return sample\n","\n","\n","\n","\n","reconstruction_function = nn.BCELoss()\n","reconstruction_function.size_average = False\n","\n","\n","def loss_function(recon_x, x, mu, logvar,loss1,loss2):\n","    BCE = reconstruction_function(recon_x, x)\n","\n","    # https://arxiv.org/abs/1312.6114 (Appendix B)\n","    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n","    KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n","    KLD = torch.sum(KLD_element).mul_(-0.5)\n","\n","    return BCE + KLD + loss1-0.4*loss2 \n","\n","\n","use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","print(device)\n","shape = (16,16)\n","ip_shape = (128,128)\n","train_ratio = 0.7\n","val_ratio = 0.15\n","test_ratio = 1-train_ratio-val_ratio\n","dataset = Male_Female_dataset(images_path,shape)\n","\n","\n","if load_net:\n","    model_high = VAE(nc=3, ngf=ip_shape[0], ndf=ip_shape[1], latent_variable_size=500).to(device)\n","    model_low = VAE_low(nc=3, ngf=shape[0], ndf=shape[1], latent_variable_size=60).to(device)\n","    class_net_SD = Net_A().to(device)\n","    class_net_MF = Net_A().to(device)\n","\n","    checkpoint_high = torch.load('/content/drive/MyDrive/vre_autoencoder_final/trained_modes/checkpoint_propose_Male_celeb_high.pth.tar')\n","    checkpoint_low = torch.load('/content/drive/MyDrive/vre_autoencoder_final/trained_modes/checkpoint_propose_Male_celeb_low.pth.tar')\n","    checkpoint_class_SD = torch.load('/content/drive/MyDrive/vre_autoencoder_final/trained_modes/checkpoint_classification_propose_SD.pth.tar')\n","    checkpoint_class_SD = torch.load('/content/drive/MyDrive/vre_autoencoder_final/trained_modes/checkpoint_classification_propose_MF.pth.tar')\n","\n","    model_high.load_state_dict ( checkpoint_high['state_dict'])\n","    model_low.load_state_dict ( checkpoint_low['state_dict'])\n","    class_net_SD.load_state_dict ( checkpoint_class_SD['state_dict'])\n","    class_net_MF.load_state_dict ( checkpoint_class_SD['state_dict'])\n","\n","    optimizer_high = optim.Adam(model_high.parameters(),lr = 0.00001, weight_decay = 0.0005)\n","    optimizer_low = optim.Adam(model_low.parameters(),lr = 0.0001, weight_decay = 0.0005)\n","    optimizer_class_SD = optim.Adam(class_net_SD.parameters(),lr = 0.0001, weight_decay = 0.0005)\n","    optimizer_class_MF = optim.Adam(class_net_SD.parameters(),lr = 0.0001, weight_decay = 0.0005)\n","\n","    optimizer_high.load_state_dict = checkpoint_high['optimizer']\n","    optimizer_low.load_state_dict = checkpoint_low['optimizer']\n","    optimizer_class_SD.load_state_dict = checkpoint_class_SD['optimizer']\n","    optimizer_class_MF.load_state_dict = checkpoint_class_SD['optimizer']\n","\n","\n","\n","else:\n","    torch.cuda.empty_cache()\n","    model_high   = VAE(nc=3, ngf=ip_shape[0], ndf=ip_shape[1], latent_variable_size=500).to(device)\n","    model_low    = VAE_low(nc=3, ngf=shape[0], ndf=shape[1], latent_variable_size=60).to(device)\n","    class_net_SD = Net_A().to(device)\n","    class_net_MF = Net_A().to(device)\n","\n","\n","    optimizer_high     = optim.Adam(model_high.parameters(),lr = 0.00001, weight_decay = 0.0005)\n","    optimizer_low      = optim.Adam(model_low.parameters(),lr = 0.00001, weight_decay = 0.0005)\n","    optimizer_class_SD = optim.Adam(class_net_SD.parameters(),lr = 0.000001, weight_decay = 0.0005)\n","    optimizer_class_MF = optim.Adam(class_net_SD.parameters(),lr = 0.000001, weight_decay = 0.0005)\n","\n","    checkpoint_class_SD= {'epoch':0}\n","\n","\n","\n","index             = np.random.permutation(len(dataset)-5)\n","train_data_length = int(train_ratio*len(index))\n","val_data_length   = int(val_ratio*len(index))\n","test_data_length  = int(test_ratio*len(index))\n","train_index       = index[:train_data_length]\n","val_index         = index[train_data_length:(train_data_length+val_data_length)]\n","test_index        = index[train_data_length+val_data_length:]\n","\n","train_dataloader = DataLoader(dataset,batch_size=50,sampler = SubsetRandomSampler(train_index))\n","\n","err_same = []\n","err_gender = []\n","\n","acc1_SD = []\n","acc1_MF = []\n","\n","criterion2 = nn.CrossEntropyLoss()\n","t = -1\n","# Check if images are already exist in the folder if not then pass\n","for ep in range(checkpoint_class_SD['epoch'],5):\n","    for i,data in enumerate(train_dataloader):\n","        torch.cuda.empty_cache()\n","        optimizer_high.zero_grad()\n","        optimizer_low.zero_grad()\n","        optimizer_class_SD.zero_grad()\n","        optimizer_class_MF.zero_grad()\n","\n","        input1, input2 , label, gender = data.items()\n","        input1, input2 = input1[1].to(device), input2[1].to(device)\n","        label1 = label[1].to(device)\n","        gender = gender[1].to(device)\n","        gender = (gender+torch.ones_like(gender).to(device))//2\n","\n","\n","\n","        recon_batch_high, mu_high, logvar_high, h2_h = model_high(input1)\n","        recon_batch_low, mu_low, logvar_low, h2_l = model_low(input2)\n","\n","        mu_all = torch.cat((mu_high,mu_low),1)\n","        logvar_all = torch.cat((logvar_high,logvar_low),1)\n","        class_input = torch.cat((mu_all,logvar_all),1)\n","        #class_input = mu_all\n","        class_out_SD = class_net_SD(class_input)\n","        class_out_MF = class_net_MF(class_input)\n","\n","\n","\n","        loss_class_SD = criterion2(class_out_SD,label1)\n","        loss_class_MF = criterion2(class_out_MF,gender)\n","        loss_high = loss_function(recon_batch_high, input1, mu_high, logvar_high,loss_class_SD,loss_class_MF)\n","        loss_low = loss_function(recon_batch_low, input2, mu_low, logvar_low,loss_class_SD,loss_class_MF)\n","\n","        loss_class_SD.backward(retain_graph=True)\n","        loss_class_MF.backward(retain_graph=True)\n","        loss_high.backward(retain_graph=True)\n","        loss_low.backward()\n","\n","\n","        optimizer_high.step()\n","        optimizer_low.step()\n","        optimizer_class_SD.step()\n","        optimizer_class_MF.step()\n","\n","\n","\n","        if (i%5==0):\n","            print (\"HQ image reconstruction loss:\",loss_high.item(),'iteration: ',i,\"epoch: \",ep)\n","            print (\"LQ image reconstruction loss:\",loss_low.item(),'iteration: ',i,\"epoch: \",ep)\n","            print (\"Same Different Classification loss:\", loss_class_SD.item(),'iteration: ',i,\"epoch: \",ep)\n","            print (\"Male Female Classification loss:\", loss_class_MF.item(),'iteration: ',i,\"epoch: \",ep)\n","            print (\"----------------------------------------------------\")\n","\n","        if (i%50==0):\n","            val_index1 = np.random.permutation(val_index)[:100]\n","            val_dataloader = DataLoader(dataset,batch_size=100,sampler = SubsetRandomSampler(val_index1))\n","            val_iter = iter(val_dataloader)\n","\n","            total = 0\n","            correct1_SD = 0\n","            correct1_MF = 0\n","            for j,dataj in enumerate(val_dataloader):\n","                input1j, input2j, labelj, gender = dataj.items()\n","                input1j, input2j = input1j[1].to(device), input2j[1].to(device)\n","\n","                labelj = labelj[1].to(device)\n","                gender = gender[1].to(device)\n","                gender = (gender+torch.ones_like(gender).to(device))//2\n","                recon_batch_high, mu_high, logvar_high, h2_h = model_high(input1j)\n","                recon_batch_low, mu_low, logvar_low, h2_l = model_low(input2j)\n","\n","                mu_all = torch.cat((mu_high,mu_low.data),1)\n","                logvar_all = torch.cat((logvar_high,logvar_low),1)\n","                class_input = torch.cat((mu_all,logvar_all),1)\n","                #class_input = mu_all\n","                class_out_SD = class_net_SD(class_input)\n","                class_out_MF = class_net_MF(class_input)\n","                _,predicted1_SD = torch.max(class_out_SD.data,1)\n","                _,predicted1_MF = torch.max(class_out_MF.data,1)\n","                # _,predicted2 = torch.max(output_A.data,1)\n","                total +=labelj.size(0)\n","                correct1_SD += (predicted1_SD == labelj).sum().item()\n","                correct1_MF += (predicted1_MF == gender).sum().item()\n","                # correct2 += (predicted2 == gender).sum().item()\n","            print('Accuracy of SD: %d %%'%(100*correct1_SD/total))\n","            print('Accuracy of MF: %d %%'%(100*correct1_MF/total))\n","\n","            # print('Accuracy_MF: %d %%'%(100*correct2/total))\n","\n","            acc1_SD.append(100*correct1_SD/total) #low high\n","            acc1_MF.append(100*correct1_MF/total) #low high\n","            print('Mean Accuracy of SD: %d %%'%(np.mean(acc1_SD)))\n","            print('Mean Accuracy of MF: %d %%'%(np.mean(acc1_MF)))\n","            print(\"----------------------------------------------------\")\n","            # acc2.append(100*correct2/total) # male female\n","    save_checkpoint_class_SD({\n","            'epoch': ep + 1,\n","            'state_dict': class_net_SD.state_dict(),\n","\n","            'optimizer' : optimizer_class_SD.state_dict(),\n","        })\n","    save_checkpoint_class_MF({\n","            'epoch': ep + 1,\n","            'state_dict': class_net_MF.state_dict(),\n","\n","            'optimizer' : optimizer_class_MF.state_dict(),\n","        })\n","    save_checkpoint_high({\n","            'epoch': ep + 1,\n","            'state_dict': model_high.state_dict(),\n","\n","            'optimizer' : optimizer_high.state_dict(),\n","        })\n","    save_checkpoint_low({\n","            'epoch': ep + 1,\n","            'state_dict': model_low.state_dict(),\n","\n","            'optimizer' : optimizer_low.state_dict(),\n","        })\n","\n","dataset.__getitem__(1)\n","\n","cele_attrib['Male'][1]\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"FGovPYknyst7"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[7 0]\n"," [1 7]]\n","confusion matrix for female\n","[[5 3]\n"," [0 0]]\n","confusion matrix for male\n","[[0 0]\n"," [2 5]]\n","confusion matrix for male female\n","[[5 3]\n"," [2 5]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[14  0]\n"," [ 2 14]]\n","confusion matrix for female\n","[[10  6]\n"," [ 0  0]]\n","confusion matrix for male\n","[[ 0  0]\n"," [ 4 10]]\n","confusion matrix for male female\n","[[10  6]\n"," [ 4 10]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[21  0]\n"," [ 3 21]]\n","confusion matrix for female\n","[[15  9]\n"," [ 0  0]]\n","confusion matrix for male\n","[[ 0  0]\n"," [ 6 15]]\n","confusion matrix for male female\n","[[15  9]\n"," [ 6 15]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[28  0]\n"," [ 4 28]]\n","confusion matrix for female\n","[[20 12]\n"," [ 0  0]]\n","confusion matrix for male\n","[[ 0  0]\n"," [ 8 20]]\n","confusion matrix for male female\n","[[20 12]\n"," [ 8 20]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[35  0]\n"," [ 5 35]]\n","confusion matrix for female\n","[[25 15]\n"," [ 0  0]]\n","confusion matrix for male\n","[[ 0  0]\n"," [10 25]]\n","confusion matrix for male female\n","[[25 15]\n"," [10 25]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[42  0]\n"," [ 6 42]]\n","confusion matrix for female\n","[[30 18]\n"," [ 0  0]]\n","confusion matrix for male\n","[[ 0  0]\n"," [12 30]]\n","confusion matrix for male female\n","[[30 18]\n"," [12 30]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[49  0]\n"," [ 7 49]]\n","confusion matrix for female\n","[[35 21]\n"," [ 0  0]]\n","confusion matrix for male\n","[[ 0  0]\n"," [14 35]]\n","confusion matrix for male female\n","[[35 21]\n"," [14 35]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[56  0]\n"," [ 8 56]]\n","confusion matrix for female\n","[[40 24]\n"," [ 0  0]]\n","confusion matrix for male\n","[[ 0  0]\n"," [16 40]]\n","confusion matrix for male female\n","[[40 24]\n"," [16 40]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[63  0]\n"," [ 9 63]]\n","confusion matrix for female\n","[[45 27]\n"," [ 0  0]]\n","confusion matrix for male\n","[[ 0  0]\n"," [18 45]]\n","confusion matrix for male female\n","[[45 27]\n"," [18 45]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[70  0]\n"," [10 70]]\n","confusion matrix for female\n","[[50 30]\n"," [ 0  0]]\n","confusion matrix for male\n","[[ 0  0]\n"," [20 50]]\n","confusion matrix for male female\n","[[50 30]\n"," [20 50]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[77  0]\n"," [11 77]]\n","confusion matrix for female\n","[[55 33]\n"," [ 0  0]]\n","confusion matrix for male\n","[[ 0  0]\n"," [22 55]]\n","confusion matrix for male female\n","[[55 33]\n"," [22 55]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[84  0]\n"," [12 84]]\n","confusion matrix for female\n","[[60 36]\n"," [ 0  0]]\n","confusion matrix for male\n","[[ 0  0]\n"," [24 60]]\n","confusion matrix for male female\n","[[60 36]\n"," [24 60]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[91  0]\n"," [13 91]]\n","confusion matrix for female\n","[[65 39]\n"," [ 0  0]]\n","confusion matrix for male\n","[[ 0  0]\n"," [26 65]]\n","confusion matrix for male female\n","[[65 39]\n"," [26 65]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[98  0]\n"," [14 98]]\n","confusion matrix for female\n","[[70 42]\n"," [ 0  0]]\n","confusion matrix for male\n","[[ 0  0]\n"," [28 70]]\n","confusion matrix for male female\n","[[70 42]\n"," [28 70]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[105   0]\n"," [ 15 105]]\n","confusion matrix for female\n","[[75 45]\n"," [ 0  0]]\n","confusion matrix for male\n","[[ 0  0]\n"," [30 75]]\n","confusion matrix for male female\n","[[75 45]\n"," [30 75]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[112   0]\n"," [ 16 112]]\n","confusion matrix for female\n","[[80 48]\n"," [ 0  0]]\n","confusion matrix for male\n","[[ 0  0]\n"," [32 80]]\n","confusion matrix for male female\n","[[80 48]\n"," [32 80]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[119   0]\n"," [ 17 119]]\n","confusion matrix for female\n","[[85 51]\n"," [ 0  0]]\n","confusion matrix for male\n","[[ 0  0]\n"," [34 85]]\n","confusion matrix for male female\n","[[85 51]\n"," [34 85]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[126   0]\n"," [ 18 126]]\n","confusion matrix for female\n","[[90 54]\n"," [ 0  0]]\n","confusion matrix for male\n","[[ 0  0]\n"," [36 90]]\n","confusion matrix for male female\n","[[90 54]\n"," [36 90]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[133   0]\n"," [ 19 133]]\n","confusion matrix for female\n","[[95 57]\n"," [ 0  0]]\n","confusion matrix for male\n","[[ 0  0]\n"," [38 95]]\n","confusion matrix for male female\n","[[95 57]\n"," [38 95]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[140   0]\n"," [ 20 140]]\n","confusion matrix for female\n","[[100  60]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [ 40 100]]\n","confusion matrix for male female\n","[[100  60]\n"," [ 40 100]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[147   0]\n"," [ 21 147]]\n","confusion matrix for female\n","[[105  63]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [ 42 105]]\n","confusion matrix for male female\n","[[105  63]\n"," [ 42 105]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[154   0]\n"," [ 22 154]]\n","confusion matrix for female\n","[[110  66]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [ 44 110]]\n","confusion matrix for male female\n","[[110  66]\n"," [ 44 110]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[161   0]\n"," [ 23 161]]\n","confusion matrix for female\n","[[115  69]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [ 46 115]]\n","confusion matrix for male female\n","[[115  69]\n"," [ 46 115]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[168   0]\n"," [ 24 168]]\n","confusion matrix for female\n","[[120  72]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [ 48 120]]\n","confusion matrix for male female\n","[[120  72]\n"," [ 48 120]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[175   0]\n"," [ 25 175]]\n","confusion matrix for female\n","[[125  75]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [ 50 125]]\n","confusion matrix for male female\n","[[125  75]\n"," [ 50 125]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[182   0]\n"," [ 26 182]]\n","confusion matrix for female\n","[[130  78]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [ 52 130]]\n","confusion matrix for male female\n","[[130  78]\n"," [ 52 130]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[189   0]\n"," [ 27 189]]\n","confusion matrix for female\n","[[135  81]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [ 54 135]]\n","confusion matrix for male female\n","[[135  81]\n"," [ 54 135]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[196   0]\n"," [ 28 196]]\n","confusion matrix for female\n","[[140  84]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [ 56 140]]\n","confusion matrix for male female\n","[[140  84]\n"," [ 56 140]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[203   0]\n"," [ 29 203]]\n","confusion matrix for female\n","[[145  87]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [ 58 145]]\n","confusion matrix for male female\n","[[145  87]\n"," [ 58 145]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[210   0]\n"," [ 30 210]]\n","confusion matrix for female\n","[[150  90]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [ 60 150]]\n","confusion matrix for male female\n","[[150  90]\n"," [ 60 150]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[217   0]\n"," [ 31 217]]\n","confusion matrix for female\n","[[155  93]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [ 62 155]]\n","confusion matrix for male female\n","[[155  93]\n"," [ 62 155]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[224   0]\n"," [ 32 224]]\n","confusion matrix for female\n","[[160  96]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [ 64 160]]\n","confusion matrix for male female\n","[[160  96]\n"," [ 64 160]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[231   0]\n"," [ 33 231]]\n","confusion matrix for female\n","[[165  99]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [ 66 165]]\n","confusion matrix for male female\n","[[165  99]\n"," [ 66 165]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[238   0]\n"," [ 34 238]]\n","confusion matrix for female\n","[[170 102]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [ 68 170]]\n","confusion matrix for male female\n","[[170 102]\n"," [ 68 170]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[245   0]\n"," [ 35 245]]\n","confusion matrix for female\n","[[175 105]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [ 70 175]]\n","confusion matrix for male female\n","[[175 105]\n"," [ 70 175]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[252   0]\n"," [ 36 252]]\n","confusion matrix for female\n","[[180 108]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [ 72 180]]\n","confusion matrix for male female\n","[[180 108]\n"," [ 72 180]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[259   0]\n"," [ 37 259]]\n","confusion matrix for female\n","[[185 111]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [ 74 185]]\n","confusion matrix for male female\n","[[185 111]\n"," [ 74 185]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[266   0]\n"," [ 38 266]]\n","confusion matrix for female\n","[[190 114]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [ 76 190]]\n","confusion matrix for male female\n","[[190 114]\n"," [ 76 190]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[273   0]\n"," [ 39 273]]\n","confusion matrix for female\n","[[195 117]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [ 78 195]]\n","confusion matrix for male female\n","[[195 117]\n"," [ 78 195]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[280   0]\n"," [ 40 280]]\n","confusion matrix for female\n","[[200 120]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [ 80 200]]\n","confusion matrix for male female\n","[[200 120]\n"," [ 80 200]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[287   0]\n"," [ 41 287]]\n","confusion matrix for female\n","[[205 123]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [ 82 205]]\n","confusion matrix for male female\n","[[205 123]\n"," [ 82 205]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[294   0]\n"," [ 42 294]]\n","confusion matrix for female\n","[[210 126]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [ 84 210]]\n","confusion matrix for male female\n","[[210 126]\n"," [ 84 210]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[301   0]\n"," [ 43 301]]\n","confusion matrix for female\n","[[215 129]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [ 86 215]]\n","confusion matrix for male female\n","[[215 129]\n"," [ 86 215]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[308   0]\n"," [ 44 308]]\n","confusion matrix for female\n","[[220 132]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [ 88 220]]\n","confusion matrix for male female\n","[[220 132]\n"," [ 88 220]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[315   0]\n"," [ 45 315]]\n","confusion matrix for female\n","[[225 135]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [ 90 225]]\n","confusion matrix for male female\n","[[225 135]\n"," [ 90 225]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[322   0]\n"," [ 46 322]]\n","confusion matrix for female\n","[[230 138]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [ 92 230]]\n","confusion matrix for male female\n","[[230 138]\n"," [ 92 230]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[329   0]\n"," [ 47 329]]\n","confusion matrix for female\n","[[235 141]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [ 94 235]]\n","confusion matrix for male female\n","[[235 141]\n"," [ 94 235]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[336   0]\n"," [ 48 336]]\n","confusion matrix for female\n","[[240 144]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [ 96 240]]\n","confusion matrix for male female\n","[[240 144]\n"," [ 96 240]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[343   0]\n"," [ 49 343]]\n","confusion matrix for female\n","[[245 147]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [ 98 245]]\n","confusion matrix for male female\n","[[245 147]\n"," [ 98 245]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[350   0]\n"," [ 50 350]]\n","confusion matrix for female\n","[[250 150]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [100 250]]\n","confusion matrix for male female\n","[[250 150]\n"," [100 250]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[357   0]\n"," [ 51 357]]\n","confusion matrix for female\n","[[255 153]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [102 255]]\n","confusion matrix for male female\n","[[255 153]\n"," [102 255]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[364   0]\n"," [ 52 364]]\n","confusion matrix for female\n","[[260 156]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [104 260]]\n","confusion matrix for male female\n","[[260 156]\n"," [104 260]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[371   0]\n"," [ 53 371]]\n","confusion matrix for female\n","[[265 159]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [106 265]]\n","confusion matrix for male female\n","[[265 159]\n"," [106 265]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[378   0]\n"," [ 54 378]]\n","confusion matrix for female\n","[[270 162]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [108 270]]\n","confusion matrix for male female\n","[[270 162]\n"," [108 270]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[385   0]\n"," [ 55 385]]\n","confusion matrix for female\n","[[275 165]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [110 275]]\n","confusion matrix for male female\n","[[275 165]\n"," [110 275]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[392   0]\n"," [ 56 392]]\n","confusion matrix for female\n","[[280 168]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [112 280]]\n","confusion matrix for male female\n","[[280 168]\n"," [112 280]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[399   0]\n"," [ 57 399]]\n","confusion matrix for female\n","[[285 171]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [114 285]]\n","confusion matrix for male female\n","[[285 171]\n"," [114 285]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[406   0]\n"," [ 58 406]]\n","confusion matrix for female\n","[[290 174]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [116 290]]\n","confusion matrix for male female\n","[[290 174]\n"," [116 290]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[413   0]\n"," [ 59 413]]\n","confusion matrix for female\n","[[295 177]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [118 295]]\n","confusion matrix for male female\n","[[295 177]\n"," [118 295]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[420   0]\n"," [ 60 420]]\n","confusion matrix for female\n","[[300 180]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [120 300]]\n","confusion matrix for male female\n","[[300 180]\n"," [120 300]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[427   0]\n"," [ 61 427]]\n","confusion matrix for female\n","[[305 183]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [122 305]]\n","confusion matrix for male female\n","[[305 183]\n"," [122 305]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[434   0]\n"," [ 62 434]]\n","confusion matrix for female\n","[[310 186]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [124 310]]\n","confusion matrix for male female\n","[[310 186]\n"," [124 310]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[441   0]\n"," [ 63 441]]\n","confusion matrix for female\n","[[315 189]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [126 315]]\n","confusion matrix for male female\n","[[315 189]\n"," [126 315]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[448   0]\n"," [ 64 448]]\n","confusion matrix for female\n","[[320 192]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [128 320]]\n","confusion matrix for male female\n","[[320 192]\n"," [128 320]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[455   0]\n"," [ 65 455]]\n","confusion matrix for female\n","[[325 195]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [130 325]]\n","confusion matrix for male female\n","[[325 195]\n"," [130 325]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[462   0]\n"," [ 66 462]]\n","confusion matrix for female\n","[[330 198]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [132 330]]\n","confusion matrix for male female\n","[[330 198]\n"," [132 330]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[469   0]\n"," [ 67 469]]\n","confusion matrix for female\n","[[335 201]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [134 335]]\n","confusion matrix for male female\n","[[335 201]\n"," [134 335]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[476   0]\n"," [ 68 476]]\n","confusion matrix for female\n","[[340 204]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [136 340]]\n","confusion matrix for male female\n","[[340 204]\n"," [136 340]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[483   0]\n"," [ 69 483]]\n","confusion matrix for female\n","[[345 207]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [138 345]]\n","confusion matrix for male female\n","[[345 207]\n"," [138 345]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[490   0]\n"," [ 70 490]]\n","confusion matrix for female\n","[[350 210]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [140 350]]\n","confusion matrix for male female\n","[[350 210]\n"," [140 350]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[497   0]\n"," [ 71 497]]\n","confusion matrix for female\n","[[355 213]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [142 355]]\n","confusion matrix for male female\n","[[355 213]\n"," [142 355]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[504   0]\n"," [ 72 504]]\n","confusion matrix for female\n","[[360 216]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [144 360]]\n","confusion matrix for male female\n","[[360 216]\n"," [144 360]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[511   0]\n"," [ 73 511]]\n","confusion matrix for female\n","[[365 219]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [146 365]]\n","confusion matrix for male female\n","[[365 219]\n"," [146 365]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[518   0]\n"," [ 74 518]]\n","confusion matrix for female\n","[[370 222]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [148 370]]\n","confusion matrix for male female\n","[[370 222]\n"," [148 370]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[525   0]\n"," [ 75 525]]\n","confusion matrix for female\n","[[375 225]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [150 375]]\n","confusion matrix for male female\n","[[375 225]\n"," [150 375]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[532   0]\n"," [ 76 532]]\n","confusion matrix for female\n","[[380 228]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [152 380]]\n","confusion matrix for male female\n","[[380 228]\n"," [152 380]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[539   0]\n"," [ 77 539]]\n","confusion matrix for female\n","[[385 231]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [154 385]]\n","confusion matrix for male female\n","[[385 231]\n"," [154 385]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[546   0]\n"," [ 78 546]]\n","confusion matrix for female\n","[[390 234]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [156 390]]\n","confusion matrix for male female\n","[[390 234]\n"," [156 390]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[553   0]\n"," [ 79 553]]\n","confusion matrix for female\n","[[395 237]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [158 395]]\n","confusion matrix for male female\n","[[395 237]\n"," [158 395]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[560   0]\n"," [ 80 560]]\n","confusion matrix for female\n","[[400 240]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [160 400]]\n","confusion matrix for male female\n","[[400 240]\n"," [160 400]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[567   0]\n"," [ 81 567]]\n","confusion matrix for female\n","[[405 243]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [162 405]]\n","confusion matrix for male female\n","[[405 243]\n"," [162 405]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[574   0]\n"," [ 82 574]]\n","confusion matrix for female\n","[[410 246]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [164 410]]\n","confusion matrix for male female\n","[[410 246]\n"," [164 410]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[581   0]\n"," [ 83 581]]\n","confusion matrix for female\n","[[415 249]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [166 415]]\n","confusion matrix for male female\n","[[415 249]\n"," [166 415]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[588   0]\n"," [ 84 588]]\n","confusion matrix for female\n","[[420 252]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [168 420]]\n","confusion matrix for male female\n","[[420 252]\n"," [168 420]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[595   0]\n"," [ 85 595]]\n","confusion matrix for female\n","[[425 255]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [170 425]]\n","confusion matrix for male female\n","[[425 255]\n"," [170 425]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[602   0]\n"," [ 86 602]]\n","confusion matrix for female\n","[[430 258]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [172 430]]\n","confusion matrix for male female\n","[[430 258]\n"," [172 430]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[609   0]\n"," [ 87 609]]\n","confusion matrix for female\n","[[435 261]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [174 435]]\n","confusion matrix for male female\n","[[435 261]\n"," [174 435]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[616   0]\n"," [ 88 616]]\n","confusion matrix for female\n","[[440 264]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [176 440]]\n","confusion matrix for male female\n","[[440 264]\n"," [176 440]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[623   0]\n"," [ 89 623]]\n","confusion matrix for female\n","[[445 267]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [178 445]]\n","confusion matrix for male female\n","[[445 267]\n"," [178 445]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[630   0]\n"," [ 90 630]]\n","confusion matrix for female\n","[[450 270]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [180 450]]\n","confusion matrix for male female\n","[[450 270]\n"," [180 450]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[637   0]\n"," [ 91 637]]\n","confusion matrix for female\n","[[455 273]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [182 455]]\n","confusion matrix for male female\n","[[455 273]\n"," [182 455]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[644   0]\n"," [ 92 644]]\n","confusion matrix for female\n","[[460 276]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [184 460]]\n","confusion matrix for male female\n","[[460 276]\n"," [184 460]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[651   0]\n"," [ 93 651]]\n","confusion matrix for female\n","[[465 279]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [186 465]]\n","confusion matrix for male female\n","[[465 279]\n"," [186 465]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[658   0]\n"," [ 94 658]]\n","confusion matrix for female\n","[[470 282]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [188 470]]\n","confusion matrix for male female\n","[[470 282]\n"," [188 470]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[665   0]\n"," [ 95 665]]\n","confusion matrix for female\n","[[475 285]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [190 475]]\n","confusion matrix for male female\n","[[475 285]\n"," [190 475]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[672   0]\n"," [ 96 672]]\n","confusion matrix for female\n","[[480 288]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [192 480]]\n","confusion matrix for male female\n","[[480 288]\n"," [192 480]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[679   0]\n"," [ 97 679]]\n","confusion matrix for female\n","[[485 291]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [194 485]]\n","confusion matrix for male female\n","[[485 291]\n"," [194 485]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[686   0]\n"," [ 98 686]]\n","confusion matrix for female\n","[[490 294]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [196 490]]\n","confusion matrix for male female\n","[[490 294]\n"," [196 490]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[693   0]\n"," [ 99 693]]\n","confusion matrix for female\n","[[495 297]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [198 495]]\n","confusion matrix for male female\n","[[495 297]\n"," [198 495]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[700   0]\n"," [100 700]]\n","confusion matrix for female\n","[[500 300]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [200 500]]\n","confusion matrix for male female\n","[[500 300]\n"," [200 500]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[707   0]\n"," [101 707]]\n","confusion matrix for female\n","[[505 303]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [202 505]]\n","confusion matrix for male female\n","[[505 303]\n"," [202 505]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[714   0]\n"," [102 714]]\n","confusion matrix for female\n","[[510 306]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [204 510]]\n","confusion matrix for male female\n","[[510 306]\n"," [204 510]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[721   0]\n"," [103 721]]\n","confusion matrix for female\n","[[515 309]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [206 515]]\n","confusion matrix for male female\n","[[515 309]\n"," [206 515]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[728   0]\n"," [104 728]]\n","confusion matrix for female\n","[[520 312]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [208 520]]\n","confusion matrix for male female\n","[[520 312]\n"," [208 520]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[735   0]\n"," [105 735]]\n","confusion matrix for female\n","[[525 315]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [210 525]]\n","confusion matrix for male female\n","[[525 315]\n"," [210 525]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[742   0]\n"," [106 742]]\n","confusion matrix for female\n","[[530 318]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [212 530]]\n","confusion matrix for male female\n","[[530 318]\n"," [212 530]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[749   0]\n"," [107 749]]\n","confusion matrix for female\n","[[535 321]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [214 535]]\n","confusion matrix for male female\n","[[535 321]\n"," [214 535]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[756   0]\n"," [108 756]]\n","confusion matrix for female\n","[[540 324]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [216 540]]\n","confusion matrix for male female\n","[[540 324]\n"," [216 540]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[763   0]\n"," [109 763]]\n","confusion matrix for female\n","[[545 327]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [218 545]]\n","confusion matrix for male female\n","[[545 327]\n"," [218 545]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[770   0]\n"," [110 770]]\n","confusion matrix for female\n","[[550 330]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [220 550]]\n","confusion matrix for male female\n","[[550 330]\n"," [220 550]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[777   0]\n"," [111 777]]\n","confusion matrix for female\n","[[555 333]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [222 555]]\n","confusion matrix for male female\n","[[555 333]\n"," [222 555]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[784   0]\n"," [112 784]]\n","confusion matrix for female\n","[[560 336]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [224 560]]\n","confusion matrix for male female\n","[[560 336]\n"," [224 560]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[791   0]\n"," [113 791]]\n","confusion matrix for female\n","[[565 339]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [226 565]]\n","confusion matrix for male female\n","[[565 339]\n"," [226 565]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[798   0]\n"," [114 798]]\n","confusion matrix for female\n","[[570 342]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [228 570]]\n","confusion matrix for male female\n","[[570 342]\n"," [228 570]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[805   0]\n"," [115 805]]\n","confusion matrix for female\n","[[575 345]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [230 575]]\n","confusion matrix for male female\n","[[575 345]\n"," [230 575]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[812   0]\n"," [116 812]]\n","confusion matrix for female\n","[[580 348]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [232 580]]\n","confusion matrix for male female\n","[[580 348]\n"," [232 580]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[819   0]\n"," [117 819]]\n","confusion matrix for female\n","[[585 351]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [234 585]]\n","confusion matrix for male female\n","[[585 351]\n"," [234 585]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[826   0]\n"," [118 826]]\n","confusion matrix for female\n","[[590 354]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [236 590]]\n","confusion matrix for male female\n","[[590 354]\n"," [236 590]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[833   0]\n"," [119 833]]\n","confusion matrix for female\n","[[595 357]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [238 595]]\n","confusion matrix for male female\n","[[595 357]\n"," [238 595]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[840   0]\n"," [120 840]]\n","confusion matrix for female\n","[[600 360]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [240 600]]\n","confusion matrix for male female\n","[[600 360]\n"," [240 600]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[847   0]\n"," [121 847]]\n","confusion matrix for female\n","[[605 363]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [242 605]]\n","confusion matrix for male female\n","[[605 363]\n"," [242 605]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[854   0]\n"," [122 854]]\n","confusion matrix for female\n","[[610 366]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [244 610]]\n","confusion matrix for male female\n","[[610 366]\n"," [244 610]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[861   0]\n"," [123 861]]\n","confusion matrix for female\n","[[615 369]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [246 615]]\n","confusion matrix for male female\n","[[615 369]\n"," [246 615]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[868   0]\n"," [124 868]]\n","confusion matrix for female\n","[[620 372]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [248 620]]\n","confusion matrix for male female\n","[[620 372]\n"," [248 620]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[875   0]\n"," [125 875]]\n","confusion matrix for female\n","[[625 375]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [250 625]]\n","confusion matrix for male female\n","[[625 375]\n"," [250 625]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[882   0]\n"," [126 882]]\n","confusion matrix for female\n","[[630 378]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [252 630]]\n","confusion matrix for male female\n","[[630 378]\n"," [252 630]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[889   0]\n"," [127 889]]\n","confusion matrix for female\n","[[635 381]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [254 635]]\n","confusion matrix for male female\n","[[635 381]\n"," [254 635]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[896   0]\n"," [128 896]]\n","confusion matrix for female\n","[[640 384]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [256 640]]\n","confusion matrix for male female\n","[[640 384]\n"," [256 640]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[903   0]\n"," [129 903]]\n","confusion matrix for female\n","[[645 387]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [258 645]]\n","confusion matrix for male female\n","[[645 387]\n"," [258 645]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[910   0]\n"," [130 910]]\n","confusion matrix for female\n","[[650 390]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [260 650]]\n","confusion matrix for male female\n","[[650 390]\n"," [260 650]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[917   0]\n"," [131 917]]\n","confusion matrix for female\n","[[655 393]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [262 655]]\n","confusion matrix for male female\n","[[655 393]\n"," [262 655]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[924   0]\n"," [132 924]]\n","confusion matrix for female\n","[[660 396]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [264 660]]\n","confusion matrix for male female\n","[[660 396]\n"," [264 660]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[931   0]\n"," [133 931]]\n","confusion matrix for female\n","[[665 399]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [266 665]]\n","confusion matrix for male female\n","[[665 399]\n"," [266 665]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[938   0]\n"," [134 938]]\n","confusion matrix for female\n","[[670 402]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [268 670]]\n","confusion matrix for male female\n","[[670 402]\n"," [268 670]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[945   0]\n"," [135 945]]\n","confusion matrix for female\n","[[675 405]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [270 675]]\n","confusion matrix for male female\n","[[675 405]\n"," [270 675]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[952   0]\n"," [136 952]]\n","confusion matrix for female\n","[[680 408]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [272 680]]\n","confusion matrix for male female\n","[[680 408]\n"," [272 680]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[959   0]\n"," [137 959]]\n","confusion matrix for female\n","[[685 411]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [274 685]]\n","confusion matrix for male female\n","[[685 411]\n"," [274 685]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[966   0]\n"," [138 966]]\n","confusion matrix for female\n","[[690 414]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [276 690]]\n","confusion matrix for male female\n","[[690 414]\n"," [276 690]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[973   0]\n"," [139 973]]\n","confusion matrix for female\n","[[695 417]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [278 695]]\n","confusion matrix for male female\n","[[695 417]\n"," [278 695]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[980   0]\n"," [140 980]]\n","confusion matrix for female\n","[[700 420]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [280 700]]\n","confusion matrix for male female\n","[[700 420]\n"," [280 700]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[987   0]\n"," [141 987]]\n","confusion matrix for female\n","[[705 423]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [282 705]]\n","confusion matrix for male female\n","[[705 423]\n"," [282 705]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[994   0]\n"," [142 994]]\n","confusion matrix for female\n","[[710 426]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [284 710]]\n","confusion matrix for male female\n","[[710 426]\n"," [284 710]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1001    0]\n"," [ 143 1001]]\n","confusion matrix for female\n","[[715 429]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [286 715]]\n","confusion matrix for male female\n","[[715 429]\n"," [286 715]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1008    0]\n"," [ 144 1008]]\n","confusion matrix for female\n","[[720 432]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [288 720]]\n","confusion matrix for male female\n","[[720 432]\n"," [288 720]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1015    0]\n"," [ 145 1015]]\n","confusion matrix for female\n","[[725 435]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [290 725]]\n","confusion matrix for male female\n","[[725 435]\n"," [290 725]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1022    0]\n"," [ 146 1022]]\n","confusion matrix for female\n","[[730 438]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [292 730]]\n","confusion matrix for male female\n","[[730 438]\n"," [292 730]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1029    0]\n"," [ 147 1029]]\n","confusion matrix for female\n","[[735 441]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [294 735]]\n","confusion matrix for male female\n","[[735 441]\n"," [294 735]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1036    0]\n"," [ 148 1036]]\n","confusion matrix for female\n","[[740 444]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [296 740]]\n","confusion matrix for male female\n","[[740 444]\n"," [296 740]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1043    0]\n"," [ 149 1043]]\n","confusion matrix for female\n","[[745 447]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [298 745]]\n","confusion matrix for male female\n","[[745 447]\n"," [298 745]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1050    0]\n"," [ 150 1050]]\n","confusion matrix for female\n","[[750 450]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [300 750]]\n","confusion matrix for male female\n","[[750 450]\n"," [300 750]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1057    0]\n"," [ 151 1057]]\n","confusion matrix for female\n","[[755 453]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [302 755]]\n","confusion matrix for male female\n","[[755 453]\n"," [302 755]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1064    0]\n"," [ 152 1064]]\n","confusion matrix for female\n","[[760 456]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [304 760]]\n","confusion matrix for male female\n","[[760 456]\n"," [304 760]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1071    0]\n"," [ 153 1071]]\n","confusion matrix for female\n","[[765 459]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [306 765]]\n","confusion matrix for male female\n","[[765 459]\n"," [306 765]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1078    0]\n"," [ 154 1078]]\n","confusion matrix for female\n","[[770 462]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [308 770]]\n","confusion matrix for male female\n","[[770 462]\n"," [308 770]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1085    0]\n"," [ 155 1085]]\n","confusion matrix for female\n","[[775 465]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [310 775]]\n","confusion matrix for male female\n","[[775 465]\n"," [310 775]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1092    0]\n"," [ 156 1092]]\n","confusion matrix for female\n","[[780 468]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [312 780]]\n","confusion matrix for male female\n","[[780 468]\n"," [312 780]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1099    0]\n"," [ 157 1099]]\n","confusion matrix for female\n","[[785 471]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [314 785]]\n","confusion matrix for male female\n","[[785 471]\n"," [314 785]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1106    0]\n"," [ 158 1106]]\n","confusion matrix for female\n","[[790 474]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [316 790]]\n","confusion matrix for male female\n","[[790 474]\n"," [316 790]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1113    0]\n"," [ 159 1113]]\n","confusion matrix for female\n","[[795 477]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [318 795]]\n","confusion matrix for male female\n","[[795 477]\n"," [318 795]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1120    0]\n"," [ 160 1120]]\n","confusion matrix for female\n","[[800 480]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [320 800]]\n","confusion matrix for male female\n","[[800 480]\n"," [320 800]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1127    0]\n"," [ 161 1127]]\n","confusion matrix for female\n","[[805 483]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [322 805]]\n","confusion matrix for male female\n","[[805 483]\n"," [322 805]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1134    0]\n"," [ 162 1134]]\n","confusion matrix for female\n","[[810 486]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [324 810]]\n","confusion matrix for male female\n","[[810 486]\n"," [324 810]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1141    0]\n"," [ 163 1141]]\n","confusion matrix for female\n","[[815 489]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [326 815]]\n","confusion matrix for male female\n","[[815 489]\n"," [326 815]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1148    0]\n"," [ 164 1148]]\n","confusion matrix for female\n","[[820 492]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [328 820]]\n","confusion matrix for male female\n","[[820 492]\n"," [328 820]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1155    0]\n"," [ 165 1155]]\n","confusion matrix for female\n","[[825 495]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [330 825]]\n","confusion matrix for male female\n","[[825 495]\n"," [330 825]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1162    0]\n"," [ 166 1162]]\n","confusion matrix for female\n","[[830 498]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [332 830]]\n","confusion matrix for male female\n","[[830 498]\n"," [332 830]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1169    0]\n"," [ 167 1169]]\n","confusion matrix for female\n","[[835 501]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [334 835]]\n","confusion matrix for male female\n","[[835 501]\n"," [334 835]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1176    0]\n"," [ 168 1176]]\n","confusion matrix for female\n","[[840 504]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [336 840]]\n","confusion matrix for male female\n","[[840 504]\n"," [336 840]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1183    0]\n"," [ 169 1183]]\n","confusion matrix for female\n","[[845 507]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [338 845]]\n","confusion matrix for male female\n","[[845 507]\n"," [338 845]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1190    0]\n"," [ 170 1190]]\n","confusion matrix for female\n","[[850 510]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [340 850]]\n","confusion matrix for male female\n","[[850 510]\n"," [340 850]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1197    0]\n"," [ 171 1197]]\n","confusion matrix for female\n","[[855 513]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [342 855]]\n","confusion matrix for male female\n","[[855 513]\n"," [342 855]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1204    0]\n"," [ 172 1204]]\n","confusion matrix for female\n","[[860 516]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [344 860]]\n","confusion matrix for male female\n","[[860 516]\n"," [344 860]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1211    0]\n"," [ 173 1211]]\n","confusion matrix for female\n","[[865 519]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [346 865]]\n","confusion matrix for male female\n","[[865 519]\n"," [346 865]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1218    0]\n"," [ 174 1218]]\n","confusion matrix for female\n","[[870 522]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [348 870]]\n","confusion matrix for male female\n","[[870 522]\n"," [348 870]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1225    0]\n"," [ 175 1225]]\n","confusion matrix for female\n","[[875 525]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [350 875]]\n","confusion matrix for male female\n","[[875 525]\n"," [350 875]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1232    0]\n"," [ 176 1232]]\n","confusion matrix for female\n","[[880 528]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [352 880]]\n","confusion matrix for male female\n","[[880 528]\n"," [352 880]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1239    0]\n"," [ 177 1239]]\n","confusion matrix for female\n","[[885 531]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [354 885]]\n","confusion matrix for male female\n","[[885 531]\n"," [354 885]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1246    0]\n"," [ 178 1246]]\n","confusion matrix for female\n","[[890 534]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [356 890]]\n","confusion matrix for male female\n","[[890 534]\n"," [356 890]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1253    0]\n"," [ 179 1253]]\n","confusion matrix for female\n","[[895 537]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [358 895]]\n","confusion matrix for male female\n","[[895 537]\n"," [358 895]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1260    0]\n"," [ 180 1260]]\n","confusion matrix for female\n","[[900 540]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [360 900]]\n","confusion matrix for male female\n","[[900 540]\n"," [360 900]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1267    0]\n"," [ 181 1267]]\n","confusion matrix for female\n","[[905 543]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [362 905]]\n","confusion matrix for male female\n","[[905 543]\n"," [362 905]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1274    0]\n"," [ 182 1274]]\n","confusion matrix for female\n","[[910 546]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [364 910]]\n","confusion matrix for male female\n","[[910 546]\n"," [364 910]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1281    0]\n"," [ 183 1281]]\n","confusion matrix for female\n","[[915 549]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [366 915]]\n","confusion matrix for male female\n","[[915 549]\n"," [366 915]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1288    0]\n"," [ 184 1288]]\n","confusion matrix for female\n","[[920 552]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [368 920]]\n","confusion matrix for male female\n","[[920 552]\n"," [368 920]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1295    0]\n"," [ 185 1295]]\n","confusion matrix for female\n","[[925 555]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [370 925]]\n","confusion matrix for male female\n","[[925 555]\n"," [370 925]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1302    0]\n"," [ 186 1302]]\n","confusion matrix for female\n","[[930 558]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [372 930]]\n","confusion matrix for male female\n","[[930 558]\n"," [372 930]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1309    0]\n"," [ 187 1309]]\n","confusion matrix for female\n","[[935 561]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [374 935]]\n","confusion matrix for male female\n","[[935 561]\n"," [374 935]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1316    0]\n"," [ 188 1316]]\n","confusion matrix for female\n","[[940 564]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [376 940]]\n","confusion matrix for male female\n","[[940 564]\n"," [376 940]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1323    0]\n"," [ 189 1323]]\n","confusion matrix for female\n","[[945 567]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [378 945]]\n","confusion matrix for male female\n","[[945 567]\n"," [378 945]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1330    0]\n"," [ 190 1330]]\n","confusion matrix for female\n","[[950 570]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [380 950]]\n","confusion matrix for male female\n","[[950 570]\n"," [380 950]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1337    0]\n"," [ 191 1337]]\n","confusion matrix for female\n","[[955 573]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [382 955]]\n","confusion matrix for male female\n","[[955 573]\n"," [382 955]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1344    0]\n"," [ 192 1344]]\n","confusion matrix for female\n","[[960 576]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [384 960]]\n","confusion matrix for male female\n","[[960 576]\n"," [384 960]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1351    0]\n"," [ 193 1351]]\n","confusion matrix for female\n","[[965 579]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [386 965]]\n","confusion matrix for male female\n","[[965 579]\n"," [386 965]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1358    0]\n"," [ 194 1358]]\n","confusion matrix for female\n","[[970 582]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [388 970]]\n","confusion matrix for male female\n","[[970 582]\n"," [388 970]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1365    0]\n"," [ 195 1365]]\n","confusion matrix for female\n","[[975 585]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [390 975]]\n","confusion matrix for male female\n","[[975 585]\n"," [390 975]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1372    0]\n"," [ 196 1372]]\n","confusion matrix for female\n","[[980 588]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [392 980]]\n","confusion matrix for male female\n","[[980 588]\n"," [392 980]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1379    0]\n"," [ 197 1379]]\n","confusion matrix for female\n","[[985 591]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [394 985]]\n","confusion matrix for male female\n","[[985 591]\n"," [394 985]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1386    0]\n"," [ 198 1386]]\n","confusion matrix for female\n","[[990 594]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [396 990]]\n","confusion matrix for male female\n","[[990 594]\n"," [396 990]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1393    0]\n"," [ 199 1393]]\n","confusion matrix for female\n","[[995 597]\n"," [  0   0]]\n","confusion matrix for male\n","[[  0   0]\n"," [398 995]]\n","confusion matrix for male female\n","[[995 597]\n"," [398 995]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1400    0]\n"," [ 200 1400]]\n","confusion matrix for female\n","[[1000  600]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 400 1000]]\n","confusion matrix for male female\n","[[1000  600]\n"," [ 400 1000]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1407    0]\n"," [ 201 1407]]\n","confusion matrix for female\n","[[1005  603]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 402 1005]]\n","confusion matrix for male female\n","[[1005  603]\n"," [ 402 1005]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1414    0]\n"," [ 202 1414]]\n","confusion matrix for female\n","[[1010  606]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 404 1010]]\n","confusion matrix for male female\n","[[1010  606]\n"," [ 404 1010]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1421    0]\n"," [ 203 1421]]\n","confusion matrix for female\n","[[1015  609]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 406 1015]]\n","confusion matrix for male female\n","[[1015  609]\n"," [ 406 1015]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1428    0]\n"," [ 204 1428]]\n","confusion matrix for female\n","[[1020  612]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 408 1020]]\n","confusion matrix for male female\n","[[1020  612]\n"," [ 408 1020]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1435    0]\n"," [ 205 1435]]\n","confusion matrix for female\n","[[1025  615]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 410 1025]]\n","confusion matrix for male female\n","[[1025  615]\n"," [ 410 1025]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1442    0]\n"," [ 206 1442]]\n","confusion matrix for female\n","[[1030  618]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 412 1030]]\n","confusion matrix for male female\n","[[1030  618]\n"," [ 412 1030]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1449    0]\n"," [ 207 1449]]\n","confusion matrix for female\n","[[1035  621]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 414 1035]]\n","confusion matrix for male female\n","[[1035  621]\n"," [ 414 1035]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1456    0]\n"," [ 208 1456]]\n","confusion matrix for female\n","[[1040  624]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 416 1040]]\n","confusion matrix for male female\n","[[1040  624]\n"," [ 416 1040]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1463    0]\n"," [ 209 1463]]\n","confusion matrix for female\n","[[1045  627]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 418 1045]]\n","confusion matrix for male female\n","[[1045  627]\n"," [ 418 1045]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1470    0]\n"," [ 210 1470]]\n","confusion matrix for female\n","[[1050  630]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 420 1050]]\n","confusion matrix for male female\n","[[1050  630]\n"," [ 420 1050]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1477    0]\n"," [ 211 1477]]\n","confusion matrix for female\n","[[1055  633]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 422 1055]]\n","confusion matrix for male female\n","[[1055  633]\n"," [ 422 1055]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1484    0]\n"," [ 212 1484]]\n","confusion matrix for female\n","[[1060  636]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 424 1060]]\n","confusion matrix for male female\n","[[1060  636]\n"," [ 424 1060]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1491    0]\n"," [ 213 1491]]\n","confusion matrix for female\n","[[1065  639]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 426 1065]]\n","confusion matrix for male female\n","[[1065  639]\n"," [ 426 1065]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1498    0]\n"," [ 214 1498]]\n","confusion matrix for female\n","[[1070  642]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 428 1070]]\n","confusion matrix for male female\n","[[1070  642]\n"," [ 428 1070]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1505    0]\n"," [ 215 1505]]\n","confusion matrix for female\n","[[1075  645]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 430 1075]]\n","confusion matrix for male female\n","[[1075  645]\n"," [ 430 1075]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1512    0]\n"," [ 216 1512]]\n","confusion matrix for female\n","[[1080  648]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 432 1080]]\n","confusion matrix for male female\n","[[1080  648]\n"," [ 432 1080]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1519    0]\n"," [ 217 1519]]\n","confusion matrix for female\n","[[1085  651]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 434 1085]]\n","confusion matrix for male female\n","[[1085  651]\n"," [ 434 1085]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1526    0]\n"," [ 218 1526]]\n","confusion matrix for female\n","[[1090  654]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 436 1090]]\n","confusion matrix for male female\n","[[1090  654]\n"," [ 436 1090]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1533    0]\n"," [ 219 1533]]\n","confusion matrix for female\n","[[1095  657]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 438 1095]]\n","confusion matrix for male female\n","[[1095  657]\n"," [ 438 1095]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1540    0]\n"," [ 220 1540]]\n","confusion matrix for female\n","[[1100  660]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 440 1100]]\n","confusion matrix for male female\n","[[1100  660]\n"," [ 440 1100]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1547    0]\n"," [ 221 1547]]\n","confusion matrix for female\n","[[1105  663]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 442 1105]]\n","confusion matrix for male female\n","[[1105  663]\n"," [ 442 1105]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1554    0]\n"," [ 222 1554]]\n","confusion matrix for female\n","[[1110  666]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 444 1110]]\n","confusion matrix for male female\n","[[1110  666]\n"," [ 444 1110]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1561    0]\n"," [ 223 1561]]\n","confusion matrix for female\n","[[1115  669]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 446 1115]]\n","confusion matrix for male female\n","[[1115  669]\n"," [ 446 1115]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1568    0]\n"," [ 224 1568]]\n","confusion matrix for female\n","[[1120  672]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 448 1120]]\n","confusion matrix for male female\n","[[1120  672]\n"," [ 448 1120]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1575    0]\n"," [ 225 1575]]\n","confusion matrix for female\n","[[1125  675]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 450 1125]]\n","confusion matrix for male female\n","[[1125  675]\n"," [ 450 1125]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1582    0]\n"," [ 226 1582]]\n","confusion matrix for female\n","[[1130  678]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 452 1130]]\n","confusion matrix for male female\n","[[1130  678]\n"," [ 452 1130]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1589    0]\n"," [ 227 1589]]\n","confusion matrix for female\n","[[1135  681]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 454 1135]]\n","confusion matrix for male female\n","[[1135  681]\n"," [ 454 1135]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1596    0]\n"," [ 228 1596]]\n","confusion matrix for female\n","[[1140  684]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 456 1140]]\n","confusion matrix for male female\n","[[1140  684]\n"," [ 456 1140]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1603    0]\n"," [ 229 1603]]\n","confusion matrix for female\n","[[1145  687]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 458 1145]]\n","confusion matrix for male female\n","[[1145  687]\n"," [ 458 1145]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1610    0]\n"," [ 230 1610]]\n","confusion matrix for female\n","[[1150  690]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 460 1150]]\n","confusion matrix for male female\n","[[1150  690]\n"," [ 460 1150]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1617    0]\n"," [ 231 1617]]\n","confusion matrix for female\n","[[1155  693]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 462 1155]]\n","confusion matrix for male female\n","[[1155  693]\n"," [ 462 1155]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1624    0]\n"," [ 232 1624]]\n","confusion matrix for female\n","[[1160  696]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 464 1160]]\n","confusion matrix for male female\n","[[1160  696]\n"," [ 464 1160]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1631    0]\n"," [ 233 1631]]\n","confusion matrix for female\n","[[1165  699]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 466 1165]]\n","confusion matrix for male female\n","[[1165  699]\n"," [ 466 1165]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1638    0]\n"," [ 234 1638]]\n","confusion matrix for female\n","[[1170  702]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 468 1170]]\n","confusion matrix for male female\n","[[1170  702]\n"," [ 468 1170]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1645    0]\n"," [ 235 1645]]\n","confusion matrix for female\n","[[1175  705]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 470 1175]]\n","confusion matrix for male female\n","[[1175  705]\n"," [ 470 1175]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1652    0]\n"," [ 236 1652]]\n","confusion matrix for female\n","[[1180  708]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 472 1180]]\n","confusion matrix for male female\n","[[1180  708]\n"," [ 472 1180]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1659    0]\n"," [ 237 1659]]\n","confusion matrix for female\n","[[1185  711]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 474 1185]]\n","confusion matrix for male female\n","[[1185  711]\n"," [ 474 1185]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1666    0]\n"," [ 238 1666]]\n","confusion matrix for female\n","[[1190  714]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 476 1190]]\n","confusion matrix for male female\n","[[1190  714]\n"," [ 476 1190]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1673    0]\n"," [ 239 1673]]\n","confusion matrix for female\n","[[1195  717]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 478 1195]]\n","confusion matrix for male female\n","[[1195  717]\n"," [ 478 1195]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1680    0]\n"," [ 240 1680]]\n","confusion matrix for female\n","[[1200  720]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 480 1200]]\n","confusion matrix for male female\n","[[1200  720]\n"," [ 480 1200]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1687    0]\n"," [ 241 1687]]\n","confusion matrix for female\n","[[1205  723]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 482 1205]]\n","confusion matrix for male female\n","[[1205  723]\n"," [ 482 1205]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1694    0]\n"," [ 242 1694]]\n","confusion matrix for female\n","[[1210  726]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 484 1210]]\n","confusion matrix for male female\n","[[1210  726]\n"," [ 484 1210]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1701    0]\n"," [ 243 1701]]\n","confusion matrix for female\n","[[1215  729]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 486 1215]]\n","confusion matrix for male female\n","[[1215  729]\n"," [ 486 1215]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1708    0]\n"," [ 244 1708]]\n","confusion matrix for female\n","[[1220  732]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 488 1220]]\n","confusion matrix for male female\n","[[1220  732]\n"," [ 488 1220]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1715    0]\n"," [ 245 1715]]\n","confusion matrix for female\n","[[1225  735]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 490 1225]]\n","confusion matrix for male female\n","[[1225  735]\n"," [ 490 1225]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1722    0]\n"," [ 246 1722]]\n","confusion matrix for female\n","[[1230  738]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 492 1230]]\n","confusion matrix for male female\n","[[1230  738]\n"," [ 492 1230]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1729    0]\n"," [ 247 1729]]\n","confusion matrix for female\n","[[1235  741]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 494 1235]]\n","confusion matrix for male female\n","[[1235  741]\n"," [ 494 1235]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1736    0]\n"," [ 248 1736]]\n","confusion matrix for female\n","[[1240  744]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 496 1240]]\n","confusion matrix for male female\n","[[1240  744]\n"," [ 496 1240]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1743    0]\n"," [ 249 1743]]\n","confusion matrix for female\n","[[1245  747]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 498 1245]]\n","confusion matrix for male female\n","[[1245  747]\n"," [ 498 1245]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1750    0]\n"," [ 250 1750]]\n","confusion matrix for female\n","[[1250  750]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 500 1250]]\n","confusion matrix for male female\n","[[1250  750]\n"," [ 500 1250]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1757    0]\n"," [ 251 1757]]\n","confusion matrix for female\n","[[1255  753]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 502 1255]]\n","confusion matrix for male female\n","[[1255  753]\n"," [ 502 1255]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1764    0]\n"," [ 252 1764]]\n","confusion matrix for female\n","[[1260  756]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 504 1260]]\n","confusion matrix for male female\n","[[1260  756]\n"," [ 504 1260]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1771    0]\n"," [ 253 1771]]\n","confusion matrix for female\n","[[1265  759]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 506 1265]]\n","confusion matrix for male female\n","[[1265  759]\n"," [ 506 1265]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1778    0]\n"," [ 254 1778]]\n","confusion matrix for female\n","[[1270  762]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 508 1270]]\n","confusion matrix for male female\n","[[1270  762]\n"," [ 508 1270]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1785    0]\n"," [ 255 1785]]\n","confusion matrix for female\n","[[1275  765]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 510 1275]]\n","confusion matrix for male female\n","[[1275  765]\n"," [ 510 1275]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1792    0]\n"," [ 256 1792]]\n","confusion matrix for female\n","[[1280  768]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 512 1280]]\n","confusion matrix for male female\n","[[1280  768]\n"," [ 512 1280]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1799    0]\n"," [ 257 1799]]\n","confusion matrix for female\n","[[1285  771]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 514 1285]]\n","confusion matrix for male female\n","[[1285  771]\n"," [ 514 1285]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1806    0]\n"," [ 258 1806]]\n","confusion matrix for female\n","[[1290  774]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 516 1290]]\n","confusion matrix for male female\n","[[1290  774]\n"," [ 516 1290]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1813    0]\n"," [ 259 1813]]\n","confusion matrix for female\n","[[1295  777]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 518 1295]]\n","confusion matrix for male female\n","[[1295  777]\n"," [ 518 1295]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1820    0]\n"," [ 260 1820]]\n","confusion matrix for female\n","[[1300  780]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 520 1300]]\n","confusion matrix for male female\n","[[1300  780]\n"," [ 520 1300]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1827    0]\n"," [ 261 1827]]\n","confusion matrix for female\n","[[1305  783]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 522 1305]]\n","confusion matrix for male female\n","[[1305  783]\n"," [ 522 1305]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1834    0]\n"," [ 262 1834]]\n","confusion matrix for female\n","[[1310  786]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 524 1310]]\n","confusion matrix for male female\n","[[1310  786]\n"," [ 524 1310]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1841    0]\n"," [ 263 1841]]\n","confusion matrix for female\n","[[1315  789]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 526 1315]]\n","confusion matrix for male female\n","[[1315  789]\n"," [ 526 1315]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1848    0]\n"," [ 264 1848]]\n","confusion matrix for female\n","[[1320  792]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 528 1320]]\n","confusion matrix for male female\n","[[1320  792]\n"," [ 528 1320]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1855    0]\n"," [ 265 1855]]\n","confusion matrix for female\n","[[1325  795]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 530 1325]]\n","confusion matrix for male female\n","[[1325  795]\n"," [ 530 1325]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1862    0]\n"," [ 266 1862]]\n","confusion matrix for female\n","[[1330  798]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 532 1330]]\n","confusion matrix for male female\n","[[1330  798]\n"," [ 532 1330]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1869    0]\n"," [ 267 1869]]\n","confusion matrix for female\n","[[1335  801]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 534 1335]]\n","confusion matrix for male female\n","[[1335  801]\n"," [ 534 1335]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1876    0]\n"," [ 268 1876]]\n","confusion matrix for female\n","[[1340  804]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 536 1340]]\n","confusion matrix for male female\n","[[1340  804]\n"," [ 536 1340]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1883    0]\n"," [ 269 1883]]\n","confusion matrix for female\n","[[1345  807]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 538 1345]]\n","confusion matrix for male female\n","[[1345  807]\n"," [ 538 1345]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1890    0]\n"," [ 270 1890]]\n","confusion matrix for female\n","[[1350  810]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 540 1350]]\n","confusion matrix for male female\n","[[1350  810]\n"," [ 540 1350]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1897    0]\n"," [ 271 1897]]\n","confusion matrix for female\n","[[1355  813]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 542 1355]]\n","confusion matrix for male female\n","[[1355  813]\n"," [ 542 1355]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1904    0]\n"," [ 272 1904]]\n","confusion matrix for female\n","[[1360  816]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 544 1360]]\n","confusion matrix for male female\n","[[1360  816]\n"," [ 544 1360]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1911    0]\n"," [ 273 1911]]\n","confusion matrix for female\n","[[1365  819]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 546 1365]]\n","confusion matrix for male female\n","[[1365  819]\n"," [ 546 1365]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1918    0]\n"," [ 274 1918]]\n","confusion matrix for female\n","[[1370  822]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 548 1370]]\n","confusion matrix for male female\n","[[1370  822]\n"," [ 548 1370]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1925    0]\n"," [ 275 1925]]\n","confusion matrix for female\n","[[1375  825]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 550 1375]]\n","confusion matrix for male female\n","[[1375  825]\n"," [ 550 1375]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1932    0]\n"," [ 276 1932]]\n","confusion matrix for female\n","[[1380  828]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 552 1380]]\n","confusion matrix for male female\n","[[1380  828]\n"," [ 552 1380]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1939    0]\n"," [ 277 1939]]\n","confusion matrix for female\n","[[1385  831]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 554 1385]]\n","confusion matrix for male female\n","[[1385  831]\n"," [ 554 1385]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1946    0]\n"," [ 278 1946]]\n","confusion matrix for female\n","[[1390  834]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 556 1390]]\n","confusion matrix for male female\n","[[1390  834]\n"," [ 556 1390]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1953    0]\n"," [ 279 1953]]\n","confusion matrix for female\n","[[1395  837]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 558 1395]]\n","confusion matrix for male female\n","[[1395  837]\n"," [ 558 1395]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1960    0]\n"," [ 280 1960]]\n","confusion matrix for female\n","[[1400  840]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 560 1400]]\n","confusion matrix for male female\n","[[1400  840]\n"," [ 560 1400]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1967    0]\n"," [ 281 1967]]\n","confusion matrix for female\n","[[1405  843]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 562 1405]]\n","confusion matrix for male female\n","[[1405  843]\n"," [ 562 1405]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1974    0]\n"," [ 282 1974]]\n","confusion matrix for female\n","[[1410  846]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 564 1410]]\n","confusion matrix for male female\n","[[1410  846]\n"," [ 564 1410]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1981    0]\n"," [ 283 1981]]\n","confusion matrix for female\n","[[1415  849]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 566 1415]]\n","confusion matrix for male female\n","[[1415  849]\n"," [ 566 1415]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1988    0]\n"," [ 284 1988]]\n","confusion matrix for female\n","[[1420  852]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 568 1420]]\n","confusion matrix for male female\n","[[1420  852]\n"," [ 568 1420]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[1995    0]\n"," [ 285 1995]]\n","confusion matrix for female\n","[[1425  855]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 570 1425]]\n","confusion matrix for male female\n","[[1425  855]\n"," [ 570 1425]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2002    0]\n"," [ 286 2002]]\n","confusion matrix for female\n","[[1430  858]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 572 1430]]\n","confusion matrix for male female\n","[[1430  858]\n"," [ 572 1430]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2009    0]\n"," [ 287 2009]]\n","confusion matrix for female\n","[[1435  861]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 574 1435]]\n","confusion matrix for male female\n","[[1435  861]\n"," [ 574 1435]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2016    0]\n"," [ 288 2016]]\n","confusion matrix for female\n","[[1440  864]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 576 1440]]\n","confusion matrix for male female\n","[[1440  864]\n"," [ 576 1440]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2023    0]\n"," [ 289 2023]]\n","confusion matrix for female\n","[[1445  867]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 578 1445]]\n","confusion matrix for male female\n","[[1445  867]\n"," [ 578 1445]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2030    0]\n"," [ 290 2030]]\n","confusion matrix for female\n","[[1450  870]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 580 1450]]\n","confusion matrix for male female\n","[[1450  870]\n"," [ 580 1450]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2037    0]\n"," [ 291 2037]]\n","confusion matrix for female\n","[[1455  873]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 582 1455]]\n","confusion matrix for male female\n","[[1455  873]\n"," [ 582 1455]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2044    0]\n"," [ 292 2044]]\n","confusion matrix for female\n","[[1460  876]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 584 1460]]\n","confusion matrix for male female\n","[[1460  876]\n"," [ 584 1460]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2051    0]\n"," [ 293 2051]]\n","confusion matrix for female\n","[[1465  879]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 586 1465]]\n","confusion matrix for male female\n","[[1465  879]\n"," [ 586 1465]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2058    0]\n"," [ 294 2058]]\n","confusion matrix for female\n","[[1470  882]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 588 1470]]\n","confusion matrix for male female\n","[[1470  882]\n"," [ 588 1470]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2065    0]\n"," [ 295 2065]]\n","confusion matrix for female\n","[[1475  885]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 590 1475]]\n","confusion matrix for male female\n","[[1475  885]\n"," [ 590 1475]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2072    0]\n"," [ 296 2072]]\n","confusion matrix for female\n","[[1480  888]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 592 1480]]\n","confusion matrix for male female\n","[[1480  888]\n"," [ 592 1480]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2079    0]\n"," [ 297 2079]]\n","confusion matrix for female\n","[[1485  891]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 594 1485]]\n","confusion matrix for male female\n","[[1485  891]\n"," [ 594 1485]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2086    0]\n"," [ 298 2086]]\n","confusion matrix for female\n","[[1490  894]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 596 1490]]\n","confusion matrix for male female\n","[[1490  894]\n"," [ 596 1490]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2093    0]\n"," [ 299 2093]]\n","confusion matrix for female\n","[[1495  897]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 598 1495]]\n","confusion matrix for male female\n","[[1495  897]\n"," [ 598 1495]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2100    0]\n"," [ 300 2100]]\n","confusion matrix for female\n","[[1500  900]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 600 1500]]\n","confusion matrix for male female\n","[[1500  900]\n"," [ 600 1500]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2107    0]\n"," [ 301 2107]]\n","confusion matrix for female\n","[[1505  903]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 602 1505]]\n","confusion matrix for male female\n","[[1505  903]\n"," [ 602 1505]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2114    0]\n"," [ 302 2114]]\n","confusion matrix for female\n","[[1510  906]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 604 1510]]\n","confusion matrix for male female\n","[[1510  906]\n"," [ 604 1510]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2121    0]\n"," [ 303 2121]]\n","confusion matrix for female\n","[[1515  909]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 606 1515]]\n","confusion matrix for male female\n","[[1515  909]\n"," [ 606 1515]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2128    0]\n"," [ 304 2128]]\n","confusion matrix for female\n","[[1520  912]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 608 1520]]\n","confusion matrix for male female\n","[[1520  912]\n"," [ 608 1520]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2135    0]\n"," [ 305 2135]]\n","confusion matrix for female\n","[[1525  915]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 610 1525]]\n","confusion matrix for male female\n","[[1525  915]\n"," [ 610 1525]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2142    0]\n"," [ 306 2142]]\n","confusion matrix for female\n","[[1530  918]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 612 1530]]\n","confusion matrix for male female\n","[[1530  918]\n"," [ 612 1530]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2149    0]\n"," [ 307 2149]]\n","confusion matrix for female\n","[[1535  921]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 614 1535]]\n","confusion matrix for male female\n","[[1535  921]\n"," [ 614 1535]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2156    0]\n"," [ 308 2156]]\n","confusion matrix for female\n","[[1540  924]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 616 1540]]\n","confusion matrix for male female\n","[[1540  924]\n"," [ 616 1540]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2163    0]\n"," [ 309 2163]]\n","confusion matrix for female\n","[[1545  927]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 618 1545]]\n","confusion matrix for male female\n","[[1545  927]\n"," [ 618 1545]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2170    0]\n"," [ 310 2170]]\n","confusion matrix for female\n","[[1550  930]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 620 1550]]\n","confusion matrix for male female\n","[[1550  930]\n"," [ 620 1550]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2177    0]\n"," [ 311 2177]]\n","confusion matrix for female\n","[[1555  933]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 622 1555]]\n","confusion matrix for male female\n","[[1555  933]\n"," [ 622 1555]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2184    0]\n"," [ 312 2184]]\n","confusion matrix for female\n","[[1560  936]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 624 1560]]\n","confusion matrix for male female\n","[[1560  936]\n"," [ 624 1560]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2191    0]\n"," [ 313 2191]]\n","confusion matrix for female\n","[[1565  939]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 626 1565]]\n","confusion matrix for male female\n","[[1565  939]\n"," [ 626 1565]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2198    0]\n"," [ 314 2198]]\n","confusion matrix for female\n","[[1570  942]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 628 1570]]\n","confusion matrix for male female\n","[[1570  942]\n"," [ 628 1570]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2205    0]\n"," [ 315 2205]]\n","confusion matrix for female\n","[[1575  945]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 630 1575]]\n","confusion matrix for male female\n","[[1575  945]\n"," [ 630 1575]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2212    0]\n"," [ 316 2212]]\n","confusion matrix for female\n","[[1580  948]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 632 1580]]\n","confusion matrix for male female\n","[[1580  948]\n"," [ 632 1580]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2219    0]\n"," [ 317 2219]]\n","confusion matrix for female\n","[[1585  951]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 634 1585]]\n","confusion matrix for male female\n","[[1585  951]\n"," [ 634 1585]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2226    0]\n"," [ 318 2226]]\n","confusion matrix for female\n","[[1590  954]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 636 1590]]\n","confusion matrix for male female\n","[[1590  954]\n"," [ 636 1590]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2233    0]\n"," [ 319 2233]]\n","confusion matrix for female\n","[[1595  957]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 638 1595]]\n","confusion matrix for male female\n","[[1595  957]\n"," [ 638 1595]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2240    0]\n"," [ 320 2240]]\n","confusion matrix for female\n","[[1600  960]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 640 1600]]\n","confusion matrix for male female\n","[[1600  960]\n"," [ 640 1600]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2247    0]\n"," [ 321 2247]]\n","confusion matrix for female\n","[[1605  963]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 642 1605]]\n","confusion matrix for male female\n","[[1605  963]\n"," [ 642 1605]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2254    0]\n"," [ 322 2254]]\n","confusion matrix for female\n","[[1610  966]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 644 1610]]\n","confusion matrix for male female\n","[[1610  966]\n"," [ 644 1610]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2261    0]\n"," [ 323 2261]]\n","confusion matrix for female\n","[[1615  969]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 646 1615]]\n","confusion matrix for male female\n","[[1615  969]\n"," [ 646 1615]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2268    0]\n"," [ 324 2268]]\n","confusion matrix for female\n","[[1620  972]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 648 1620]]\n","confusion matrix for male female\n","[[1620  972]\n"," [ 648 1620]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2275    0]\n"," [ 325 2275]]\n","confusion matrix for female\n","[[1625  975]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 650 1625]]\n","confusion matrix for male female\n","[[1625  975]\n"," [ 650 1625]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2282    0]\n"," [ 326 2282]]\n","confusion matrix for female\n","[[1630  978]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 652 1630]]\n","confusion matrix for male female\n","[[1630  978]\n"," [ 652 1630]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2289    0]\n"," [ 327 2289]]\n","confusion matrix for female\n","[[1635  981]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 654 1635]]\n","confusion matrix for male female\n","[[1635  981]\n"," [ 654 1635]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2296    0]\n"," [ 328 2296]]\n","confusion matrix for female\n","[[1640  984]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 656 1640]]\n","confusion matrix for male female\n","[[1640  984]\n"," [ 656 1640]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2303    0]\n"," [ 329 2303]]\n","confusion matrix for female\n","[[1645  987]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 658 1645]]\n","confusion matrix for male female\n","[[1645  987]\n"," [ 658 1645]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2310    0]\n"," [ 330 2310]]\n","confusion matrix for female\n","[[1650  990]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 660 1650]]\n","confusion matrix for male female\n","[[1650  990]\n"," [ 660 1650]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2317    0]\n"," [ 331 2317]]\n","confusion matrix for female\n","[[1655  993]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 662 1655]]\n","confusion matrix for male female\n","[[1655  993]\n"," [ 662 1655]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2324    0]\n"," [ 332 2324]]\n","confusion matrix for female\n","[[1660  996]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 664 1660]]\n","confusion matrix for male female\n","[[1660  996]\n"," [ 664 1660]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2331    0]\n"," [ 333 2331]]\n","confusion matrix for female\n","[[1665  999]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 666 1665]]\n","confusion matrix for male female\n","[[1665  999]\n"," [ 666 1665]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2338    0]\n"," [ 334 2338]]\n","confusion matrix for female\n","[[1670 1002]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 668 1670]]\n","confusion matrix for male female\n","[[1670 1002]\n"," [ 668 1670]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2345    0]\n"," [ 335 2345]]\n","confusion matrix for female\n","[[1675 1005]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 670 1675]]\n","confusion matrix for male female\n","[[1675 1005]\n"," [ 670 1675]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2352    0]\n"," [ 336 2352]]\n","confusion matrix for female\n","[[1680 1008]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 672 1680]]\n","confusion matrix for male female\n","[[1680 1008]\n"," [ 672 1680]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2359    0]\n"," [ 337 2359]]\n","confusion matrix for female\n","[[1685 1011]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 674 1685]]\n","confusion matrix for male female\n","[[1685 1011]\n"," [ 674 1685]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2366    0]\n"," [ 338 2366]]\n","confusion matrix for female\n","[[1690 1014]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 676 1690]]\n","confusion matrix for male female\n","[[1690 1014]\n"," [ 676 1690]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2373    0]\n"," [ 339 2373]]\n","confusion matrix for female\n","[[1695 1017]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 678 1695]]\n","confusion matrix for male female\n","[[1695 1017]\n"," [ 678 1695]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2380    0]\n"," [ 340 2380]]\n","confusion matrix for female\n","[[1700 1020]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 680 1700]]\n","confusion matrix for male female\n","[[1700 1020]\n"," [ 680 1700]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2387    0]\n"," [ 341 2387]]\n","confusion matrix for female\n","[[1705 1023]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 682 1705]]\n","confusion matrix for male female\n","[[1705 1023]\n"," [ 682 1705]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2394    0]\n"," [ 342 2394]]\n","confusion matrix for female\n","[[1710 1026]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 684 1710]]\n","confusion matrix for male female\n","[[1710 1026]\n"," [ 684 1710]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2401    0]\n"," [ 343 2401]]\n","confusion matrix for female\n","[[1715 1029]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 686 1715]]\n","confusion matrix for male female\n","[[1715 1029]\n"," [ 686 1715]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2408    0]\n"," [ 344 2408]]\n","confusion matrix for female\n","[[1720 1032]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 688 1720]]\n","confusion matrix for male female\n","[[1720 1032]\n"," [ 688 1720]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2415    0]\n"," [ 345 2415]]\n","confusion matrix for female\n","[[1725 1035]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 690 1725]]\n","confusion matrix for male female\n","[[1725 1035]\n"," [ 690 1725]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2422    0]\n"," [ 346 2422]]\n","confusion matrix for female\n","[[1730 1038]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 692 1730]]\n","confusion matrix for male female\n","[[1730 1038]\n"," [ 692 1730]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2429    0]\n"," [ 347 2429]]\n","confusion matrix for female\n","[[1735 1041]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 694 1735]]\n","confusion matrix for male female\n","[[1735 1041]\n"," [ 694 1735]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2436    0]\n"," [ 348 2436]]\n","confusion matrix for female\n","[[1740 1044]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 696 1740]]\n","confusion matrix for male female\n","[[1740 1044]\n"," [ 696 1740]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2443    0]\n"," [ 349 2443]]\n","confusion matrix for female\n","[[1745 1047]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 698 1745]]\n","confusion matrix for male female\n","[[1745 1047]\n"," [ 698 1745]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2450    0]\n"," [ 350 2450]]\n","confusion matrix for female\n","[[1750 1050]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 700 1750]]\n","confusion matrix for male female\n","[[1750 1050]\n"," [ 700 1750]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2457    0]\n"," [ 351 2457]]\n","confusion matrix for female\n","[[1755 1053]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 702 1755]]\n","confusion matrix for male female\n","[[1755 1053]\n"," [ 702 1755]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2464    0]\n"," [ 352 2464]]\n","confusion matrix for female\n","[[1760 1056]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 704 1760]]\n","confusion matrix for male female\n","[[1760 1056]\n"," [ 704 1760]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2471    0]\n"," [ 353 2471]]\n","confusion matrix for female\n","[[1765 1059]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 706 1765]]\n","confusion matrix for male female\n","[[1765 1059]\n"," [ 706 1765]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2478    0]\n"," [ 354 2478]]\n","confusion matrix for female\n","[[1770 1062]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 708 1770]]\n","confusion matrix for male female\n","[[1770 1062]\n"," [ 708 1770]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2485    0]\n"," [ 355 2485]]\n","confusion matrix for female\n","[[1775 1065]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 710 1775]]\n","confusion matrix for male female\n","[[1775 1065]\n"," [ 710 1775]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2492    0]\n"," [ 356 2492]]\n","confusion matrix for female\n","[[1780 1068]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 712 1780]]\n","confusion matrix for male female\n","[[1780 1068]\n"," [ 712 1780]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2499    0]\n"," [ 357 2499]]\n","confusion matrix for female\n","[[1785 1071]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 714 1785]]\n","confusion matrix for male female\n","[[1785 1071]\n"," [ 714 1785]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2506    0]\n"," [ 358 2506]]\n","confusion matrix for female\n","[[1790 1074]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 716 1790]]\n","confusion matrix for male female\n","[[1790 1074]\n"," [ 716 1790]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2513    0]\n"," [ 359 2513]]\n","confusion matrix for female\n","[[1795 1077]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 718 1795]]\n","confusion matrix for male female\n","[[1795 1077]\n"," [ 718 1795]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2520    0]\n"," [ 360 2520]]\n","confusion matrix for female\n","[[1800 1080]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 720 1800]]\n","confusion matrix for male female\n","[[1800 1080]\n"," [ 720 1800]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2527    0]\n"," [ 361 2527]]\n","confusion matrix for female\n","[[1805 1083]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 722 1805]]\n","confusion matrix for male female\n","[[1805 1083]\n"," [ 722 1805]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2534    0]\n"," [ 362 2534]]\n","confusion matrix for female\n","[[1810 1086]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 724 1810]]\n","confusion matrix for male female\n","[[1810 1086]\n"," [ 724 1810]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2541    0]\n"," [ 363 2541]]\n","confusion matrix for female\n","[[1815 1089]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 726 1815]]\n","confusion matrix for male female\n","[[1815 1089]\n"," [ 726 1815]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2548    0]\n"," [ 364 2548]]\n","confusion matrix for female\n","[[1820 1092]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 728 1820]]\n","confusion matrix for male female\n","[[1820 1092]\n"," [ 728 1820]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2555    0]\n"," [ 365 2555]]\n","confusion matrix for female\n","[[1825 1095]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 730 1825]]\n","confusion matrix for male female\n","[[1825 1095]\n"," [ 730 1825]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2562    0]\n"," [ 366 2562]]\n","confusion matrix for female\n","[[1830 1098]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 732 1830]]\n","confusion matrix for male female\n","[[1830 1098]\n"," [ 732 1830]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2569    0]\n"," [ 367 2569]]\n","confusion matrix for female\n","[[1835 1101]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 734 1835]]\n","confusion matrix for male female\n","[[1835 1101]\n"," [ 734 1835]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2576    0]\n"," [ 368 2576]]\n","confusion matrix for female\n","[[1840 1104]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 736 1840]]\n","confusion matrix for male female\n","[[1840 1104]\n"," [ 736 1840]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2583    0]\n"," [ 369 2583]]\n","confusion matrix for female\n","[[1845 1107]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 738 1845]]\n","confusion matrix for male female\n","[[1845 1107]\n"," [ 738 1845]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2590    0]\n"," [ 370 2590]]\n","confusion matrix for female\n","[[1850 1110]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 740 1850]]\n","confusion matrix for male female\n","[[1850 1110]\n"," [ 740 1850]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2597    0]\n"," [ 371 2597]]\n","confusion matrix for female\n","[[1855 1113]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 742 1855]]\n","confusion matrix for male female\n","[[1855 1113]\n"," [ 742 1855]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2604    0]\n"," [ 372 2604]]\n","confusion matrix for female\n","[[1860 1116]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 744 1860]]\n","confusion matrix for male female\n","[[1860 1116]\n"," [ 744 1860]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2611    0]\n"," [ 373 2611]]\n","confusion matrix for female\n","[[1865 1119]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 746 1865]]\n","confusion matrix for male female\n","[[1865 1119]\n"," [ 746 1865]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2618    0]\n"," [ 374 2618]]\n","confusion matrix for female\n","[[1870 1122]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 748 1870]]\n","confusion matrix for male female\n","[[1870 1122]\n"," [ 748 1870]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2625    0]\n"," [ 375 2625]]\n","confusion matrix for female\n","[[1875 1125]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 750 1875]]\n","confusion matrix for male female\n","[[1875 1125]\n"," [ 750 1875]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2632    0]\n"," [ 376 2632]]\n","confusion matrix for female\n","[[1880 1128]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 752 1880]]\n","confusion matrix for male female\n","[[1880 1128]\n"," [ 752 1880]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2639    0]\n"," [ 377 2639]]\n","confusion matrix for female\n","[[1885 1131]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 754 1885]]\n","confusion matrix for male female\n","[[1885 1131]\n"," [ 754 1885]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2646    0]\n"," [ 378 2646]]\n","confusion matrix for female\n","[[1890 1134]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 756 1890]]\n","confusion matrix for male female\n","[[1890 1134]\n"," [ 756 1890]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2653    0]\n"," [ 379 2653]]\n","confusion matrix for female\n","[[1895 1137]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 758 1895]]\n","confusion matrix for male female\n","[[1895 1137]\n"," [ 758 1895]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2660    0]\n"," [ 380 2660]]\n","confusion matrix for female\n","[[1900 1140]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 760 1900]]\n","confusion matrix for male female\n","[[1900 1140]\n"," [ 760 1900]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2667    0]\n"," [ 381 2667]]\n","confusion matrix for female\n","[[1905 1143]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 762 1905]]\n","confusion matrix for male female\n","[[1905 1143]\n"," [ 762 1905]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2674    0]\n"," [ 382 2674]]\n","confusion matrix for female\n","[[1910 1146]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 764 1910]]\n","confusion matrix for male female\n","[[1910 1146]\n"," [ 764 1910]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2681    0]\n"," [ 383 2681]]\n","confusion matrix for female\n","[[1915 1149]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 766 1915]]\n","confusion matrix for male female\n","[[1915 1149]\n"," [ 766 1915]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2688    0]\n"," [ 384 2688]]\n","confusion matrix for female\n","[[1920 1152]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 768 1920]]\n","confusion matrix for male female\n","[[1920 1152]\n"," [ 768 1920]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2695    0]\n"," [ 385 2695]]\n","confusion matrix for female\n","[[1925 1155]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 770 1925]]\n","confusion matrix for male female\n","[[1925 1155]\n"," [ 770 1925]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2702    0]\n"," [ 386 2702]]\n","confusion matrix for female\n","[[1930 1158]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 772 1930]]\n","confusion matrix for male female\n","[[1930 1158]\n"," [ 772 1930]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2709    0]\n"," [ 387 2709]]\n","confusion matrix for female\n","[[1935 1161]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 774 1935]]\n","confusion matrix for male female\n","[[1935 1161]\n"," [ 774 1935]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2716    0]\n"," [ 388 2716]]\n","confusion matrix for female\n","[[1940 1164]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 776 1940]]\n","confusion matrix for male female\n","[[1940 1164]\n"," [ 776 1940]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2723    0]\n"," [ 389 2723]]\n","confusion matrix for female\n","[[1945 1167]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 778 1945]]\n","confusion matrix for male female\n","[[1945 1167]\n"," [ 778 1945]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2730    0]\n"," [ 390 2730]]\n","confusion matrix for female\n","[[1950 1170]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 780 1950]]\n","confusion matrix for male female\n","[[1950 1170]\n"," [ 780 1950]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2737    0]\n"," [ 391 2737]]\n","confusion matrix for female\n","[[1955 1173]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 782 1955]]\n","confusion matrix for male female\n","[[1955 1173]\n"," [ 782 1955]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2744    0]\n"," [ 392 2744]]\n","confusion matrix for female\n","[[1960 1176]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 784 1960]]\n","confusion matrix for male female\n","[[1960 1176]\n"," [ 784 1960]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2751    0]\n"," [ 393 2751]]\n","confusion matrix for female\n","[[1965 1179]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 786 1965]]\n","confusion matrix for male female\n","[[1965 1179]\n"," [ 786 1965]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2758    0]\n"," [ 394 2758]]\n","confusion matrix for female\n","[[1970 1182]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 788 1970]]\n","confusion matrix for male female\n","[[1970 1182]\n"," [ 788 1970]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2765    0]\n"," [ 395 2765]]\n","confusion matrix for female\n","[[1975 1185]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 790 1975]]\n","confusion matrix for male female\n","[[1975 1185]\n"," [ 790 1975]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2772    0]\n"," [ 396 2772]]\n","confusion matrix for female\n","[[1980 1188]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 792 1980]]\n","confusion matrix for male female\n","[[1980 1188]\n"," [ 792 1980]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2779    0]\n"," [ 397 2779]]\n","confusion matrix for female\n","[[1985 1191]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 794 1985]]\n","confusion matrix for male female\n","[[1985 1191]\n"," [ 794 1985]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2786    0]\n"," [ 398 2786]]\n","confusion matrix for female\n","[[1990 1194]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 796 1990]]\n","confusion matrix for male female\n","[[1990 1194]\n"," [ 796 1990]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2793    0]\n"," [ 399 2793]]\n","confusion matrix for female\n","[[1995 1197]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 798 1995]]\n","confusion matrix for male female\n","[[1995 1197]\n"," [ 798 1995]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2800    0]\n"," [ 400 2800]]\n","confusion matrix for female\n","[[2000 1200]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 800 2000]]\n","confusion matrix for male female\n","[[2000 1200]\n"," [ 800 2000]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2807    0]\n"," [ 401 2807]]\n","confusion matrix for female\n","[[2005 1203]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 802 2005]]\n","confusion matrix for male female\n","[[2005 1203]\n"," [ 802 2005]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2814    0]\n"," [ 402 2814]]\n","confusion matrix for female\n","[[2010 1206]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 804 2010]]\n","confusion matrix for male female\n","[[2010 1206]\n"," [ 804 2010]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2821    0]\n"," [ 403 2821]]\n","confusion matrix for female\n","[[2015 1209]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 806 2015]]\n","confusion matrix for male female\n","[[2015 1209]\n"," [ 806 2015]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2828    0]\n"," [ 404 2828]]\n","confusion matrix for female\n","[[2020 1212]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 808 2020]]\n","confusion matrix for male female\n","[[2020 1212]\n"," [ 808 2020]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2835    0]\n"," [ 405 2835]]\n","confusion matrix for female\n","[[2025 1215]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 810 2025]]\n","confusion matrix for male female\n","[[2025 1215]\n"," [ 810 2025]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2842    0]\n"," [ 406 2842]]\n","confusion matrix for female\n","[[2030 1218]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 812 2030]]\n","confusion matrix for male female\n","[[2030 1218]\n"," [ 812 2030]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2849    0]\n"," [ 407 2849]]\n","confusion matrix for female\n","[[2035 1221]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 814 2035]]\n","confusion matrix for male female\n","[[2035 1221]\n"," [ 814 2035]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2856    0]\n"," [ 408 2856]]\n","confusion matrix for female\n","[[2040 1224]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 816 2040]]\n","confusion matrix for male female\n","[[2040 1224]\n"," [ 816 2040]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2863    0]\n"," [ 409 2863]]\n","confusion matrix for female\n","[[2045 1227]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 818 2045]]\n","confusion matrix for male female\n","[[2045 1227]\n"," [ 818 2045]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2870    0]\n"," [ 410 2870]]\n","confusion matrix for female\n","[[2050 1230]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 820 2050]]\n","confusion matrix for male female\n","[[2050 1230]\n"," [ 820 2050]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2877    0]\n"," [ 411 2877]]\n","confusion matrix for female\n","[[2055 1233]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 822 2055]]\n","confusion matrix for male female\n","[[2055 1233]\n"," [ 822 2055]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2884    0]\n"," [ 412 2884]]\n","confusion matrix for female\n","[[2060 1236]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 824 2060]]\n","confusion matrix for male female\n","[[2060 1236]\n"," [ 824 2060]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2891    0]\n"," [ 413 2891]]\n","confusion matrix for female\n","[[2065 1239]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 826 2065]]\n","confusion matrix for male female\n","[[2065 1239]\n"," [ 826 2065]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2898    0]\n"," [ 414 2898]]\n","confusion matrix for female\n","[[2070 1242]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 828 2070]]\n","confusion matrix for male female\n","[[2070 1242]\n"," [ 828 2070]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2905    0]\n"," [ 415 2905]]\n","confusion matrix for female\n","[[2075 1245]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 830 2075]]\n","confusion matrix for male female\n","[[2075 1245]\n"," [ 830 2075]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2912    0]\n"," [ 416 2912]]\n","confusion matrix for female\n","[[2080 1248]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 832 2080]]\n","confusion matrix for male female\n","[[2080 1248]\n"," [ 832 2080]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2919    0]\n"," [ 417 2919]]\n","confusion matrix for female\n","[[2085 1251]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 834 2085]]\n","confusion matrix for male female\n","[[2085 1251]\n"," [ 834 2085]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2926    0]\n"," [ 418 2926]]\n","confusion matrix for female\n","[[2090 1254]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 836 2090]]\n","confusion matrix for male female\n","[[2090 1254]\n"," [ 836 2090]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2933    0]\n"," [ 419 2933]]\n","confusion matrix for female\n","[[2095 1257]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 838 2095]]\n","confusion matrix for male female\n","[[2095 1257]\n"," [ 838 2095]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2940    0]\n"," [ 420 2940]]\n","confusion matrix for female\n","[[2100 1260]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 840 2100]]\n","confusion matrix for male female\n","[[2100 1260]\n"," [ 840 2100]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2947    0]\n"," [ 421 2947]]\n","confusion matrix for female\n","[[2105 1263]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 842 2105]]\n","confusion matrix for male female\n","[[2105 1263]\n"," [ 842 2105]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2954    0]\n"," [ 422 2954]]\n","confusion matrix for female\n","[[2110 1266]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 844 2110]]\n","confusion matrix for male female\n","[[2110 1266]\n"," [ 844 2110]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2961    0]\n"," [ 423 2961]]\n","confusion matrix for female\n","[[2115 1269]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 846 2115]]\n","confusion matrix for male female\n","[[2115 1269]\n"," [ 846 2115]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2968    0]\n"," [ 424 2968]]\n","confusion matrix for female\n","[[2120 1272]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 848 2120]]\n","confusion matrix for male female\n","[[2120 1272]\n"," [ 848 2120]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2975    0]\n"," [ 425 2975]]\n","confusion matrix for female\n","[[2125 1275]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 850 2125]]\n","confusion matrix for male female\n","[[2125 1275]\n"," [ 850 2125]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2982    0]\n"," [ 426 2982]]\n","confusion matrix for female\n","[[2130 1278]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 852 2130]]\n","confusion matrix for male female\n","[[2130 1278]\n"," [ 852 2130]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2989    0]\n"," [ 427 2989]]\n","confusion matrix for female\n","[[2135 1281]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 854 2135]]\n","confusion matrix for male female\n","[[2135 1281]\n"," [ 854 2135]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[2996    0]\n"," [ 428 2996]]\n","confusion matrix for female\n","[[2140 1284]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 856 2140]]\n","confusion matrix for male female\n","[[2140 1284]\n"," [ 856 2140]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3003    0]\n"," [ 429 3003]]\n","confusion matrix for female\n","[[2145 1287]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 858 2145]]\n","confusion matrix for male female\n","[[2145 1287]\n"," [ 858 2145]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3010    0]\n"," [ 430 3010]]\n","confusion matrix for female\n","[[2150 1290]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 860 2150]]\n","confusion matrix for male female\n","[[2150 1290]\n"," [ 860 2150]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3017    0]\n"," [ 431 3017]]\n","confusion matrix for female\n","[[2155 1293]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 862 2155]]\n","confusion matrix for male female\n","[[2155 1293]\n"," [ 862 2155]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3024    0]\n"," [ 432 3024]]\n","confusion matrix for female\n","[[2160 1296]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 864 2160]]\n","confusion matrix for male female\n","[[2160 1296]\n"," [ 864 2160]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3031    0]\n"," [ 433 3031]]\n","confusion matrix for female\n","[[2165 1299]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 866 2165]]\n","confusion matrix for male female\n","[[2165 1299]\n"," [ 866 2165]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3038    0]\n"," [ 434 3038]]\n","confusion matrix for female\n","[[2170 1302]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 868 2170]]\n","confusion matrix for male female\n","[[2170 1302]\n"," [ 868 2170]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3045    0]\n"," [ 435 3045]]\n","confusion matrix for female\n","[[2175 1305]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 870 2175]]\n","confusion matrix for male female\n","[[2175 1305]\n"," [ 870 2175]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3052    0]\n"," [ 436 3052]]\n","confusion matrix for female\n","[[2180 1308]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 872 2180]]\n","confusion matrix for male female\n","[[2180 1308]\n"," [ 872 2180]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3059    0]\n"," [ 437 3059]]\n","confusion matrix for female\n","[[2185 1311]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 874 2185]]\n","confusion matrix for male female\n","[[2185 1311]\n"," [ 874 2185]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3066    0]\n"," [ 438 3066]]\n","confusion matrix for female\n","[[2190 1314]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 876 2190]]\n","confusion matrix for male female\n","[[2190 1314]\n"," [ 876 2190]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3073    0]\n"," [ 439 3073]]\n","confusion matrix for female\n","[[2195 1317]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 878 2195]]\n","confusion matrix for male female\n","[[2195 1317]\n"," [ 878 2195]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3080    0]\n"," [ 440 3080]]\n","confusion matrix for female\n","[[2200 1320]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 880 2200]]\n","confusion matrix for male female\n","[[2200 1320]\n"," [ 880 2200]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3087    0]\n"," [ 441 3087]]\n","confusion matrix for female\n","[[2205 1323]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 882 2205]]\n","confusion matrix for male female\n","[[2205 1323]\n"," [ 882 2205]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3094    0]\n"," [ 442 3094]]\n","confusion matrix for female\n","[[2210 1326]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 884 2210]]\n","confusion matrix for male female\n","[[2210 1326]\n"," [ 884 2210]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3101    0]\n"," [ 443 3101]]\n","confusion matrix for female\n","[[2215 1329]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 886 2215]]\n","confusion matrix for male female\n","[[2215 1329]\n"," [ 886 2215]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3108    0]\n"," [ 444 3108]]\n","confusion matrix for female\n","[[2220 1332]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 888 2220]]\n","confusion matrix for male female\n","[[2220 1332]\n"," [ 888 2220]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3115    0]\n"," [ 445 3115]]\n","confusion matrix for female\n","[[2225 1335]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 890 2225]]\n","confusion matrix for male female\n","[[2225 1335]\n"," [ 890 2225]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3122    0]\n"," [ 446 3122]]\n","confusion matrix for female\n","[[2230 1338]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 892 2230]]\n","confusion matrix for male female\n","[[2230 1338]\n"," [ 892 2230]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3129    0]\n"," [ 447 3129]]\n","confusion matrix for female\n","[[2235 1341]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 894 2235]]\n","confusion matrix for male female\n","[[2235 1341]\n"," [ 894 2235]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3136    0]\n"," [ 448 3136]]\n","confusion matrix for female\n","[[2240 1344]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 896 2240]]\n","confusion matrix for male female\n","[[2240 1344]\n"," [ 896 2240]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3143    0]\n"," [ 449 3143]]\n","confusion matrix for female\n","[[2245 1347]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 898 2245]]\n","confusion matrix for male female\n","[[2245 1347]\n"," [ 898 2245]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3150    0]\n"," [ 450 3150]]\n","confusion matrix for female\n","[[2250 1350]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 900 2250]]\n","confusion matrix for male female\n","[[2250 1350]\n"," [ 900 2250]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3157    0]\n"," [ 451 3157]]\n","confusion matrix for female\n","[[2255 1353]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 902 2255]]\n","confusion matrix for male female\n","[[2255 1353]\n"," [ 902 2255]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3164    0]\n"," [ 452 3164]]\n","confusion matrix for female\n","[[2260 1356]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 904 2260]]\n","confusion matrix for male female\n","[[2260 1356]\n"," [ 904 2260]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3171    0]\n"," [ 453 3171]]\n","confusion matrix for female\n","[[2265 1359]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 906 2265]]\n","confusion matrix for male female\n","[[2265 1359]\n"," [ 906 2265]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3178    0]\n"," [ 454 3178]]\n","confusion matrix for female\n","[[2270 1362]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 908 2270]]\n","confusion matrix for male female\n","[[2270 1362]\n"," [ 908 2270]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3185    0]\n"," [ 455 3185]]\n","confusion matrix for female\n","[[2275 1365]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 910 2275]]\n","confusion matrix for male female\n","[[2275 1365]\n"," [ 910 2275]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3192    0]\n"," [ 456 3192]]\n","confusion matrix for female\n","[[2280 1368]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 912 2280]]\n","confusion matrix for male female\n","[[2280 1368]\n"," [ 912 2280]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3199    0]\n"," [ 457 3199]]\n","confusion matrix for female\n","[[2285 1371]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 914 2285]]\n","confusion matrix for male female\n","[[2285 1371]\n"," [ 914 2285]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3206    0]\n"," [ 458 3206]]\n","confusion matrix for female\n","[[2290 1374]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 916 2290]]\n","confusion matrix for male female\n","[[2290 1374]\n"," [ 916 2290]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3213    0]\n"," [ 459 3213]]\n","confusion matrix for female\n","[[2295 1377]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 918 2295]]\n","confusion matrix for male female\n","[[2295 1377]\n"," [ 918 2295]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3220    0]\n"," [ 460 3220]]\n","confusion matrix for female\n","[[2300 1380]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 920 2300]]\n","confusion matrix for male female\n","[[2300 1380]\n"," [ 920 2300]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3227    0]\n"," [ 461 3227]]\n","confusion matrix for female\n","[[2305 1383]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 922 2305]]\n","confusion matrix for male female\n","[[2305 1383]\n"," [ 922 2305]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3234    0]\n"," [ 462 3234]]\n","confusion matrix for female\n","[[2310 1386]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 924 2310]]\n","confusion matrix for male female\n","[[2310 1386]\n"," [ 924 2310]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3241    0]\n"," [ 463 3241]]\n","confusion matrix for female\n","[[2315 1389]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 926 2315]]\n","confusion matrix for male female\n","[[2315 1389]\n"," [ 926 2315]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3248    0]\n"," [ 464 3248]]\n","confusion matrix for female\n","[[2320 1392]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 928 2320]]\n","confusion matrix for male female\n","[[2320 1392]\n"," [ 928 2320]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3255    0]\n"," [ 465 3255]]\n","confusion matrix for female\n","[[2325 1395]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 930 2325]]\n","confusion matrix for male female\n","[[2325 1395]\n"," [ 930 2325]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3262    0]\n"," [ 466 3262]]\n","confusion matrix for female\n","[[2330 1398]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 932 2330]]\n","confusion matrix for male female\n","[[2330 1398]\n"," [ 932 2330]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3269    0]\n"," [ 467 3269]]\n","confusion matrix for female\n","[[2335 1401]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 934 2335]]\n","confusion matrix for male female\n","[[2335 1401]\n"," [ 934 2335]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3276    0]\n"," [ 468 3276]]\n","confusion matrix for female\n","[[2340 1404]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 936 2340]]\n","confusion matrix for male female\n","[[2340 1404]\n"," [ 936 2340]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3283    0]\n"," [ 469 3283]]\n","confusion matrix for female\n","[[2345 1407]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 938 2345]]\n","confusion matrix for male female\n","[[2345 1407]\n"," [ 938 2345]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3290    0]\n"," [ 470 3290]]\n","confusion matrix for female\n","[[2350 1410]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 940 2350]]\n","confusion matrix for male female\n","[[2350 1410]\n"," [ 940 2350]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3297    0]\n"," [ 471 3297]]\n","confusion matrix for female\n","[[2355 1413]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 942 2355]]\n","confusion matrix for male female\n","[[2355 1413]\n"," [ 942 2355]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3304    0]\n"," [ 472 3304]]\n","confusion matrix for female\n","[[2360 1416]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 944 2360]]\n","confusion matrix for male female\n","[[2360 1416]\n"," [ 944 2360]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3311    0]\n"," [ 473 3311]]\n","confusion matrix for female\n","[[2365 1419]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 946 2365]]\n","confusion matrix for male female\n","[[2365 1419]\n"," [ 946 2365]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3318    0]\n"," [ 474 3318]]\n","confusion matrix for female\n","[[2370 1422]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 948 2370]]\n","confusion matrix for male female\n","[[2370 1422]\n"," [ 948 2370]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3325    0]\n"," [ 475 3325]]\n","confusion matrix for female\n","[[2375 1425]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 950 2375]]\n","confusion matrix for male female\n","[[2375 1425]\n"," [ 950 2375]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3332    0]\n"," [ 476 3332]]\n","confusion matrix for female\n","[[2380 1428]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 952 2380]]\n","confusion matrix for male female\n","[[2380 1428]\n"," [ 952 2380]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3339    0]\n"," [ 477 3339]]\n","confusion matrix for female\n","[[2385 1431]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 954 2385]]\n","confusion matrix for male female\n","[[2385 1431]\n"," [ 954 2385]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3346    0]\n"," [ 478 3346]]\n","confusion matrix for female\n","[[2390 1434]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 956 2390]]\n","confusion matrix for male female\n","[[2390 1434]\n"," [ 956 2390]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3353    0]\n"," [ 479 3353]]\n","confusion matrix for female\n","[[2395 1437]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 958 2395]]\n","confusion matrix for male female\n","[[2395 1437]\n"," [ 958 2395]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3360    0]\n"," [ 480 3360]]\n","confusion matrix for female\n","[[2400 1440]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 960 2400]]\n","confusion matrix for male female\n","[[2400 1440]\n"," [ 960 2400]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3367    0]\n"," [ 481 3367]]\n","confusion matrix for female\n","[[2405 1443]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 962 2405]]\n","confusion matrix for male female\n","[[2405 1443]\n"," [ 962 2405]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3374    0]\n"," [ 482 3374]]\n","confusion matrix for female\n","[[2410 1446]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 964 2410]]\n","confusion matrix for male female\n","[[2410 1446]\n"," [ 964 2410]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3381    0]\n"," [ 483 3381]]\n","confusion matrix for female\n","[[2415 1449]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 966 2415]]\n","confusion matrix for male female\n","[[2415 1449]\n"," [ 966 2415]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3388    0]\n"," [ 484 3388]]\n","confusion matrix for female\n","[[2420 1452]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 968 2420]]\n","confusion matrix for male female\n","[[2420 1452]\n"," [ 968 2420]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3395    0]\n"," [ 485 3395]]\n","confusion matrix for female\n","[[2425 1455]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 970 2425]]\n","confusion matrix for male female\n","[[2425 1455]\n"," [ 970 2425]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3402    0]\n"," [ 486 3402]]\n","confusion matrix for female\n","[[2430 1458]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 972 2430]]\n","confusion matrix for male female\n","[[2430 1458]\n"," [ 972 2430]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3409    0]\n"," [ 487 3409]]\n","confusion matrix for female\n","[[2435 1461]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 974 2435]]\n","confusion matrix for male female\n","[[2435 1461]\n"," [ 974 2435]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3416    0]\n"," [ 488 3416]]\n","confusion matrix for female\n","[[2440 1464]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 976 2440]]\n","confusion matrix for male female\n","[[2440 1464]\n"," [ 976 2440]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3423    0]\n"," [ 489 3423]]\n","confusion matrix for female\n","[[2445 1467]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 978 2445]]\n","confusion matrix for male female\n","[[2445 1467]\n"," [ 978 2445]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3430    0]\n"," [ 490 3430]]\n","confusion matrix for female\n","[[2450 1470]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 980 2450]]\n","confusion matrix for male female\n","[[2450 1470]\n"," [ 980 2450]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3437    0]\n"," [ 491 3437]]\n","confusion matrix for female\n","[[2455 1473]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 982 2455]]\n","confusion matrix for male female\n","[[2455 1473]\n"," [ 982 2455]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3444    0]\n"," [ 492 3444]]\n","confusion matrix for female\n","[[2460 1476]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 984 2460]]\n","confusion matrix for male female\n","[[2460 1476]\n"," [ 984 2460]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3451    0]\n"," [ 493 3451]]\n","confusion matrix for female\n","[[2465 1479]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 986 2465]]\n","confusion matrix for male female\n","[[2465 1479]\n"," [ 986 2465]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3458    0]\n"," [ 494 3458]]\n","confusion matrix for female\n","[[2470 1482]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 988 2470]]\n","confusion matrix for male female\n","[[2470 1482]\n"," [ 988 2470]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3465    0]\n"," [ 495 3465]]\n","confusion matrix for female\n","[[2475 1485]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 990 2475]]\n","confusion matrix for male female\n","[[2475 1485]\n"," [ 990 2475]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3472    0]\n"," [ 496 3472]]\n","confusion matrix for female\n","[[2480 1488]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 992 2480]]\n","confusion matrix for male female\n","[[2480 1488]\n"," [ 992 2480]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3479    0]\n"," [ 497 3479]]\n","confusion matrix for female\n","[[2485 1491]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 994 2485]]\n","confusion matrix for male female\n","[[2485 1491]\n"," [ 994 2485]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3486    0]\n"," [ 498 3486]]\n","confusion matrix for female\n","[[2490 1494]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 996 2490]]\n","confusion matrix for male female\n","[[2490 1494]\n"," [ 996 2490]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3493    0]\n"," [ 499 3493]]\n","confusion matrix for female\n","[[2495 1497]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [ 998 2495]]\n","confusion matrix for male female\n","[[2495 1497]\n"," [ 998 2495]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3500    0]\n"," [ 500 3500]]\n","confusion matrix for female\n","[[2500 1500]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1000 2500]]\n","confusion matrix for male female\n","[[2500 1500]\n"," [1000 2500]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3507    0]\n"," [ 501 3507]]\n","confusion matrix for female\n","[[2505 1503]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1002 2505]]\n","confusion matrix for male female\n","[[2505 1503]\n"," [1002 2505]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3514    0]\n"," [ 502 3514]]\n","confusion matrix for female\n","[[2510 1506]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1004 2510]]\n","confusion matrix for male female\n","[[2510 1506]\n"," [1004 2510]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3521    0]\n"," [ 503 3521]]\n","confusion matrix for female\n","[[2515 1509]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1006 2515]]\n","confusion matrix for male female\n","[[2515 1509]\n"," [1006 2515]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3528    0]\n"," [ 504 3528]]\n","confusion matrix for female\n","[[2520 1512]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1008 2520]]\n","confusion matrix for male female\n","[[2520 1512]\n"," [1008 2520]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3535    0]\n"," [ 505 3535]]\n","confusion matrix for female\n","[[2525 1515]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1010 2525]]\n","confusion matrix for male female\n","[[2525 1515]\n"," [1010 2525]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3542    0]\n"," [ 506 3542]]\n","confusion matrix for female\n","[[2530 1518]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1012 2530]]\n","confusion matrix for male female\n","[[2530 1518]\n"," [1012 2530]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3549    0]\n"," [ 507 3549]]\n","confusion matrix for female\n","[[2535 1521]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1014 2535]]\n","confusion matrix for male female\n","[[2535 1521]\n"," [1014 2535]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3556    0]\n"," [ 508 3556]]\n","confusion matrix for female\n","[[2540 1524]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1016 2540]]\n","confusion matrix for male female\n","[[2540 1524]\n"," [1016 2540]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3563    0]\n"," [ 509 3563]]\n","confusion matrix for female\n","[[2545 1527]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1018 2545]]\n","confusion matrix for male female\n","[[2545 1527]\n"," [1018 2545]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3570    0]\n"," [ 510 3570]]\n","confusion matrix for female\n","[[2550 1530]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1020 2550]]\n","confusion matrix for male female\n","[[2550 1530]\n"," [1020 2550]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3577    0]\n"," [ 511 3577]]\n","confusion matrix for female\n","[[2555 1533]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1022 2555]]\n","confusion matrix for male female\n","[[2555 1533]\n"," [1022 2555]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3584    0]\n"," [ 512 3584]]\n","confusion matrix for female\n","[[2560 1536]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1024 2560]]\n","confusion matrix for male female\n","[[2560 1536]\n"," [1024 2560]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3591    0]\n"," [ 513 3591]]\n","confusion matrix for female\n","[[2565 1539]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1026 2565]]\n","confusion matrix for male female\n","[[2565 1539]\n"," [1026 2565]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3598    0]\n"," [ 514 3598]]\n","confusion matrix for female\n","[[2570 1542]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1028 2570]]\n","confusion matrix for male female\n","[[2570 1542]\n"," [1028 2570]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3605    0]\n"," [ 515 3605]]\n","confusion matrix for female\n","[[2575 1545]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1030 2575]]\n","confusion matrix for male female\n","[[2575 1545]\n"," [1030 2575]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3612    0]\n"," [ 516 3612]]\n","confusion matrix for female\n","[[2580 1548]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1032 2580]]\n","confusion matrix for male female\n","[[2580 1548]\n"," [1032 2580]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3619    0]\n"," [ 517 3619]]\n","confusion matrix for female\n","[[2585 1551]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1034 2585]]\n","confusion matrix for male female\n","[[2585 1551]\n"," [1034 2585]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3626    0]\n"," [ 518 3626]]\n","confusion matrix for female\n","[[2590 1554]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1036 2590]]\n","confusion matrix for male female\n","[[2590 1554]\n"," [1036 2590]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3633    0]\n"," [ 519 3633]]\n","confusion matrix for female\n","[[2595 1557]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1038 2595]]\n","confusion matrix for male female\n","[[2595 1557]\n"," [1038 2595]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3640    0]\n"," [ 520 3640]]\n","confusion matrix for female\n","[[2600 1560]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1040 2600]]\n","confusion matrix for male female\n","[[2600 1560]\n"," [1040 2600]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3647    0]\n"," [ 521 3647]]\n","confusion matrix for female\n","[[2605 1563]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1042 2605]]\n","confusion matrix for male female\n","[[2605 1563]\n"," [1042 2605]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3654    0]\n"," [ 522 3654]]\n","confusion matrix for female\n","[[2610 1566]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1044 2610]]\n","confusion matrix for male female\n","[[2610 1566]\n"," [1044 2610]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3661    0]\n"," [ 523 3661]]\n","confusion matrix for female\n","[[2615 1569]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1046 2615]]\n","confusion matrix for male female\n","[[2615 1569]\n"," [1046 2615]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3668    0]\n"," [ 524 3668]]\n","confusion matrix for female\n","[[2620 1572]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1048 2620]]\n","confusion matrix for male female\n","[[2620 1572]\n"," [1048 2620]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3675    0]\n"," [ 525 3675]]\n","confusion matrix for female\n","[[2625 1575]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1050 2625]]\n","confusion matrix for male female\n","[[2625 1575]\n"," [1050 2625]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3682    0]\n"," [ 526 3682]]\n","confusion matrix for female\n","[[2630 1578]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1052 2630]]\n","confusion matrix for male female\n","[[2630 1578]\n"," [1052 2630]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3689    0]\n"," [ 527 3689]]\n","confusion matrix for female\n","[[2635 1581]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1054 2635]]\n","confusion matrix for male female\n","[[2635 1581]\n"," [1054 2635]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3696    0]\n"," [ 528 3696]]\n","confusion matrix for female\n","[[2640 1584]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1056 2640]]\n","confusion matrix for male female\n","[[2640 1584]\n"," [1056 2640]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3703    0]\n"," [ 529 3703]]\n","confusion matrix for female\n","[[2645 1587]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1058 2645]]\n","confusion matrix for male female\n","[[2645 1587]\n"," [1058 2645]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3710    0]\n"," [ 530 3710]]\n","confusion matrix for female\n","[[2650 1590]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1060 2650]]\n","confusion matrix for male female\n","[[2650 1590]\n"," [1060 2650]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3717    0]\n"," [ 531 3717]]\n","confusion matrix for female\n","[[2655 1593]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1062 2655]]\n","confusion matrix for male female\n","[[2655 1593]\n"," [1062 2655]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3724    0]\n"," [ 532 3724]]\n","confusion matrix for female\n","[[2660 1596]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1064 2660]]\n","confusion matrix for male female\n","[[2660 1596]\n"," [1064 2660]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3731    0]\n"," [ 533 3731]]\n","confusion matrix for female\n","[[2665 1599]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1066 2665]]\n","confusion matrix for male female\n","[[2665 1599]\n"," [1066 2665]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3738    0]\n"," [ 534 3738]]\n","confusion matrix for female\n","[[2670 1602]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1068 2670]]\n","confusion matrix for male female\n","[[2670 1602]\n"," [1068 2670]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3745    0]\n"," [ 535 3745]]\n","confusion matrix for female\n","[[2675 1605]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1070 2675]]\n","confusion matrix for male female\n","[[2675 1605]\n"," [1070 2675]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3752    0]\n"," [ 536 3752]]\n","confusion matrix for female\n","[[2680 1608]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1072 2680]]\n","confusion matrix for male female\n","[[2680 1608]\n"," [1072 2680]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3759    0]\n"," [ 537 3759]]\n","confusion matrix for female\n","[[2685 1611]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1074 2685]]\n","confusion matrix for male female\n","[[2685 1611]\n"," [1074 2685]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3766    0]\n"," [ 538 3766]]\n","confusion matrix for female\n","[[2690 1614]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1076 2690]]\n","confusion matrix for male female\n","[[2690 1614]\n"," [1076 2690]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3773    0]\n"," [ 539 3773]]\n","confusion matrix for female\n","[[2695 1617]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1078 2695]]\n","confusion matrix for male female\n","[[2695 1617]\n"," [1078 2695]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3780    0]\n"," [ 540 3780]]\n","confusion matrix for female\n","[[2700 1620]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1080 2700]]\n","confusion matrix for male female\n","[[2700 1620]\n"," [1080 2700]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3787    0]\n"," [ 541 3787]]\n","confusion matrix for female\n","[[2705 1623]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1082 2705]]\n","confusion matrix for male female\n","[[2705 1623]\n"," [1082 2705]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3794    0]\n"," [ 542 3794]]\n","confusion matrix for female\n","[[2710 1626]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1084 2710]]\n","confusion matrix for male female\n","[[2710 1626]\n"," [1084 2710]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3801    0]\n"," [ 543 3801]]\n","confusion matrix for female\n","[[2715 1629]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1086 2715]]\n","confusion matrix for male female\n","[[2715 1629]\n"," [1086 2715]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3808    0]\n"," [ 544 3808]]\n","confusion matrix for female\n","[[2720 1632]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1088 2720]]\n","confusion matrix for male female\n","[[2720 1632]\n"," [1088 2720]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3815    0]\n"," [ 545 3815]]\n","confusion matrix for female\n","[[2725 1635]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1090 2725]]\n","confusion matrix for male female\n","[[2725 1635]\n"," [1090 2725]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3822    0]\n"," [ 546 3822]]\n","confusion matrix for female\n","[[2730 1638]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1092 2730]]\n","confusion matrix for male female\n","[[2730 1638]\n"," [1092 2730]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3829    0]\n"," [ 547 3829]]\n","confusion matrix for female\n","[[2735 1641]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1094 2735]]\n","confusion matrix for male female\n","[[2735 1641]\n"," [1094 2735]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3836    0]\n"," [ 548 3836]]\n","confusion matrix for female\n","[[2740 1644]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1096 2740]]\n","confusion matrix for male female\n","[[2740 1644]\n"," [1096 2740]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3843    0]\n"," [ 549 3843]]\n","confusion matrix for female\n","[[2745 1647]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1098 2745]]\n","confusion matrix for male female\n","[[2745 1647]\n"," [1098 2745]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3850    0]\n"," [ 550 3850]]\n","confusion matrix for female\n","[[2750 1650]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1100 2750]]\n","confusion matrix for male female\n","[[2750 1650]\n"," [1100 2750]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3857    0]\n"," [ 551 3857]]\n","confusion matrix for female\n","[[2755 1653]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1102 2755]]\n","confusion matrix for male female\n","[[2755 1653]\n"," [1102 2755]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3864    0]\n"," [ 552 3864]]\n","confusion matrix for female\n","[[2760 1656]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1104 2760]]\n","confusion matrix for male female\n","[[2760 1656]\n"," [1104 2760]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3871    0]\n"," [ 553 3871]]\n","confusion matrix for female\n","[[2765 1659]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1106 2765]]\n","confusion matrix for male female\n","[[2765 1659]\n"," [1106 2765]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3878    0]\n"," [ 554 3878]]\n","confusion matrix for female\n","[[2770 1662]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1108 2770]]\n","confusion matrix for male female\n","[[2770 1662]\n"," [1108 2770]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3885    0]\n"," [ 555 3885]]\n","confusion matrix for female\n","[[2775 1665]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1110 2775]]\n","confusion matrix for male female\n","[[2775 1665]\n"," [1110 2775]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3892    0]\n"," [ 556 3892]]\n","confusion matrix for female\n","[[2780 1668]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1112 2780]]\n","confusion matrix for male female\n","[[2780 1668]\n"," [1112 2780]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3899    0]\n"," [ 557 3899]]\n","confusion matrix for female\n","[[2785 1671]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1114 2785]]\n","confusion matrix for male female\n","[[2785 1671]\n"," [1114 2785]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3906    0]\n"," [ 558 3906]]\n","confusion matrix for female\n","[[2790 1674]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1116 2790]]\n","confusion matrix for male female\n","[[2790 1674]\n"," [1116 2790]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3913    0]\n"," [ 559 3913]]\n","confusion matrix for female\n","[[2795 1677]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1118 2795]]\n","confusion matrix for male female\n","[[2795 1677]\n"," [1118 2795]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3920    0]\n"," [ 560 3920]]\n","confusion matrix for female\n","[[2800 1680]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1120 2800]]\n","confusion matrix for male female\n","[[2800 1680]\n"," [1120 2800]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3927    0]\n"," [ 561 3927]]\n","confusion matrix for female\n","[[2805 1683]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1122 2805]]\n","confusion matrix for male female\n","[[2805 1683]\n"," [1122 2805]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3934    0]\n"," [ 562 3934]]\n","confusion matrix for female\n","[[2810 1686]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1124 2810]]\n","confusion matrix for male female\n","[[2810 1686]\n"," [1124 2810]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3941    0]\n"," [ 563 3941]]\n","confusion matrix for female\n","[[2815 1689]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1126 2815]]\n","confusion matrix for male female\n","[[2815 1689]\n"," [1126 2815]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3948    0]\n"," [ 564 3948]]\n","confusion matrix for female\n","[[2820 1692]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1128 2820]]\n","confusion matrix for male female\n","[[2820 1692]\n"," [1128 2820]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3955    0]\n"," [ 565 3955]]\n","confusion matrix for female\n","[[2825 1695]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1130 2825]]\n","confusion matrix for male female\n","[[2825 1695]\n"," [1130 2825]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3962    0]\n"," [ 566 3962]]\n","confusion matrix for female\n","[[2830 1698]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1132 2830]]\n","confusion matrix for male female\n","[[2830 1698]\n"," [1132 2830]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3969    0]\n"," [ 567 3969]]\n","confusion matrix for female\n","[[2835 1701]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1134 2835]]\n","confusion matrix for male female\n","[[2835 1701]\n"," [1134 2835]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3976    0]\n"," [ 568 3976]]\n","confusion matrix for female\n","[[2840 1704]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1136 2840]]\n","confusion matrix for male female\n","[[2840 1704]\n"," [1136 2840]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3983    0]\n"," [ 569 3983]]\n","confusion matrix for female\n","[[2845 1707]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1138 2845]]\n","confusion matrix for male female\n","[[2845 1707]\n"," [1138 2845]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3990    0]\n"," [ 570 3990]]\n","confusion matrix for female\n","[[2850 1710]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1140 2850]]\n","confusion matrix for male female\n","[[2850 1710]\n"," [1140 2850]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[3997    0]\n"," [ 571 3997]]\n","confusion matrix for female\n","[[2855 1713]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1142 2855]]\n","confusion matrix for male female\n","[[2855 1713]\n"," [1142 2855]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[4004    0]\n"," [ 572 4004]]\n","confusion matrix for female\n","[[2860 1716]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1144 2860]]\n","confusion matrix for male female\n","[[2860 1716]\n"," [1144 2860]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[4011    0]\n"," [ 573 4011]]\n","confusion matrix for female\n","[[2865 1719]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1146 2865]]\n","confusion matrix for male female\n","[[2865 1719]\n"," [1146 2865]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[4018    0]\n"," [ 574 4018]]\n","confusion matrix for female\n","[[2870 1722]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1148 2870]]\n","confusion matrix for male female\n","[[2870 1722]\n"," [1148 2870]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[4025    0]\n"," [ 575 4025]]\n","confusion matrix for female\n","[[2875 1725]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1150 2875]]\n","confusion matrix for male female\n","[[2875 1725]\n"," [1150 2875]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[4032    0]\n"," [ 576 4032]]\n","confusion matrix for female\n","[[2880 1728]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1152 2880]]\n","confusion matrix for male female\n","[[2880 1728]\n"," [1152 2880]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[4039    0]\n"," [ 577 4039]]\n","confusion matrix for female\n","[[2885 1731]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1154 2885]]\n","confusion matrix for male female\n","[[2885 1731]\n"," [1154 2885]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[4046    0]\n"," [ 578 4046]]\n","confusion matrix for female\n","[[2890 1734]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1156 2890]]\n","confusion matrix for male female\n","[[2890 1734]\n"," [1156 2890]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[4053    0]\n"," [ 579 4053]]\n","confusion matrix for female\n","[[2895 1737]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1158 2895]]\n","confusion matrix for male female\n","[[2895 1737]\n"," [1158 2895]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[4060    0]\n"," [ 580 4060]]\n","confusion matrix for female\n","[[2900 1740]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1160 2900]]\n","confusion matrix for male female\n","[[2900 1740]\n"," [1160 2900]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[4067    0]\n"," [ 581 4067]]\n","confusion matrix for female\n","[[2905 1743]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1162 2905]]\n","confusion matrix for male female\n","[[2905 1743]\n"," [1162 2905]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[4074    0]\n"," [ 582 4074]]\n","confusion matrix for female\n","[[2910 1746]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1164 2910]]\n","confusion matrix for male female\n","[[2910 1746]\n"," [1164 2910]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[4081    0]\n"," [ 583 4081]]\n","confusion matrix for female\n","[[2915 1749]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1166 2915]]\n","confusion matrix for male female\n","[[2915 1749]\n"," [1166 2915]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[4088    0]\n"," [ 584 4088]]\n","confusion matrix for female\n","[[2920 1752]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1168 2920]]\n","confusion matrix for male female\n","[[2920 1752]\n"," [1168 2920]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[4095    0]\n"," [ 585 4095]]\n","confusion matrix for female\n","[[2925 1755]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1170 2925]]\n","confusion matrix for male female\n","[[2925 1755]\n"," [1170 2925]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[4102    0]\n"," [ 586 4102]]\n","confusion matrix for female\n","[[2930 1758]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1172 2930]]\n","confusion matrix for male female\n","[[2930 1758]\n"," [1172 2930]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[4109    0]\n"," [ 587 4109]]\n","confusion matrix for female\n","[[2935 1761]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1174 2935]]\n","confusion matrix for male female\n","[[2935 1761]\n"," [1174 2935]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[4116    0]\n"," [ 588 4116]]\n","confusion matrix for female\n","[[2940 1764]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1176 2940]]\n","confusion matrix for male female\n","[[2940 1764]\n"," [1176 2940]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[4123    0]\n"," [ 589 4123]]\n","confusion matrix for female\n","[[2945 1767]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1178 2945]]\n","confusion matrix for male female\n","[[2945 1767]\n"," [1178 2945]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[4130    0]\n"," [ 590 4130]]\n","confusion matrix for female\n","[[2950 1770]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1180 2950]]\n","confusion matrix for male female\n","[[2950 1770]\n"," [1180 2950]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[4137    0]\n"," [ 591 4137]]\n","confusion matrix for female\n","[[2955 1773]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1182 2955]]\n","confusion matrix for male female\n","[[2955 1773]\n"," [1182 2955]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[4144    0]\n"," [ 592 4144]]\n","confusion matrix for female\n","[[2960 1776]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1184 2960]]\n","confusion matrix for male female\n","[[2960 1776]\n"," [1184 2960]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[4151    0]\n"," [ 593 4151]]\n","confusion matrix for female\n","[[2965 1779]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1186 2965]]\n","confusion matrix for male female\n","[[2965 1779]\n"," [1186 2965]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[4158    0]\n"," [ 594 4158]]\n","confusion matrix for female\n","[[2970 1782]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1188 2970]]\n","confusion matrix for male female\n","[[2970 1782]\n"," [1188 2970]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[4165    0]\n"," [ 595 4165]]\n","confusion matrix for female\n","[[2975 1785]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1190 2975]]\n","confusion matrix for male female\n","[[2975 1785]\n"," [1190 2975]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[4172    0]\n"," [ 596 4172]]\n","confusion matrix for female\n","[[2980 1788]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1192 2980]]\n","confusion matrix for male female\n","[[2980 1788]\n"," [1192 2980]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[4179    0]\n"," [ 597 4179]]\n","confusion matrix for female\n","[[2985 1791]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1194 2985]]\n","confusion matrix for male female\n","[[2985 1791]\n"," [1194 2985]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[4186    0]\n"," [ 598 4186]]\n","confusion matrix for female\n","[[2990 1794]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1196 2990]]\n","confusion matrix for male female\n","[[2990 1794]\n"," [1196 2990]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[4193    0]\n"," [ 599 4193]]\n","confusion matrix for female\n","[[2995 1797]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1198 2995]]\n","confusion matrix for male female\n","[[2995 1797]\n"," [1198 2995]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[4200    0]\n"," [ 600 4200]]\n","confusion matrix for female\n","[[3000 1800]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1200 3000]]\n","confusion matrix for male female\n","[[3000 1800]\n"," [1200 3000]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[4207    0]\n"," [ 601 4207]]\n","confusion matrix for female\n","[[3005 1803]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1202 3005]]\n","confusion matrix for male female\n","[[3005 1803]\n"," [1202 3005]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[4214    0]\n"," [ 602 4214]]\n","confusion matrix for female\n","[[3010 1806]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1204 3010]]\n","confusion matrix for male female\n","[[3010 1806]\n"," [1204 3010]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[4221    0]\n"," [ 603 4221]]\n","confusion matrix for female\n","[[3015 1809]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1206 3015]]\n","confusion matrix for male female\n","[[3015 1809]\n"," [1206 3015]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[4228    0]\n"," [ 604 4228]]\n","confusion matrix for female\n","[[3020 1812]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1208 3020]]\n","confusion matrix for male female\n","[[3020 1812]\n"," [1208 3020]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[4235    0]\n"," [ 605 4235]]\n","confusion matrix for female\n","[[3025 1815]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1210 3025]]\n","confusion matrix for male female\n","[[3025 1815]\n"," [1210 3025]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[4242    0]\n"," [ 606 4242]]\n","confusion matrix for female\n","[[3030 1818]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1212 3030]]\n","confusion matrix for male female\n","[[3030 1818]\n"," [1212 3030]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[4249    0]\n"," [ 607 4249]]\n","confusion matrix for female\n","[[3035 1821]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1214 3035]]\n","confusion matrix for male female\n","[[3035 1821]\n"," [1214 3035]]\n","Accuracy of SD: 93 %\n","Accuracy of MF: 66 %\n","Accuracy of F: 62 %\n","Accuracy of M: 71 %\n","confusion matrix for same differen\n","[[4256    0]\n"," [ 608 4256]]\n","confusion matrix for female\n","[[3040 1824]\n"," [   0    0]]\n","confusion matrix for male\n","[[   0    0]\n"," [1216 3040]]\n","confusion matrix for male female\n","[[3040 1824]\n"," [1216 3040]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"]}],"source":["import numpy as np\n","\n","val_index1 = np.random.permutation(val_index)[:100]\n","val_dataloader = DataLoader(dataset,batch_size=50,sampler = SubsetRandomSampler(test_index))\n","val_iter = iter(val_dataloader)\n","predicted_lables = []\n","real_labels_g = []\n","predicted_lables_g = []\n","real_labels = []\n","real_labels_y = []\n","predicted_lables_y = []\n","predicted_lables_F=[]\n","predicted_lables_M=[]\n","real_labels_F = []\n","real_labels_M = []\n","\n","total = 0\n","total_F = 0\n","total_M = 0\n","correct1_SD = 0\n","correct1_MF = 0\n","correct_M = 0\n","correct_F = 0\n","correct1_YO = 0\n","acc1=[]\n","acc2=[]\n","acc_M = []\n","acc_F = []\n","acc1_YO = []\n","for j,dataj in enumerate(val_dataloader):\n","            input1, input2 , label, gender = data.items()\n","            input1, input2 = input1[1].to(device), input2[1].to(device)\n","            labelj = label[1].to(device)\n","            gender = gender[1].to(device)\n","            # young = young[1].to(device)\n","            gender = (gender+torch.ones_like(gender).to(device))//2\n","            # young = (young+torch.ones_like(young).to(device))//2\n","\n","            mu_all = torch.cat((mu_high,mu_low.data),1)\n","            logvar_all = torch.cat((logvar_high,logvar_low),1)\n","            class_input = torch.cat((mu_all,logvar_all),1)\n","            #class_input = mu_all\n","            class_out_SD = class_net_SD(class_input)\n","            class_out_MF = class_net_MF(class_input)\n","            # class_out_YO = class_net_YO(class_input)\n","            _,predicted1_SD = torch.max(class_out_SD.data,1)\n","            _,predicted1_MF = torch.max(class_out_MF.data,1)\n","            # _,predicted1_YO = torch.max(class_out_YO.data,1)\n","            # _,predicted2 = torch.max(output_A.data,1)\n","            total +=labelj.size(0)\n","            correct1_SD += (predicted1_SD == labelj).sum().item()\n","            correct1_MF += (predicted1_MF == gender).sum().item()\n","            # for gender == 0\n","            correct_F += (predicted1_MF[gender==0] == gender[gender==0]).sum().item()\n","            total_F += len(gender[gender==0])\n","            predicted_lables_F.append(torch.Tensor.numpy(predicted1_MF[gender==0].cpu()))\n","            real_labels_F.append(torch.Tensor.numpy(gender[gender==0].cpu()))\n","\n","             # for gender == 1\n","            correct_M += (predicted1_MF[gender==1] == gender[gender==1]).sum().item()\n","            total_M += gender[gender==1].sum().item()\n","\n","            predicted_lables_M.append(torch.Tensor.numpy(predicted1_MF[gender==1].cpu()))\n","            real_labels_M.append(torch.Tensor.numpy(gender[gender==1].cpu()))\n","\n","\n","\n","            # correct1_YO += (predicted1_YO == young).sum().item()\n","            predicted_lables.append(torch.Tensor.numpy(predicted1_SD.cpu()))\n","            real_labels.append(torch.Tensor.numpy(labelj.cpu()))\n","\n","            predicted_lables_g.append(torch.Tensor.numpy(predicted1_MF.cpu()))\n","            real_labels_g.append(torch.Tensor.numpy(gender.cpu()))\n","            # predicted_lables_y.append(torch.Tensor.numpy(predicted1_YO.cpu()))\n","            # real_labels_y.append(torch.Tensor.numpy(young.cpu()))\n","\n","            # correct2 += (predicted2 == gender).sum().item()\n","            print('Accuracy of SD: %d %%'%(100*correct1_SD/total))\n","            print('Accuracy of MF: %d %%'%(100*correct1_MF/total))\n","            print('Accuracy of F: %d %%'%(100*correct_F/total_F))\n","            print('Accuracy of M: %d %%'%(100*correct_M/total_M))\n","\n","            # print('Accuracy of YO: %d %%'%(100*correct1_YO/total))\n","\n","           \n","            acc1.append(100*correct1_SD/total)\n","            acc2.append(100*correct1_MF/total)\n","            acc1_YO.append(100*correct1_YO/total)\n","            print('confusion matrix for same differen')\n","            print(mt.confusion_matrix(np.array(real_labels).flatten(),np.array(predicted_lables).flatten()))\n","            print('confusion matrix for female')\n","            print(mt.confusion_matrix(np.array(real_labels_F).flatten(),np.array(predicted_lables_F).flatten()))\n","            print('confusion matrix for male')\n","            print(mt.confusion_matrix(np.array(real_labels_M).flatten(),np.array(predicted_lables_M).flatten()))\n","            print('confusion matrix for male female')\n","            print(mt.confusion_matrix(np.array(real_labels_g).flatten(),np.array(predicted_lables_g).flatten()))\n","            # print('confusion matrix for young old')\n","            # print(mt.confusion_matrix(np.array(real_labels_y).flatten(),np.array(predicted_lables_y).flatten()))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"10GaTVtcypFz"},"outputs":[],"source":["# val_index1 = np.random.permutation(val_index)[:100]\n","# val_dataloader = DataLoader(dataset,batch_size=50,sampler = SubsetRandomSampler(test_index))\n","# val_iter = iter(val_dataloader)\n","# predicted_lables = []\n","# real_labels_g = []\n","# predicted_lables_g = []\n","# real_labels = []\n","# total = 0\n","# correct1_SD = 0\n","# correct1_MF = 0\n","# acc1=[]\n","# acc2=[]\n","# for j,dataj in enumerate(val_dataloader):\n","#             input1j, input2j, labelj, gender = dataj.items()\n","#             input1j, input2j = input1j[1].to(device), input2j[1].to(device)\n","\n","#             labelj = labelj[1].to(device)\n","#             gender = gender[1].to(device)\n","#             gender = (gender+torch.ones_like(gender).to(device))//2\n","#             recon_batch_high, mu_high, logvar_high, h2_h = model_high(input1j)\n","#             recon_batch_low, mu_low, logvar_low, h2_l = model_low(input2j)\n","\n","#             mu_all = torch.cat((mu_high,mu_low.data),1)\n","#             logvar_all = torch.cat((logvar_high,logvar_low),1)\n","#             class_input = torch.cat((mu_all,logvar_all),1)\n","#             #class_input = mu_all\n","#             class_out_SD = class_net_SD(class_input)\n","#             class_out_MF = class_net_MF(class_input)\n","#             _,predicted1_SD = torch.max(class_out_SD.data,1)\n","#             _,predicted1_MF = torch.max(class_out_MF.data,1)\n","#             # _,predicted2 = torch.max(output_A.data,1)\n","#             total +=labelj.size(0)\n","#             correct1_SD += (predicted1_SD == labelj).sum().item()\n","#             correct1_MF += (predicted1_MF == gender).sum().item()\n","#             predicted_lables.append(torch.Tensor.numpy(predicted1_SD.cpu()))\n","#             real_labels.append(torch.Tensor.numpy(labelj.cpu()))\n","\n","#             predicted_lables_g.append(torch.Tensor.numpy(predicted1_MF.cpu()))\n","#             real_labels_g.append(torch.Tensor.numpy(gender.cpu()))\n","#             # correct2 += (predicted2 == gender).sum().item()\n","#             print('Accuracy of SD: %d %%'%(100*correct1_SD/total))\n","#             print('Accuracy of MF: %d %%'%(100*correct1_MF/total))\n","\n","           \n","#             acc1.append(100*correct1_SD/total)\n","#             acc2.append(100*correct1_MF/total)\n","#             print(mt.confusion_matrix(np.array(real_labels).flatten(),np.array(predicted_lables).flatten()))\n","            \n","#             print(mt.confusion_matrix(np.array(real_labels_g).flatten(),np.array(predicted_lables_g).flatten()))\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyP4ztvorPMtImeRZho+zMcj","collapsed_sections":[],"name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}